<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Getting Started with Ruby on Rails</title>
      <link href="/blog//2020/01/28/rails-getting-started/"/>
      <url>/blog//2020/01/28/rails-getting-started/</url>
      
        <content type="html"><![CDATA[<img src="/blog/2020/01/28/rails-getting-started/ruby-on-rails.png" class=""><p>We recently inducted a couple of engineers into our Rails team. Both of them had a web frontend experience, but zero experience with Ruby or backend development. Thanks to the simplicity and convention focussed approach of the Rails framework, both of them were writing test driven production grade code within a week!</p><p>First thing was setting up their machines. We decided to go with</p><ul><li>Ubuntu 18.04 for the OS</li><li>RVM as the Ruby version manager</li><li>RubyMine as the IDE</li><li>Postgresql as the database</li></ul><h3 id="Ruby"><a href="#Ruby" class="headerlink" title="Ruby"></a>Ruby</h3><p>First step was to get them comfortable with Ruby. We believe that doing is the best way of learning. So we got them to complete the wonderful koans at <a href="http://rubykoans.com/" target="_blank" rel="noopener">http://rubykoans.com/</a> to get hands on practice.</p><p>An important thing while working in a team is to have consistent coding style across all members. We follow the style guide at <a href="https://github.com/rubocop-hq/ruby-style-guide" target="_blank" rel="noopener">https://github.com/rubocop-hq/ruby-style-guide</a> , so they read through the guide.</p><h3 id="Rails"><a href="#Rails" class="headerlink" title="Rails"></a>Rails</h3><p>We feel the best guide for Rails is the official guide itself.</p><p><a href="https://edgeguides.rubyonrails.org/" target="_blank" rel="noopener">https://edgeguides.rubyonrails.org/</a></p><p>The three sections that were assigned were,</p><ul><li>Getting Started with Rails</li><li>Models</li><li>Controllers</li></ul><p>We skipped the views as we mostly work on API only apps.</p><p>After this, we covered the Rails style guide <a href="https://github.com/rubocop-hq/rails-style-guide" target="_blank" rel="noopener">https://github.com/rubocop-hq/rails-style-guide</a> .</p><h3 id="Tests"><a href="#Tests" class="headerlink" title="Tests"></a>Tests</h3><p>We use <a href="https://github.com/rspec/rspec-rails" target="_blank" rel="noopener">RSpec</a> and <a href="https://github.com/thoughtbot/factory_bot" target="_blank" rel="noopener">FactoryBot</a> internally for writing our tests.</p><p>For RSpec, the Github page is a good starting point <a href="https://github.com/rspec/rspec-rails" target="_blank" rel="noopener">https://github.com/rspec/rspec-rails</a></p><p>For FactoryBot, we assigned the Getting Started guide on Github <a href="https://github.com/thoughtbot/factory_bot/blob/master/GETTING_STARTED.md" target="_blank" rel="noopener">https://github.com/thoughtbot/factory_bot/blob/master/GETTING_STARTED.md</a></p><h3 id="Continuing-Education"><a href="#Continuing-Education" class="headerlink" title="Continuing Education"></a>Continuing Education</h3><p>With these basic tutorials, the engineers were basically ready for contributing to production. Their first few pull requests had many comments, but they came down significantly within the first two weeks.</p><p>For continuing our Rails education, we keep reading up on the frequent gems that we use, such as</p><ul><li>devise</li><li>aasm</li><li>active_model_serializers</li><li>pundit</li><li>resque</li><li>whenever</li><li>carrierwave</li></ul><p>and other excellent gems.</p><h3 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h3><p>We got a pleasant reminder as to why Rails is our favorite framework to work on. Within the first few weeks only the engineers were writing production grade, well tested code.</p><p>Hats off to the Ruby philosophy and Matz that our fresh Ruby engineers were able to start guessing the function names for different classes almost immediately!</p>]]></content>
      
      
      <categories>
          
          <category> Rails </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ruby </tag>
            
            <tag> Rails </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Efficient-Residual-Factorized-Neural-Network-for-Semantic-Segmentation</title>
      <link href="/blog//2020/01/09/Efficient-Residual-Factorized-Neural-Network-for-Semantic-Segmentation/"/>
      <url>/blog//2020/01/09/Efficient-Residual-Factorized-Neural-Network-for-Semantic-Segmentation/</url>
      
        <content type="html"><![CDATA[<!-- # Efficient Residual Factorized Neural Network for Semantic Segmentation --><blockquote><p>This post explains a research paper ERFNET, a real time and accurate ConvNet for semantic segmentation and the underlying concepts.</p></blockquote><h2 id="Semantic-Segmentation"><a href="#Semantic-Segmentation" class="headerlink" title="Semantic Segmentation"></a>Semantic Segmentation</h2><p>Semantic segmentation is the task of classifying each image pixel to a class label. It is a classification task but at pixel level instead of image level. The labels could include a person, car, flower, piece of furniture, etc. For example, in the below image, all the cars will have the same labels. However, one can differentiate between the same class objects, this is called instance segmentation. For example, in an image that has many cars, <strong>instance segmentation</strong> can differentiate between each car object.</p><!-- ![Image 1](https://cdn-images-1.medium.com/max/3840/1*CUp00rA4mSlJ8CRflVvKcA.jpeg)*Image 1* --><img src="/blog/2020/01/09/Efficient-Residual-Factorized-Neural-Network-for-Semantic-Segmentation/image_1.jpeg" class="" title="Image 1"><p>Semantic segmentation has many applications in autonomous vehicles, Advanced Driver Assistance Systems (ADAS), robotics, self-driving cars because it is important to know the context in which the agent operates.</p><p>Convolutional Neural Networks (CNN), which initially designed for classification tasks, have impressive capabilities in solving complex segmentation tasks as well. <strong>Residual layers</strong> have created a new trend in ConvNets design. Their reformulation of the convolutional layers to avoid the degradation problem of deep architectures allowed neural networks to achieve very high accuracies with large amounts of layers.</p><p>Computation resources are key factors in self-driving and autonomous vehicles. Algorithms are not only required to operate reliably, but they are required to operate fast (real-time), fit in embedded devices due to space constraints (compactness), and have low power consumption to affect as minimum as possible the vehicle autonomy. Considering a reasonable amount of layers, enlarging the depth with more convolutions achieves only small gains in accuracy while significantly increasing the required computational resources.</p><h3 id="Residual-Layer"><a href="#Residual-Layer" class="headerlink" title="Residual Layer"></a>Residual Layer</h3><p>The paper proposes a new architecture design that leverages skip connections and convolutions with 1D kernels. While the skip connections allow the convolutions to learn residual functions that facilitate training, the 1D factorized convolutions allow a significant reduction of the computational costs while retaining a similar accuracy compared to the 2D ones.</p><!-- ![Residual Block](https://cdn-images-1.medium.com/max/2000/1*6WlIo8W1_Qc01hjWdZy-1Q.png)*Residual Block* --><img src="/blog/2020/01/09/Efficient-Residual-Factorized-Neural-Network-for-Semantic-Segmentation/residual_block.png" class="" title="Residual Block"><p>Residual blocks allow convolutional layers to learn the residual functions. For example, in the above image, x is the input vector and F(X)+x is the output vector of the y vector. F(X) is the residual function to be learned. This residual formulation facilitates learning and significantly reduces the degradation problem present in architectures that stack a large number of layers.</p><h3 id="Non-bottleneck-Residual-Layers"><a href="#Non-bottleneck-Residual-Layers" class="headerlink" title="Non-bottleneck Residual Layers"></a>Non-bottleneck Residual Layers</h3><p>There can be two instances of residual layer: the non-bottleneck design with two 3x3 convolutions as depicted in Fig. 1(a), and the bottleneck version as depicted in Fig. 1(b). Both versions have a similar number of parameters and almost equivalent accuracy. However, the bottleneck requires less computational resources and these scale in a more economical way as depth increases. Hence, the bottleneck design has been commonly adopted in state-of-the-art networks. However, it has been reported that non-bottleneck ResNets gain more accuracy from increased depth than the bottleneck versions, which indicates that they are not entirely equivalent and that the bottleneck design still suffers from the degradation problem</p><!-- ![Figure 1](https://cdn-images-1.medium.com/max/2000/1*gu-iuZvJS1-8w-UlWGXmww.png)*Figure 1* --><img src="/blog/2020/01/09/Efficient-Residual-Factorized-Neural-Network-for-Semantic-Segmentation/figure_1.png" class="" title="Figure 1"><p>The paper proposed a new implementation of the residual layer that decomposes 2D convolution into a pair of 1D convolutions to accelerate and reduce the parameters of the original non-bottleneck layer. We refer to this proposed module as “non-bottleneck-1D” (non-bt-1D), which is depicted in Fig. 1(c). This module is faster (as in computation time) and has fewer parameters than the bottleneck design while keeping a learning capacity and accuracy equivalent to the non-bottleneck one.</p><h3 id="Dilated-Convolutions"><a href="#Dilated-Convolutions" class="headerlink" title="Dilated Convolutions"></a>Dilated Convolutions</h3><p>Dilated convolutions are convolutions applied to input images with gaps. The standard convolution is 1-Dilated convolution. Dilated convolutions other than standard convolutions increase the receptive field of the network. Dilated convolutions are more effective in terms of computational cost and parameters than the convolutions with larger kernel size. The paper proposes a network that uses dilated convolution.</p><!-- ![standard convolution](https://cdn-images-1.medium.com/max/2000/1*aIPu6hDHHWFatmOCYP9YPg.gif)*standard convolution* --><!-- ![dilated convolution](https://cdn-images-1.medium.com/max/2000/1*wz4x8BcAOFBPNL6nX4tx-g.gif)*dilated convolution* --><img src="/blog/2020/01/09/Efficient-Residual-Factorized-Neural-Network-for-Semantic-Segmentation/standard_conv.gif" class="" title="Standard Convolution"><img src="/blog/2020/01/09/Efficient-Residual-Factorized-Neural-Network-for-Semantic-Segmentation/dilated_conv.gif" class="" title="Dilated Convolution"><h3 id="Network-Architecture"><a href="#Network-Architecture" class="headerlink" title="Network Architecture"></a>Network Architecture</h3><p>The paper presents an encoder-decoder architecture for semantic segmentation. The encoder segment produces downsampled feature maps and the decoder segments upsample the features to match input image resolution. Full network architecture is given in Figure 2</p><!-- ![architecture with feature maps](https://cdn-images-1.medium.com/max/2000/1*YMWnwx78KluFYgcV5KGB0g.png)*architecture with feature maps*![Figure 2](https://cdn-images-1.medium.com/max/2000/1*9Bbsq9_xHHImVSqtFkJrtw.png)*Figure 2* --><img src="/blog/2020/01/09/Efficient-Residual-Factorized-Neural-Network-for-Semantic-Segmentation/image_2.png" class="" title="Architecture with feature maps"><img src="/blog/2020/01/09/Efficient-Residual-Factorized-Neural-Network-for-Semantic-Segmentation/figure_2.png" class="" title="figure 2"><h3 id="References"><a href="#References" class="headerlink" title="References"></a>References</h3><ol><li><p><a href="https://pixabay.com/photos/traffic-locomotion-roadway-mobility-3612474/" target="_blank" rel="noopener">https://pixabay.com/photos/traffic-locomotion-roadway-mobility-3612474/</a></p></li><li><p>ERFNet: Efficient Residual Factorized ConvNet for Real-time Semantic Segmentation.</p></li><li><p><a href="https://cdn-images-1.medium.com/max/800/1*aIPu6hDHHWFatmOCYP9YPg.gif" target="_blank" rel="noopener">https://cdn-images-1.medium.com/max/800/1*wz4x8BcAOFBPNL6nX4tx-g.gif</a></p></li><li><p><a href="https://miro.medium.com/max/395/0*3cTXIemm0k3Sbask.gif" target="_blank" rel="noopener">https://miro.medium.com/max/395/0*3cTXIemm0k3Sbask.gif</a></p></li></ol>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Deep Learning </tag>
            
            <tag> Residual Networks </tag>
            
            <tag> Dilated Convolutions </tag>
            
            <tag> Semantic segmentation </tag>
            
            <tag> Autonomous driving </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Creating custom Grafana panel</title>
      <link href="/blog//2020/01/06/create-custom-grafana-panel/"/>
      <url>/blog//2020/01/06/create-custom-grafana-panel/</url>
      
        <content type="html"><![CDATA[<br><img src="/blog/2020/01/06/create-custom-grafana-panel/grafana.png" class="" title="l"><h3 id="Grafana"><a href="#Grafana" class="headerlink" title="Grafana"></a>Grafana</h3><p>Grafana is an open source platform for monitoring and observability. Grafana allows you to query, visualize, alert on and understand your metrics no matter where they are stored. Create, explore, and share dashboards with your team and foster a data driven culture.</p><p>Grafana is a tool that is used to create dashboards. Dashboards consists of Panels which are used to visualize data in many ways. Grafana has many types of panels available to visualize your data in many forms. Such panels include Graphs, Tables, Single stats, etc. But what if you want to use the Grafana but show data in your own format. Grafana allows that as well. These components are known as plugins.</p><blockquote><p>In this post we will see how to make our own <strong>custom plugin</strong> and use it in our Grafana dashboard.</p></blockquote><h3 id="Requisites"><a href="#Requisites" class="headerlink" title="Requisites:"></a>Requisites:</h3><ul><li>npm or yarn</li><li>Git</li></ul><p>First download the Grafana repo from Grafana website<br><a href="https://grafana.com/grafana/download" target="_blank" rel="noopener">https://grafana.com/grafana/download</a></p><p><img src="directory.png" alt=""></p><p>Then clone Hello World Custom panel with the command written below in the folder <strong>data/plugins</strong> or <strong>var/lib/grafana/plugins</strong>. If no such folder exists in root directory create one.<br><code>Git clone https://github.com/grafana/simple-angular-panel</code></p><p><img src="clone.png" alt=""></p><p>This folder will be created in your data/plugins.</p><p><img src="angular_panel.png" alt=""></p><p>Install the dependencies required for that panel using <code>npm install</code> or <code>yarn</code>. Then build the plugin using <code>yarn build</code> or <code>npm run build</code>. (Running the build script defined in package.json)</p><p>Then start the grafana-server:</p><ul><li>By running the command <code>./bin/grafana-server</code> in the root directory of your grafana repo (<strong>Linux</strong>).</li><li>By running the <strong>grafana-server.exe</strong> in bin Folder (<strong>Windows</strong>).</li></ul><p><img src="bin.png" alt=""></p><p>Then panel will be availables in your visualization section when you create a new panel</p><p><img src="panel_icon.png" alt=""></p><h3 id="Plugin-json"><a href="#Plugin-json" class="headerlink" title="Plugin.json"></a>Plugin.json</h3><p>It defines your panel and uniquely identifies it.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">&quot;type&quot;: &quot;panel&quot;,</span><br><span class="line">&quot;name&quot;: &quot;Simple Angular&quot;,</span><br><span class="line">&quot;id&quot;: &quot;simple-angular-panel&quot;,</span><br><span class="line"></span><br><span class="line">&quot;info&quot;: &#123;</span><br><span class="line">&quot;description&quot;: &quot;Simple Angular Panel&quot;,</span><br><span class="line">&quot;author&quot;: &#123;</span><br><span class="line">&quot;name&quot;: &quot;Grafana Labs&quot;</span><br><span class="line">&#125;,</span><br><span class="line">&quot;keywords&quot;: [ &quot;discrete&quot;, &quot;events&quot;, &quot;strings&quot; ],</span><br><span class="line">&quot;logos&quot;: &#123;</span><br><span class="line">&quot;small&quot;: &quot;img/logo.svg&quot;,</span><br><span class="line">&quot;large&quot;: &quot;img/logo.svg&quot;</span><br><span class="line">&#125;,</span><br><span class="line">&quot;links&quot;: [],</span><br><span class="line">&quot;screenshots&quot;: [],</span><br><span class="line">&quot;version&quot;: &quot;%VERSION%&quot;,</span><br><span class="line">&quot;updated&quot;: &quot;%TODAY%&quot;</span><br><span class="line">&#125;,</span><br><span class="line"></span><br><span class="line">&quot;dependencies&quot;: &#123;</span><br><span class="line">&quot;grafanaVersion&quot;: &quot;6.3.x&quot;,</span><br><span class="line">&quot;plugins&quot;: [ ]</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li><strong>“name”</strong>: Name Displayed on panel</li><li><strong>“id”</strong>: Unique Identified of your plugin. If two panels have same ID only 1 will be shown</li><li><strong>“logos”</strong>: shows the images shown for your panel which can be found in src/img folder.</li></ul><h3 id="Module-ts"><a href="#Module-ts" class="headerlink" title="Module.ts"></a>Module.ts</h3><p>It is the entry point of your panel. Functions and variables can be created in <strong>module.ts</strong> and be used In html page using “ctrl” as the object of the class.</p><h3 id="Front-HTML-Page"><a href="#Front-HTML-Page" class="headerlink" title="Front HTML Page."></a>Front HTML Page.</h3><p>This page can be found at <strong>partials/module.html</strong>.</p><p><img src="FrontPage.png" alt=""></p><p>So if you create a variable named “text” in the SimpleCtrl Class (<em>in module.ts</em>). It can be accessed in HTML page using ““.</p><p><strong>Options.html</strong> page contains the page the visualization section which is shown when you create or edit a panel.</p><p>After You create your html pages and functionality in module.ts, you can run <code>npm run build</code> or <code>yarn build</code> and use the panel in your own Grafana however you like.</p><blockquote><p>Tips : If you want to see changes in your panel while you are creating it. Use <code>npm run watch</code> or <code>yarn watch</code>. It implements changes as you make them. It basically creates build with environment settings as development. You can see the changes by refreshing the webpage.</p></blockquote><p>References:</p><ul><li><a href="https://grafana.com/grafana/download" target="_blank" rel="noopener">https://grafana.com/grafana/download</a></li><li><a href="https://github.com/grafana/simple-angular-panel" target="_blank" rel="noopener">https://github.com/grafana/simple-angular-panel</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Web </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Grafana </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Angular Charts</title>
      <link href="/blog//2020/01/02/angular-charts/"/>
      <url>/blog//2020/01/02/angular-charts/</url>
      
        <content type="html"><![CDATA[<br><img src="/blog/2020/01/02/angular-charts/BG.png" class="" title="l"><p>Chart.js is a popular JavaScript charting library and ng2-charts is a wrapper for Angular 2+ that makes it easy to integrate Chart.js in Angular. Let’s go over the basic usage.</p><h2 id="Installation"><a href="#Installation" class="headerlink" title="Installation"></a>Installation</h2><ol><li>Install ng2-charts using npm: <code>npm install --save ng2-charts</code></li><li>Install Chart.js library: <code>npm install --save chart.js</code></li><li><em>[Options]</em> Then, if you’re using the <strong>Angular CLI</strong>, you can simply add Chart.js to the list of scripts in your <code>.angular-cli.json</code> file so that it gets bundled with the app: </li></ol><p><strong><em>angular-cli.json</em></strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&quot;scripts&quot;: [</span><br><span class="line">  &quot;../node_modules/chart.js/dist/Chart.min.js&quot;</span><br><span class="line">],</span><br></pre></td></tr></table></figure><h2 id="API"><a href="#API" class="headerlink" title="API"></a>API</h2><p>Now you’ll want to import ng2-chart’s <code>ChartsModule</code> into your app module or a feature module:</p><p><strong><em>app.module.ts</em></strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">import &#123; BrowserModule &#125; from &apos;@angular/platform-browser&apos;;</span><br><span class="line">import &#123; NgModule &#125; from &apos;@angular/core&apos;;</span><br><span class="line">import &#123; ChartsModule &#125; from &apos;ng2-charts&apos;;</span><br><span class="line"></span><br><span class="line">import &#123; AppComponent &#125; from &apos;./app.component&apos;;</span><br><span class="line"></span><br><span class="line">@NgModule(&#123;</span><br><span class="line">  declarations: [AppComponent],</span><br><span class="line">  imports: [</span><br><span class="line">    BrowserModule,</span><br><span class="line">    ChartsModule</span><br><span class="line">  ],</span><br><span class="line">  providers: [],</span><br><span class="line">  bootstrap: [AppComponent]</span><br><span class="line">&#125;)</span><br><span class="line">export class AppModule &#123;&#125;</span><br></pre></td></tr></table></figure><h2 id="Usage"><a href="#Usage" class="headerlink" title="Usage"></a>Usage</h2><p>ng2-charts gives us a <code>baseChart</code> directive that can be applied on an HTML <code>canvas</code> element. Here’s an example showing-off some of the options to pass-in as inputs and the <code>chartClick</code> event that’s outputted by the directive:</p><p><strong><em>app.component.html</em></strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&lt;div style=&quot;width: 40%;&quot;&gt;</span><br><span class="line">  &lt;canvas</span><br><span class="line">      baseChart</span><br><span class="line">      [chartType]=&quot;&apos;line&apos;&quot;</span><br><span class="line">      [datasets]=&quot;chartData&quot;</span><br><span class="line">      [labels]=&quot;chartLabels&quot;</span><br><span class="line">      [options]=&quot;chartOptions&quot;</span><br><span class="line">      [legend]=&quot;true&quot;</span><br><span class="line">      (chartClick)=&quot;onChartClick($event)&quot;&gt;</span><br><span class="line">  &lt;/canvas&gt;</span><br><span class="line">&lt;/div&gt;</span><br></pre></td></tr></table></figure><p>And here’s what it can look like in our component class:</p><p><strong><em>app.component.ts</em></strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">import &#123; Component &#125; from &apos;@angular/core&apos;;</span><br><span class="line"></span><br><span class="line">@Component(&#123; ... &#125;)</span><br><span class="line">export class AppComponent &#123;</span><br><span class="line">  chartOptions = &#123;</span><br><span class="line">    responsive: true</span><br><span class="line">  &#125;;</span><br><span class="line"></span><br><span class="line">  chartData = [</span><br><span class="line">    &#123; data: [330, 600, 260, 700], label: &apos;Account A&apos; &#125;,</span><br><span class="line">    &#123; data: [120, 455, 100, 340], label: &apos;Account B&apos; &#125;,</span><br><span class="line">    &#123; data: [45, 67, 800, 500], label: &apos;Account C&apos; &#125;</span><br><span class="line">  ];</span><br><span class="line"></span><br><span class="line">  chartLabels = [&apos;January&apos;, &apos;February&apos;, &apos;Mars&apos;, &apos;April&apos;];</span><br><span class="line"></span><br><span class="line">  onChartClick(event) &#123;</span><br><span class="line">    console.log(event);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="chart_1.png" alt="Chart 1: Basic Line Chart"></p><h2 id="Options"><a href="#Options" class="headerlink" title="Options"></a>Options</h2><p>Here’s a quick breakdown of the different input options</p><ul><li>chartType: This sets the base type of the chart. The value can be <code>pie</code>, <code>doughnut</code>, <code>bar</code>, <code>line</code>, <code>polarArea</code>, <code>radar</code> or <code>horizontalBar</code>.</li><li>legend: A boolean for whether or not a legend should be displayed above the chart.</li><li>datasets: This should be an array of objects that contain a data array and a label for each data set.</li><li>data: If your chart is simple and has only one data set, you can use <code>data</code> instead of <code>datasets</code> and pass-in an array of data points.</li><li>labels: An array of labels for the X-axis.</li><li>options: An object that contains options for the chart. You can refer to the official <a href="https://www.chartjs.org/docs/latest/configuration/" target="_blank" rel="noopener"><code>Chart.js documentation</code></a> for details on the available options.</li></ul><p>In the above example we set the chart to be responsive and adapt depending on the viewport size.</p><ul><li>colors: Not shown in the above example, but you can define your own colors with the <code>colors</code> input. Pass-in an array of object literals that contain the following value:<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">myColors = [</span><br><span class="line">  &#123;</span><br><span class="line">    backgroundColor: &apos;rgba(103, 58, 183, .1)&apos;,</span><br><span class="line">    borderColor: &apos;rgb(103, 58, 183)&apos;,</span><br><span class="line">    pointBackgroundColor: &apos;rgb(103, 58, 183)&apos;,</span><br><span class="line">    pointBorderColor: &apos;#fff&apos;,</span><br><span class="line">    pointHoverBackgroundColor: &apos;#fff&apos;,</span><br><span class="line">    pointHoverBorderColor: &apos;rgba(103, 58, 183, .8)&apos;</span><br><span class="line">  &#125;,</span><br><span class="line">  ... other colors</span><br><span class="line">];</span><br></pre></td></tr></table></figure></li></ul><h2 id="Events"><a href="#Events" class="headerlink" title="Events"></a>Events</h2><p>Two events are emitted, <code>chartClick</code> and <code>chartHover</code>, and they allow to react to the user interacting with the chart. The currently active points and labels are returned as part of the emitted event’s data.</p><ul><li>chartClick: fires when click on a chart has occurred, returns information regarding active points and labels</li><li>chartHover: fires when mousemove (hover) on a chart has occurred, returns information regarding active points and labels</li><li>Updating Datasets Dynamically: Of course, the beauty of Chart.js is that your charts can easily by dynamic and update/respond to data received from a backend or from user input.</li></ul><p>In the bellow example we add a new data points for the month of May:</p><p><strong><em>app.component.ts</em></strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">newDataPoint(dataArr = [100, 100, 100], label) &#123;</span><br><span class="line"></span><br><span class="line">  this.chartData.forEach((dataset, index) =&gt; &#123;</span><br><span class="line">    this.chartData[index] = Object.assign(&#123;&#125;, this.chartData[index], &#123;</span><br><span class="line">      data: [...this.chartData[index].data, dataArr[index]]</span><br><span class="line">    &#125;);</span><br><span class="line">  &#125;);</span><br><span class="line"></span><br><span class="line">  this.chartLabels = [...this.chartLabels, label];</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>And it can be used like this:</p><p><strong><em>app.component.html</em></strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;button (click)=&quot;newDataPoint([900, 50, 300], &apos;May&apos;)&quot;&gt;</span><br><span class="line">  Add data point</span><br><span class="line">&lt;/button&gt;</span><br></pre></td></tr></table></figure><h2 id="Schematics"><a href="#Schematics" class="headerlink" title="Schematics"></a>Schematics</h2><p>There are schematics that may be used to generate chart components using Angular CLI. The components are defined in package ng2-charts-schematics.</p><p>Installation of Schematics Package<br><code>npm instal --save-dev ng2-charts-schematics</code></p><p>Example of Generating a Line Chart using Angular CLI<br><code>ng generate ng20chart0schematics:line my-line-chart</code></p><p>This calls angular’s component schematics and then modifies the result, so all the options for the component schematic are also usable here. This schematics will also add the ChartsModule as an imported module in the main app module (or another module as specified in the –module command switch).</p>]]></content>
      
      
      <categories>
          
          <category> Web </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Line Chart </tag>
            
            <tag> Javascript </tag>
            
            <tag> Data Visualization </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Paytm Gateway Integration</title>
      <link href="/blog//2020/01/02/paytm-gateway-integration/"/>
      <url>/blog//2020/01/02/paytm-gateway-integration/</url>
      
        <content type="html"><![CDATA[<p>A complete guide on adding payments to your Android app with backend as RoR</p><img src="/blog/2020/01/02/paytm-gateway-integration/img_flow_android_ios_sdk.png" class=""><h2 id="Steps"><a href="#Steps" class="headerlink" title="Steps:"></a>Steps:</h2><ol><li>Install SDK</li><li>Add Static SMS Permission (for SMS autoread)</li><li>Add Runtime SMS Permission (for SMS autoread)</li><li>Add Proguard Rules</li><li>Get Order Checksum from Server (our Server)</li><li>Generate and send Checksum (from Our Server)</li><li>Start Payment Transaction</li><li>Send Payment Response to Server</li><li>Confirm with Paytm Gateway about payment status</li><li>Update Order Status</li><li>Show Order Status on App</li></ol><br/><h3 id="Step-1-Install-SDK"><a href="#Step-1-Install-SDK" class="headerlink" title="Step 1: Install SDK"></a>Step 1: Install SDK</h3><p>Add the following dependency to your <strong>app level</strong> <code>build.gradle</code>.</p><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">dependencies &#123;</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="comment">// Paytm SDK</span></span><br><span class="line">implementation(<span class="string">'com.paytm:pgplussdk:1.4.4'</span>) &#123;</span><br><span class="line">transitive = <span class="literal">true</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Step-2-Add-Static-SMS-Permission-for-SMS-autoread"><a href="#Step-2-Add-Static-SMS-Permission-for-SMS-autoread" class="headerlink" title="Step 2: Add Static SMS Permission (for SMS autoread)"></a>Step 2: Add Static SMS Permission (for SMS autoread)</h3><p>Add the following permissions to your <code>AndroidManifest.xml</code>.</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">uses-permission</span> <span class="attr">android:name</span>=<span class="string">”android.permission.READ_SMS”/</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;<span class="name">uses-permission</span> <span class="attr">android:name</span>=<span class="string">”android.permission.RECEIVE_SMS”/</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="Step-3-Add-Runtime-SMS-Permission-for-SMS-autoread"><a href="#Step-3-Add-Runtime-SMS-Permission-for-SMS-autoread" class="headerlink" title="Step 3: Add Runtime SMS Permission (for SMS autoread)"></a>Step 3: Add Runtime SMS Permission (for SMS autoread)</h3><p>We used <a href="https://github.com/Karumi/Dexter" target="_blank" rel="noopener">Dexter</a> library for handling runtime permissions. To install that add the following dependency to your <strong>app level</strong> <code>build.gradle</code>.</p><figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">dependencies &#123;</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="comment">// Dexter runtime permissions</span></span><br><span class="line">implementation <span class="string">'com.karumi:dexter:4.2.0'</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Add the following code before starting your transaction process.</p><figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">Dexter.withActivity(<span class="keyword">this</span>)</span><br><span class="line">            .withPermissions(</span><br><span class="line">                android.Manifest.permission.READ_SMS”,</span><br><span class="line">                android.Manifest.permission.RECEIVE_SMS”</span><br><span class="line">            )</span><br><span class="line">            .withListener(<span class="keyword">object</span> : MultiplePermissionsListener &#123;</span><br><span class="line">                <span class="keyword">override</span> <span class="function"><span class="keyword">fun</span> <span class="title">onPermissionsChecked</span><span class="params">(report: <span class="type">MultiplePermissionsReport</span>?)</span></span> &#123;</span><br><span class="line">                    report?.let &#123;</span><br><span class="line">                        <span class="keyword">if</span> (it.areAllPermissionsGranted()) &#123;</span><br><span class="line">                            beginPaytmTransaction()</span><br><span class="line">                        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                            showMessage(<span class="string">"Permission Denied"</span>)</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">override</span> <span class="function"><span class="keyword">fun</span> <span class="title">onPermissionRationaleShouldBeShown</span><span class="params">(</span></span></span><br><span class="line"><span class="function"><span class="params">                    permissions: <span class="type">MutableList</span>&lt;<span class="type">PermissionRequest</span>&gt;?, token: <span class="type">PermissionToken</span>?</span></span></span><br><span class="line"><span class="function"><span class="params">                )</span></span> &#123;</span><br><span class="line">                    token?.continuePermissionRequest()</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;)</span><br><span class="line">            .withErrorListener &#123;</span><br><span class="line">                showMessage(<span class="string">"Error occurred! <span class="variable">$it</span>"</span>)</span><br><span class="line">            &#125;</span><br><span class="line">            .onSameThread()</span><br><span class="line">            .check()</span><br></pre></td></tr></table></figure><h3 id="Step-4-Add-Proguard-Rules"><a href="#Step-4-Add-Proguard-Rules" class="headerlink" title="Step 4: Add Proguard Rules"></a>Step 4: Add Proguard Rules</h3><p>Add the following rules to your <code>proguard-rules.pro</code>.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">-keepclassmembers class com.paytm.pgsdk.paytmWebView$PaytmJavaScriptInterface &#123;</span><br><span class="line">public *;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Step-5-Get-Order-Checksum-from-Server-our-Server"><a href="#Step-5-Get-Order-Checksum-from-Server-our-Server" class="headerlink" title="Step 5: Get Order Checksum from Server (our Server)"></a>Step 5: Get Order Checksum from Server (our Server)</h3><p>Make an API call to your server to get the order checksum</p><figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">interface</span> <span class="title">OurService</span> </span>&#123;</span><br><span class="line"><span class="meta">@GET(<span class="meta-string">"/subscriptions/new"</span>)</span></span><br><span class="line"><span class="keyword">suspend</span> <span class="function"><span class="keyword">fun</span> <span class="title">newSubscription</span><span class="params">()</span></span>: NewSubscriptionResponse</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UserRepository</span></span>(<span class="keyword">private</span> <span class="keyword">val</span> ourService: OurService) &#123;</span><br><span class="line">    <span class="keyword">suspend</span> <span class="function"><span class="keyword">fun</span> <span class="title">createNewSubscription</span><span class="params">()</span></span>: NewSubscriptionResponse = ourService.newSubscription()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">open</span> <span class="class"><span class="keyword">class</span> <span class="title">ActivityState</span></span></span><br></pre></td></tr></table></figure><figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ProfileViewModel</span></span>(<span class="keyword">private</span> <span class="keyword">val</span> userRepository: UserRepository) : ViewModel() &#123;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">val</span> _paytmState: MutableLiveData&lt;ActivityState&gt; = MutableLiveData(InitialState)</span><br><span class="line"><span class="keyword">val</span> paytmState: LiveData&lt;ActivityState&gt; = _paytmState</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">fun</span> <span class="title">createNewSubscription</span><span class="params">()</span></span> &#123;</span><br><span class="line">viewModelScope.launch &#123;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">_paytmState.value = ProgressState</span><br><span class="line">_paytmState.value = PaytmChecksumState(userRepository.createNewSubscription())</span><br><span class="line">&#125; <span class="keyword">catch</span> (e: HttpException) &#123;</span><br><span class="line">_paytmState.value = ErrorState(e)</span><br><span class="line">&#125; <span class="keyword">catch</span> (e: IOException) &#123;</span><br><span class="line">_paytmState.value = ErrorState(e)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">object</span> ProgressState : ActivityState()</span><br><span class="line"><span class="keyword">data</span> <span class="class"><span class="keyword">class</span> <span class="title">PaytmChecksumState</span></span>(<span class="keyword">val</span> checksumResponse: NewSubscriptionResponse) : ActivityState()</span><br><span class="line"><span class="keyword">data</span> <span class="class"><span class="keyword">class</span> <span class="title">ErrorState</span></span>(<span class="keyword">val</span> exception: Exception) : ActivityState()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ProfileFragment</span> : <span class="type">Fragment</span></span>() &#123;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">val</span> profileViewModel: ProfileViewModel <span class="keyword">by</span> viewModel()</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">fun</span> <span class="title">beginPaytmTransaction</span><span class="params">()</span></span> &#123;</span><br><span class="line">profileViewModel.createNewSubscription()</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Step-6-Generate-and-send-Checksum-from-Our-Server"><a href="#Step-6-Generate-and-send-Checksum-from-Our-Server" class="headerlink" title="Step 6: Generate and send Checksum (from Our Server)"></a>Step 6: Generate and send Checksum (from Our Server)</h3><p>To your project directory add a package named <strong>paytm</strong>.</p><p>Add <code>checksum_tool.rb</code> and <code>encryption_new_pg.rb</code> to the <strong>paytm</strong> package from <a href="https://github.com/Paytm-Payments/Paytm_App_Checksum_Kit_Ruby/tree/master/paytm" target="_blank" rel="noopener">Paytm_App_Checksum_Kit_Ruby</a></p><p>We will be creating order for a PaymentRequest and we follow model heavy approach for business logic. So we added a static method to generate checksum for our order in PaymentRequest model itself.</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PaymentRequest</span> &lt; ApplicationRecord</span></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">self</span>.<span class="title">create_checksum</span><span class="params">(user, order_id)</span></span></span><br><span class="line"><span class="keyword">require</span> <span class="string">'./paytm/encryption_new_pg.rb'</span></span><br><span class="line"><span class="keyword">require</span> <span class="string">'./paytm/checksum_tool.rb'</span></span><br><span class="line"><span class="keyword">require</span> <span class="string">'uri'</span></span><br><span class="line"></span><br><span class="line">paytm_hash = Hash.new</span><br><span class="line"></span><br><span class="line">is_staging = <span class="string">'true'</span> == ENV[<span class="string">'PAYTM_STAGING'</span>]</span><br><span class="line">merchant_id = is_staging ? ENV[<span class="string">'STAGING_PAYTM_MERCHANT_ID'</span>] : ENV[<span class="string">'PAYTM_MERCHANT_ID'</span>]</span><br><span class="line">industry_type = is_staging ? ENV[<span class="string">'STAGING_PAYTM_INDUSTRY_TYPE'</span>] : ENV[<span class="string">'PAYTM_INDUSTRY_TYPE'</span>]</span><br><span class="line">paytm_website = is_staging ? ENV[<span class="string">'STAGING_PAYTM_WEBSITE'</span>] : ENV[<span class="string">'PAYTM_WEBSITE'</span>]</span><br><span class="line">paytm_callback = is_staging ? ENV[<span class="string">'STAGING_PAYTM_CALLBACK'</span>] : ENV[<span class="string">'PAYTM_CALLBACK'</span>]</span><br><span class="line"></span><br><span class="line">paytm_hash[<span class="string">"REQUEST_TYPE"</span>] = <span class="string">'DEFAULT'</span></span><br><span class="line">paytm_hash[<span class="string">"MID"</span>] = merchant_id <span class="comment">#Provided by Paytm</span></span><br><span class="line">paytm_hash[<span class="string">"ORDER_ID"</span>] = order_id; <span class="comment">#unique OrderId for every request\</span></span><br><span class="line">paytm_hash[<span class="string">"CUST_ID"</span>] = user.id.to_s <span class="comment">#unique customer identifier</span></span><br><span class="line">paytm_hash[<span class="string">"INDUSTRY_TYPE_ID"</span>] = industry_type <span class="comment">#Provided by Paytm</span></span><br><span class="line">paytm_hash[<span class="string">"CHANNEL_ID"</span>] = <span class="string">'WAP'</span>; <span class="comment">#Provided by Paytm</span></span><br><span class="line">paytm_hash[<span class="string">"TXN_AMOUNT"</span>] = <span class="string">'1'</span>; <span class="comment">#transaction amount</span></span><br><span class="line">paytm_hash[<span class="string">"WEBSITE"</span>] = paytm_website <span class="comment">#Provided by Paytm</span></span><br><span class="line">paytm_hash[<span class="string">"EMAIL"</span>] = user.email; <span class="comment">#customer email id</span></span><br><span class="line"><span class="keyword">if</span> user.phone_number.present?</span><br><span class="line">paytm_hash[<span class="string">"MOBILE_NO"</span>] = user.phone_number; <span class="comment">#customer 10 digit mobile no.</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">paytm_hash[<span class="string">"CALLBACK_URL"</span>] = paytm_callback + <span class="string">"<span class="subst">#&#123;order_id&#125;</span>"</span></span><br><span class="line"></span><br><span class="line">checksum_hash = ChecksumTool.new.get_checksum_hash(paytm_hash).gsub(<span class="string">"\n"</span>, <span class="string">''</span>)</span><br><span class="line">paytm_hash[<span class="string">"CHECKSUMHASH"</span>] = checksum_hash</span><br><span class="line">paytm_hash</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><p>First create a subscription for current user, then create a payment request for that subscription and create checksum treating that payment request as your Order.</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SubscriptionsController</span> &lt; ApplicationController</span></span><br><span class="line">before_action <span class="symbol">:authenticate_user!</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">new</span></span></span><br><span class="line">user = current_user</span><br><span class="line">subscription = user.subscriptions.create!</span><br><span class="line">payment_request = subscription.payment_requests.create!</span><br><span class="line"></span><br><span class="line">checksum = PaymentRequest.create_checksum(user, payment_request.id)</span><br><span class="line">render <span class="symbol">json:</span> &#123; <span class="symbol">paytm_params:</span> checksum, </span><br><span class="line"><span class="symbol">is_staging:</span> <span class="string">'true'</span> == ENV[<span class="string">'PAYTM_STAGING'</span>]&#125;, </span><br><span class="line"><span class="symbol">status:</span> <span class="symbol">:ok</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><h3 id="Step-7-Start-Payment-Transaction"><a href="#Step-7-Start-Payment-Transaction" class="headerlink" title="Step 7: Start Payment Transaction"></a>Step 7: Start Payment Transaction</h3><p>With checksum response from server, initiate the paytm purchase.</p><figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ProfileFragment</span> : <span class="type">Fragment</span></span>() &#123;</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">fun</span> <span class="title">initiatePaytmPurchase</span><span class="params">(checksumResponse: <span class="type">NewSubscriptionResponse</span>)</span></span> &#123;</span><br><span class="line"><span class="keyword">val</span> order = PaytmOrder(checksumResponse.paytmParams)</span><br><span class="line"><span class="keyword">val</span> service = <span class="keyword">if</span> (checksumResponse.isStaging)</span><br><span class="line">PaytmPGService.getStagingService(<span class="literal">null</span>)</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">PaytmPGService.getProductionService()</span><br><span class="line"></span><br><span class="line">service.initialize(order, <span class="literal">null</span>)</span><br><span class="line"></span><br><span class="line">service.startPaymentTransaction(context, <span class="literal">true</span>, <span class="literal">true</span>, <span class="keyword">object</span> : PaytmPaymentTransactionCallback &#123;</span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">fun</span> <span class="title">onTransactionResponse</span><span class="params">(inResponse: <span class="type">Bundle</span>?)</span></span> &#123;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">fun</span> <span class="title">clientAuthenticationFailed</span><span class="params">(inErrorMessage: <span class="type">String</span>?)</span></span> &#123;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">fun</span> <span class="title">someUIErrorOccurred</span><span class="params">(inErrorMessage: <span class="type">String</span>?)</span></span> &#123;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">fun</span> <span class="title">onTransactionCancel</span><span class="params">(inErrorMessage: <span class="type">String</span>?, inResponse: <span class="type">Bundle</span>?)</span></span> &#123;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">fun</span> <span class="title">networkNotAvailable</span><span class="params">()</span></span> &#123;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">fun</span> <span class="title">onErrorLoadingWebPage</span><span class="params">(iniErrorCode: <span class="type">Int</span>, inErrorMessage: <span class="type">String</span>?, inFailingUrl: <span class="type">String</span>?)</span></span> &#123;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">fun</span> <span class="title">onBackPressedCancelTransaction</span><span class="params">()</span></span> &#123;</span><br><span class="line">&#125;</span><br><span class="line">&#125;)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="merchant_pg_android.gif" alt="Demo of Paytm checkout flow in app"></p><h3 id="Step-8-Send-Payment-Response-to-Server"><a href="#Step-8-Send-Payment-Response-to-Server" class="headerlink" title="Step 8: Send Payment Response to Server"></a>Step 8: Send Payment Response to Server</h3><p>On Transaction response from paytm payments Activity, send the response to your server to update payment status.</p><figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">interface</span> <span class="title">OurService</span> </span>&#123;</span><br><span class="line"><span class="meta">@POST(<span class="meta-string">"/payment_requests/&#123;requestId&#125;/update_status"</span>)</span></span><br><span class="line"><span class="keyword">suspend</span> <span class="function"><span class="keyword">fun</span> <span class="title">updatePaymentStatus</span><span class="params">(<span class="meta">@Path(value = <span class="meta-string">"requestId"</span>)</span> requestId: <span class="type">Int</span>,</span></span></span><br><span class="line"><span class="function"><span class="params"><span class="meta">@Body</span> transactionResponse: <span class="type">JsonObject</span>)</span></span>: UpdatePaymentResponse</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UserRepository</span></span>(<span class="keyword">private</span> <span class="keyword">val</span> ourService: OurService) &#123;</span><br><span class="line"><span class="keyword">suspend</span> <span class="function"><span class="keyword">fun</span> <span class="title">updatePaymentResponse</span><span class="params">(requestId: <span class="type">Int</span>, transactionResponse: <span class="type">JsonObject</span>)</span></span>: UpdatePaymentResponse = ourService.updatePaymentStatus(requestId, transactionResponse)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">open</span> <span class="class"><span class="keyword">class</span> <span class="title">ActivityState</span></span></span><br></pre></td></tr></table></figure><figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ProfileViewModel</span></span>(<span class="keyword">private</span> <span class="keyword">val</span> userRepository: UserRepository) : ViewModel() &#123;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">val</span> _paytmState: MutableLiveData&lt;ActivityState&gt; = MutableLiveData(InitialState)</span><br><span class="line"><span class="keyword">val</span> paytmState: LiveData&lt;ActivityState&gt; = _paytmState</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">fun</span> <span class="title">updatePaymentStatus</span><span class="params">(requestId: <span class="type">Int</span>, transactionResponse: <span class="type">JsonObject</span>)</span></span> &#123;</span><br><span class="line">viewModelScope.launch &#123;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">_paytmState.value = ProgressState</span><br><span class="line"><span class="keyword">val</span> response = userRepository.updatePaymentResponse(requestId, transactionResponse)</span><br><span class="line">_paytmState.value = PaytmStatusState(response)</span><br><span class="line">_paytmState.value = PaytmIdleState</span><br><span class="line">&#125; <span class="keyword">catch</span> (e: HttpException) &#123;</span><br><span class="line">_paytmState.value = ErrorState(e)</span><br><span class="line">&#125; <span class="keyword">catch</span> (e: IOException) &#123;</span><br><span class="line">_paytmState.value = ErrorState(e)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">object</span> ProgressState : ActivityState()</span><br><span class="line"><span class="keyword">data</span> <span class="class"><span class="keyword">class</span> <span class="title">PaytmStatusState</span></span>(<span class="keyword">val</span> updatePaymentResponse: UpdatePaymentResponse) : ActivityState()</span><br><span class="line"><span class="keyword">object</span> PaytmIdleState : ActivityState()</span><br><span class="line"><span class="keyword">data</span> <span class="class"><span class="keyword">class</span> <span class="title">ErrorState</span></span>(<span class="keyword">val</span> exception: Exception) : ActivityState()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ProfileFragment</span> : <span class="type">Fragment</span></span>() &#123;</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">fun</span> <span class="title">onTransactionResponse</span><span class="params">(inResponse: <span class="type">Bundle</span>?)</span></span> &#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> orderId = inResponse?.getString(<span class="string">"ORDERID"</span>)</span><br><span class="line">orderId?.let &#123;</span><br><span class="line"><span class="keyword">val</span> responseJson = JsonObject()</span><br><span class="line">inResponse.keySet()?.forEach &#123;</span><br><span class="line">responseJson.addProperty(it, inResponse.getString(it))</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">val</span> responseJsonWrapper = JsonObject()</span><br><span class="line">responseJsonWrapper.add(<span class="string">"gateway_response"</span>, responseJson)</span><br><span class="line"></span><br><span class="line">profileViewModel.updatePaymentStatus(Integer.parseInt(orderId), responseJsonWrapper)</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Step-9-Confirm-with-Paytm-Gateway-about-payment-status"><a href="#Step-9-Confirm-with-Paytm-Gateway-about-payment-status" class="headerlink" title="Step 9: Confirm with Paytm Gateway about payment status"></a>Step 9: Confirm with Paytm Gateway about payment status</h3><p>Confirm with Paytm gateway using <a href="https://developer.paytm.com/docs/transaction-status-api/" target="_blank" rel="noopener">Transaction Status API</a>.</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PaymentRequest</span> &lt; ApplicationRecord</span></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">confirm_with_gateway</span><span class="params">(user)</span></span></span><br><span class="line">is_staging = <span class="string">'true'</span> == ENV[<span class="string">'PAYTM_STAGING'</span>]</span><br><span class="line">status_api_url = is_staging ? ENV[<span class="string">'STAGING_PAYTM_STATUS'</span>] : ENV[<span class="string">'PAYTM_STATUS'</span>]</span><br><span class="line">merchant_id = is_staging ? ENV[<span class="string">'STAGING_PAYTM_MERCHANT_ID'</span>] : ENV[<span class="string">'PAYTM_MERCHANT_ID'</span>]</span><br><span class="line">order_id = <span class="keyword">self</span>.id</span><br><span class="line"></span><br><span class="line">response = HTTParty.post(status_api_url,</span><br><span class="line"><span class="symbol">body:</span> &#123;</span><br><span class="line"><span class="symbol">MID:</span> merchant_id,</span><br><span class="line"><span class="symbol">ORDERID:</span> order_id,</span><br><span class="line"><span class="symbol">CHECKSUMHASH:</span> PaymentRequest.create_checksum(user, order_id)[<span class="string">"CHECKSUMHASH"</span>]</span><br><span class="line">&#125;.to_json,</span><br><span class="line"><span class="symbol">multipart:</span> <span class="literal">false</span>,</span><br><span class="line"><span class="symbol">headers:</span> &#123;</span><br><span class="line"><span class="string">'Content-Type'</span> =&gt; <span class="string">'application/json'</span></span><br><span class="line">&#125;,</span><br><span class="line"><span class="symbol">timeout:</span> <span class="number">10000</span>)</span><br><span class="line">update_status(response) <span class="comment"># We will learn about this in next step</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><p>After confirming with gateway, send back the response to App.</p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PaymentRequestsController</span> &lt; ApplicationController</span></span><br><span class="line">before_action <span class="symbol">:authenticate_user!</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_status</span></span></span><br><span class="line">payment_request = PaymentRequest.find(params[<span class="symbol">:id</span>])</span><br><span class="line">payment_request.initiate! <span class="comment">#We are using aasm gem for this https://github.com/aasm/aasm</span></span><br><span class="line">payment_request.confirm_with_gateway(current_user)</span><br><span class="line">payment_request.reload</span><br><span class="line"></span><br><span class="line">render <span class="symbol">json:</span> &#123; <span class="symbol">message:</span> get_status_message(payment_request), </span><br><span class="line"><span class="symbol">status:</span> payment_request.aasm_state&#125;, </span><br><span class="line"><span class="symbol">status:</span> <span class="symbol">:ok</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_status_message</span><span class="params">(payment_request)</span></span></span><br><span class="line"><span class="keyword">case</span> payment_request.aasm_state</span><br><span class="line"><span class="keyword">when</span> <span class="symbol">:gateway_confirmation_pending</span></span><br><span class="line"><span class="string">'Payment is still under process, please wait until the status of transaction is updated'</span></span><br><span class="line"><span class="keyword">when</span> <span class="symbol">:success</span></span><br><span class="line"><span class="string">'Payment was successful'</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="string">'Payment has failed'</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><h3 id="Step-10-Update-Order-Status"><a href="#Step-10-Update-Order-Status" class="headerlink" title="Step 10: Update Order Status"></a>Step 10: Update Order Status</h3><p>Based on response codes, update the payment status of order. Response codes and statuses can be found <a href="https://developer.paytm.com/docs/transaction-status-api/" target="_blank" rel="noopener">Transaction Status API’s Response codes and Messages section</a> and <a href="https://developer.paytm.com/assets/Transaction%20response%20codes%20and%20messages.pdf" target="_blank" rel="noopener">Transaction response codes and messages</a></p><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PaymentRequest</span> &lt; ApplicationRecord</span></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">private</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_status</span><span class="params">(response)</span></span></span><br><span class="line">response = response.symbolize_keys</span><br><span class="line">response_code = response[<span class="symbol">:RESPCODE</span>]</span><br><span class="line"><span class="keyword">if</span> response_code == <span class="string">"01"</span></span><br><span class="line"><span class="keyword">self</span>.update(<span class="symbol">transaction_reference:</span> response[<span class="symbol">:TXNID</span>], <span class="symbol">metadata:</span> response)</span><br><span class="line"><span class="keyword">self</span>.mark_as_succeed!</span><br><span class="line"><span class="keyword">elsif</span> response_code == <span class="string">"400"</span> <span class="params">||</span> response_code == <span class="string">"402"</span></span><br><span class="line"><span class="keyword">elsif</span> response_code == <span class="string">"294"</span></span><br><span class="line"><span class="keyword">self</span>.mark_as_expired!</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="keyword">self</span>.update(<span class="symbol">transaction_reference:</span> response[<span class="symbol">:TXNID</span>], <span class="symbol">error_message:</span> response[<span class="symbol">:RESPMSG</span>], <span class="symbol">metadata:</span> response)</span><br><span class="line"><span class="keyword">self</span>.mark_as_failed!</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><h3 id="Step-11-Show-Order-Status-on-App"><a href="#Step-11-Show-Order-Status-on-App" class="headerlink" title="Step 11: Show Order Status on App"></a>Step 11: Show Order Status on App</h3><p>Based on server response, show messages on UI.</p><figure class="highlight kotlin"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ProfileFragment</span> : <span class="type">Fragment</span></span>() &#123;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">val</span> paytmStateObserver: Observer&lt;ActivityState&gt; = Observer &#123;</span><br><span class="line"><span class="keyword">when</span> (it) &#123;</span><br><span class="line"><span class="keyword">is</span> ProfileViewModel.PaytmStatusState -&gt; &#123;</span><br><span class="line"><span class="keyword">when</span> (it.updatePaymentResponse.status) &#123;</span><br><span class="line"><span class="string">"success"</span> -&gt; &#123;</span><br><span class="line">Snackbar.make(progressBar, <span class="string">"Payment success"</span>, Snackbar.LENGTH_LONG).show()</span><br><span class="line">&#125;</span><br><span class="line"><span class="string">"failed"</span> -&gt; &#123;</span><br><span class="line">Snackbar.make(progressBar, <span class="string">"Payment failed. <span class="subst">$&#123;it.updatePaymentResponse.message&#125;</span>"</span>, Snackbar.LENGTH_LONG).show()</span><br><span class="line">&#125;</span><br><span class="line"><span class="string">"expired"</span> -&gt; &#123;</span><br><span class="line">Snackbar.make(progressBar, <span class="string">"Payment expired. Please try again"</span>, Snackbar.LENGTH_LONG).show()</span><br><span class="line">&#125;</span><br><span class="line"><span class="string">"gateway_confirmation_pending"</span> -&gt; &#123;</span><br><span class="line">Snackbar.make(progressBar, <span class="string">"Payment pending. <span class="subst">$&#123;it.updatePaymentResponse.message&#125;</span>"</span>, Snackbar.LENGTH_LONG).show()</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span> -&gt; &#123;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="References"><a href="#References" class="headerlink" title="References:-"></a>References:-</h2><ol><li><a href="https://developer.paytm.com/docs/v1/android-sdk/" target="_blank" rel="noopener">Add payments to your Android app with Paytm SDK</a></li><li><a href="https://github.com/Paytm-Payments/Paytm_App_Checksum_Kit_Ruby" target="_blank" rel="noopener">Paytm_App_Checksum_Kit_Ruby</a></li><li><a href="https://medium.com/the-zalonin/android-payment-gateway-integration-guide-paytm-fa2ee01286e" target="_blank" rel="noopener">Android Payment Gateway Integration Guide: PAYTM</a></li><li><a href="https://developer.paytm.com/docs/transaction-status-api/" target="_blank" rel="noopener">Transaction Status API</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Android </category>
          
          <category> Ruby on Rails </category>
          
      </categories>
      
      
        <tags>
            
            <tag> android </tag>
            
            <tag> paytm </tag>
            
            <tag> gateway </tag>
            
            <tag> payment </tag>
            
            <tag> backend </tag>
            
            <tag> rails </tag>
            
            <tag> ror </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>How to make a web application for any AI model</title>
      <link href="/blog//2019/12/03/How-to-make-a-web-application-for-any-AI-model/"/>
      <url>/blog//2019/12/03/How-to-make-a-web-application-for-any-AI-model/</url>
      
        <content type="html"><![CDATA[<p>Yes! you read the title right. So, today in this post I’ll show you how to setup a basic image-classifier in the form of a web application.</p><h2 id="Basic-requirements"><a href="#Basic-requirements" class="headerlink" title="Basic requirements"></a>Basic requirements</h2><p>Before we getting dive more into it, I am listing down the basic ingredients which are required to make a web application in python.</p><ul><li><p>Flask</p></li><li><p>Flask Bootstrap</p></li></ul><p>Please note that, I am not showing about how to create an AI classifier model, so make sure you have your classifier already before seeking into this post, if not then you can download a pre-trained model.</p><p>Let’s get started !</p><h2 id="Installation"><a href="#Installation" class="headerlink" title="Installation"></a>Installation</h2><p>You need to have installed the above mentioned libraries. You can easily install them by using <strong>pip</strong>.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install flask</span><br><span class="line">pip install flask-bootstrap</span><br></pre></td></tr></table></figure><h2 id="Getting-started"><a href="#Getting-started" class="headerlink" title="Getting started"></a>Getting started</h2><p>So, firstly we arrange our files and folders in the below shown order.</p><p><img src="https://cdn-images-1.medium.com/max/2000/1*UcQSsTBtUdYL4x-HffjTNg.png" alt="Figure 1: Showing basic folder structure for the project"></p><p>You can change the main folder name <em>image-classifier (In my case)</em> to any other name as you like.</p><p>So, firstly we will write a basic flask app structure in <code>__init__.py</code>. This file can be found inside the classifier. Inside the <code>__init__.py</code> file write the below code</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">from flask import Flask</span><br><span class="line">from flask_bootstrap import Bootstrap</span><br><span class="line">from classifier import routes</span><br><span class="line"></span><br><span class="line">app = Flask(__name__) ## defining our flask application</span><br><span class="line">Bootstrap(app) ## giving a nice bootstrap touch to our application</span><br></pre></td></tr></table></figure><p>Now, writing our run.py file. You can find this run.py file inside the main folder. Open the file and write the below code</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">from classifier import app as  application</span><br><span class="line">application.config.from_pyfile(&apos;config/config.py&apos;)  </span><br><span class="line"></span><br><span class="line">if __name__ == &quot;__main__&quot;:</span><br><span class="line">    application.run(host=&apos;0.0.0.0&apos;, port=8000)  ## This tells our</span><br><span class="line">      ##application will run on this host and on this port.</span><br></pre></td></tr></table></figure><p>Now, we will create write a config.py file. This file contains the configuration for the application. The basic configuration we can put now is that, we can just put our application in DEBUG mode. So, open the config.py file and write the below line.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">from os.path import join, dirname, realpath</span><br><span class="line"></span><br><span class="line">DEBUG = True</span><br><span class="line">## If True, then it refresh the server after making any changes to the code.</span><br><span class="line">UPLOAD_FOLDER = join(dirname(realpath(__file__)), &apos;uploaded_images/&apos;)</span><br></pre></td></tr></table></figure><p>Make a <strong>uploaded_images</strong> folder inside the config folder, this folder contains the image which will be uploaded on the server via user.</p><h2 id="Writing-a-basic-route-for-our-flask-app"><a href="#Writing-a-basic-route-for-our-flask-app" class="headerlink" title="Writing a basic route for our flask app"></a>Writing a basic route for our flask app</h2><p>We have completely setup our basic flask environment. Now, its time to write a very basic flask api for hello world. Open the routes.py inside the classifier folder. Add the below lines to it</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">from classifier import app</span><br><span class="line">import flask</span><br><span class="line"></span><br><span class="line">@app.route(&apos;/testing&apos;)</span><br><span class="line">def testing():</span><br><span class="line">    return &quot;&lt;h1&gt;Hello world&lt;/h1&gt;&quot;</span><br></pre></td></tr></table></figure><p>Here, the <strong>route()</strong> function of the Flask class is a decorator, which tells the application which URL should call the associated function.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">app.route(rule, options)</span><br></pre></td></tr></table></figure><ul><li><p>The <strong>rule</strong> parameter represents URL binding with the function.</p></li><li><p>The <strong>options</strong> is a list of parameters to be forwarded to the underlying Rule object.</p></li></ul><p>and in the end we just returned a simple message using some HTML tags.</p><p>To run this code, follows below steps.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd path/to/main-folder</span><br><span class="line">python run.py</span><br></pre></td></tr></table></figure><p><img src="https://cdn-images-1.medium.com/max/2000/1*kJBAht4yhvPymR1dyV9Eow.png" alt="Figure 2: After running the above command you should see above like messages"></p><p>Just open the browser and type localhost:8000/testing. You should see a screen just like below</p><p><img src="https://cdn-images-1.medium.com/max/2000/1*vLAQroEaj-4mJwLDFe4Tlg.png" alt="Figure 3: This is the route which we wrote for “/testing”"></p><h2 id="Lets-start-our-main-route"><a href="#Lets-start-our-main-route" class="headerlink" title="Lets start our main route"></a>Lets start our main route</h2><p>Lets make our template. For this, firstly makes a folder called <strong>template</strong> inside the <strong>classifier</strong> folder. Now inside the template, create a home.html file and paste the code from this <a href="https://raw.githubusercontent.com/hghimanshu/Blog/master/image-classifier/classifier/templates/home.html" target="_blank" rel="noopener">link</a> in it.</p><p>Now, we make our home url and our backend part !!. So we will rewrite our routes.py file</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">@app.route(&apos;/home&apos;)</span><br><span class="line">def home():</span><br><span class="line">    return render_template(&apos;home.html&apos;)  ## The template which we created above</span><br></pre></td></tr></table></figure><p>Now, our image processing function will be like below</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">from flask import render_template</span><br><span class="line">from werkzeug import secure_filename</span><br><span class="line">from classifier.backend.prediction import image_prediction</span><br><span class="line"></span><br><span class="line">@app.route(&quot;/fetchingImage&quot;, methods = [&apos;POST&apos;])</span><br><span class="line">def fetchingImage():</span><br><span class="line">    if flask.request.method == &apos;POST&apos;</span><br><span class="line">        image = flask.request.files[&apos;image&apos;]</span><br><span class="line">        image.save(app.config[&apos;UPLOAD_FOLDER&apos;] + secure_filename(image.filename))</span><br><span class="line">        full_img = app.config[&apos;UPLOAD_FOLDER&apos;] + image.filename</span><br><span class="line">        data = image_prediction(full_img)</span><br><span class="line">        if len(data)==2:</span><br><span class="line">            return render_template(&apos;prediction.html&apos;, results = data)</span><br><span class="line">        else:</span><br><span class="line">            return render_template(&apos;error.html&apos;, results = data)</span><br></pre></td></tr></table></figure><p>Here, firstly we get the image from the form in the <strong>home.html</strong>, then save the image into the system and then fetch that file and send it to another function <em>image_prediction</em> for processing. Then, we simply render the response from the model to the webpage. If there is no error, then we display the <strong>prediction.html</strong> template or else, we render the <strong>error.html</strong>. Now, working on our <em>image_prediction</em> function.</p><p>For making the this, firstly create a folder named <strong>backend</strong> inside the <em>classifier</em> folder, then inside the <strong>backend</strong> folder, create a <strong>prediction.py</strong> file and write the below code into it.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">import keras</span><br><span class="line">import numpy as np</span><br><span class="line">import tensorflow as tf</span><br><span class="line">import cv2</span><br><span class="line">from keras.models import load_model</span><br><span class="line">from keras.applications.vgg19 import VGG19</span><br><span class="line">from keras.applications.vgg19 import decode_predictions</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def image_prediction(image):</span><br><span class="line">    MODEL = VGG19()</span><br><span class="line">    try:</span><br><span class="line">        image = cv2.imread(image)</span><br><span class="line">        image = cv2.resize(image, (224, 224))</span><br><span class="line">        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))</span><br><span class="line">        yhat = MODEL.predict(image)</span><br><span class="line">        label = decode_predictions(yhat)</span><br><span class="line">        label = label[0][0]</span><br><span class="line">        label, conf = label[1], label[2]*100</span><br><span class="line">        results = [label, conf]</span><br><span class="line">    except Exception as e:</span><br><span class="line">        results = &quot;Please check the image.&quot;</span><br><span class="line">    return results</span><br></pre></td></tr></table></figure><p>Now, let’s make our <strong>error.html</strong> and <strong>prediction.html</strong> templates. These templates are also, saved inside the <em>templates</em> folder. So, you can get the code for both the templates from <a href="https://github.com/hghimanshu/Blog/tree/master/image-classifier/classifier/templates" target="_blank" rel="noopener">here</a>.</p><p>Well the coding part is mostly done, now we will test our web application. Now open your console and run the below command</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python run.py</span><br></pre></td></tr></table></figure><p>After this, it will firstly download the VGG19 pretrained weights <em>(if you are following my code.)</em>, then it will start the server. Now, open the browser and go to <strong>localhost:8000/home</strong>, you will see something like this</p><p><img src="https://cdn-images-1.medium.com/max/2454/1*D8SVbfL_3Mdj_00zhNFSuQ.png" alt="Figure 4: Our classifier’s home page"></p><p>Now, click on browse to upload any image and click the <strong>predict</strong> button. You’ll see some message like this (based on your image)</p><p><img src="https://cdn-images-1.medium.com/max/2000/1*IH_y0QAKul0HqqtZxboYrA.png" alt="Figure 5: Our Image Classification result"></p><p>If there is any some issue with your image, then below screen will appear</p><p><img src="https://cdn-images-1.medium.com/max/2000/1*ZHXZWF3g6hnKU7cSvhknuA.png" alt="Figure 6: Error message"></p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>So, this is how we can make a very basic web application for our image classifier. You can also find the whole code from my <a href="https://github.com/hghimanshu/Blog/tree/master/image-classifier" target="_blank" rel="noopener">github</a>.</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><p><a href="https://www.tutorialspoint.com/flask/index.htm" target="_blank" rel="noopener">https://www.tutorialspoint.com/flask/index.htm</a></p></li><li><p><a href="https://www.w3schools.com/bootstrap/bootstrap_templates.asp" target="_blank" rel="noopener">https://www.w3schools.com/bootstrap/bootstrap_templates.asp</a></p></li><li><p><a href="https://pythonprogramming.net/flask-send-file-tutorial/" target="_blank" rel="noopener">https://pythonprogramming.net/flask-send-file-tutorial/</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Image-Classification </tag>
            
            <tag> Keras </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Drawing Lines on Images</title>
      <link href="/blog//2019/12/02/Images/"/>
      <url>/blog//2019/12/02/Images/</url>
      
        <content type="html"><![CDATA[<p>You can draw different types of line on Canvas in Android. You can change it’s color, stroke, effect, etc. You can redo or undo lines , can select and delete them. Here we will see the basics of drawing line on Canvas.</p><h2 id="Let’s-get-started"><a href="#Let’s-get-started" class="headerlink" title="Let’s get started"></a>Let’s get started</h2><h3 id="Step-1-Add-dependencies"><a href="#Step-1-Add-dependencies" class="headerlink" title="Step -1: Add dependencies"></a>Step -1: Add dependencies</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">// PhotoView</span><br><span class="line">    implementation &apos;com.github.chrisbanes:PhotoView:2.3.0&apos;</span><br><span class="line">//Glide</span><br><span class="line">    implementation &apos;com.github.bumptech.glide:glide:4.9.0&apos;</span><br><span class="line">    annotationProcessor &apos;com.github.bumptech.glide:compiler:4.9.0&apos;</span><br><span class="line">// Timber for logging</span><br><span class="line">    implementation &quot;com.jakewharton.timber:timber:$&#123;rootProject.ext.timber_version&#125;&quot;</span><br><span class="line">// Coroutines</span><br><span class="line">    implementation &quot;org.jetbrains.kotlinx:kotlinx-coroutines-core:$&#123;rootProject.ext.coroutines_version&#125;&quot;</span><br><span class="line">    implementation &quot;org.jetbrains.kotlinx:kotlinx-coroutines-android:$&#123;rootProject.ext.coroutines_version&#125;&quot;</span><br><span class="line">    implementation &quot;androidx.lifecycle:lifecycle-viewmodel-ktx:$&#123;rootProject.ext.lifecycle_extensions_version&#125;&quot;</span><br><span class="line">    implementation &quot;androidx.lifecycle:lifecycle-runtime-ktx:$&#123;rootProject.ext.lifecycle_extensions_version&#125;&quot;</span><br><span class="line">    implementation &quot;androidx.lifecycle:lifecycle-livedata-ktx:$&#123;rootProject.ext.lifecycle_extensions_version&#125;&quot;</span><br></pre></td></tr></table></figure><h3 id="Step-2-Create-bitmap-of-Image-and-canvas-to-draw"><a href="#Step-2-Create-bitmap-of-Image-and-canvas-to-draw" class="headerlink" title="Step -2: Create bitmap of Image and canvas to draw"></a>Step -2: Create bitmap of Image and canvas to draw</h3><p>The first step is to create a bitmap of Image on which you want to draw lines. We are using <a href="https://github.com/bumptech/glide" target="_blank" rel="noopener" title="Glide">Glide</a> to download image and to get bitmap from it.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">private suspend fun getOriginalBitmapFromUrl(): Bitmap =</span><br><span class="line">    withContext(Dispatchers.IO) &#123;</span><br><span class="line">        Glide.with(this@MainActivity)</span><br><span class="line">            .asBitmap()</span><br><span class="line">            .load(Constants.IMAGE_URL)</span><br><span class="line">            .submit().get()</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure><h3 id="Step-3-Create-event-listener"><a href="#Step-3-Create-event-listener" class="headerlink" title="Step -3: Create event listener"></a>Step -3: Create event listener</h3><p>Create an interface to handle onTouch events of photoviewattacher.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">fun touchPoints(x: Float, y: Float)</span><br><span class="line">fun closeStroke()</span><br><span class="line">fun discardStroke()</span><br></pre></td></tr></table></figure><h3 id="Step-4-Create-your-overlay-view-attacher"><a href="#Step-4-Create-your-overlay-view-attacher" class="headerlink" title="Step -4: Create your overlay view attacher"></a>Step -4: Create your overlay view attacher</h3><p>Now we create overlayview attacher, where we define onTouch events.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">MotionEvent.ACTION_UP -&gt; &#123;</span><br><span class="line">    if (multiTouch) &#123;</span><br><span class="line">        onPencilDrawListener.discardStroke()</span><br><span class="line">        multiTouch = false</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">        onPencilDrawListener.closeStroke()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">MotionEvent.ACTION_DOWN -&gt; &#123;</span><br><span class="line">    if (!multiTouch &amp;&amp; isSelectModeEnabled) &#123;</span><br><span class="line">        sendEventToListener(ev)</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">        super.onTouch(v, ev)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">MotionEvent.ACTION_MOVE -&gt; &#123;</span><br><span class="line">    if (!multiTouch) &#123;</span><br><span class="line">        sendEventToListener(ev)</span><br><span class="line">    &#125; else &#123;</span><br><span class="line">        super.onTouch(v, ev)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Step-5-Create-your-overlay-photo-view"><a href="#Step-5-Create-your-overlay-photo-view" class="headerlink" title="Step -5: Create your overlay photo view"></a>Step -5: Create your overlay photo view</h3><p>Create your own imageview by extending it from photoview and added some properties</p><h4 id="a-For-selecting-and-deleting-lines"><a href="#a-For-selecting-and-deleting-lines" class="headerlink" title="a. For selecting and deleting lines:"></a>a. For selecting and deleting lines:</h4><p>If the angle at touch point is an obtuse angle and area of triangle formed is under the threshold limit, then the current line is found to be selected</p><h4 id="To-calculate-area"><a href="#To-calculate-area" class="headerlink" title="To calculate area:"></a>To calculate area:</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">abs((touchX * (y1 — y2) + x1 * (y2 — touchY) + x2 * (touchY — y1)) / 2)</span><br></pre></td></tr></table></figure><p>where (x1,y1) and (x2,y2) are endpoints of selected line.</p><h4 id="To-calculate-angle"><a href="#To-calculate-angle" class="headerlink" title="To calculate angle:"></a>To calculate angle:</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">private fun calculateAngle(</span><br><span class="line">    tapPoint: List&lt;Float&gt;,</span><br><span class="line">    linePoint1: List&lt;Float&gt;,</span><br><span class="line">    linePoint2: List&lt;Float&gt;</span><br><span class="line">): Float &#123;</span><br><span class="line">    // Square of lengths be a2, b2, c2</span><br><span class="line">    val a2 = lengthSquare(linePoint1, linePoint2)</span><br><span class="line">    val b2 = lengthSquare(tapPoint, linePoint2)</span><br><span class="line">    val c2 = lengthSquare(tapPoint, linePoint1)</span><br><span class="line"></span><br><span class="line">    // length of sides be b, c</span><br><span class="line">    val b = sqrt(b2)</span><br><span class="line">    val c = sqrt(c2)</span><br><span class="line"></span><br><span class="line">    // From Cosine law</span><br><span class="line">    var alpha = acos((b2 + c2 - a2) / (2f * b * c))</span><br><span class="line"></span><br><span class="line">    // Converting to degree</span><br><span class="line">    alpha = (alpha * 180 / PI).toFloat()</span><br><span class="line"></span><br><span class="line">    return alpha</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="b-For-redo-operation"><a href="#b-For-redo-operation" class="headerlink" title="b. For redo operation:"></a>b. For redo operation:</h4><p>If last undo operation list is not empty, add the first line points from last undo operation list to overlay data list and remove it from there.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">fun redo() &#123;</span><br><span class="line">    if (lastUndoOperation != null) &#123;</span><br><span class="line">        if (lastUndoOperation?.isDeleteOperation == true) &#123;</span><br><span class="line">            val ids = lastUndoOperation?.overlayDataList?.map &#123; overlayData -&gt; overlayData.id &#125;</span><br><span class="line">            if (ids != null) &#123;</span><br><span class="line">                for (data in overlayDataList) &#123;</span><br><span class="line">                    if (data.id in ids) &#123;</span><br><span class="line">                        data.isDeleted = true</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">            lastUndoOperation?.overlayDataList?.first()?.let &#123; overlayDataList.add(it) &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        lastUndoOperation?.let &#123; overlayDataOperations.add(it) &#125;</span><br><span class="line">        lastUndoOperation = null</span><br><span class="line">        invalidate()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="c-For-undo-operation"><a href="#c-For-undo-operation" class="headerlink" title="c. For undo operation:"></a>c. For undo operation:</h4><p>If there is any point present in overlay data list then delete it from there and added it in last undo operation list.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">fun undo() &#123;</span><br><span class="line">    if (overlayDataOperations.size &gt; 0) &#123;</span><br><span class="line">        val index = overlayDataOperations.lastIndex</span><br><span class="line">        lastUndoOperation = overlayDataOperations[index]</span><br><span class="line">        overlayDataOperations.removeAt(index)</span><br><span class="line"></span><br><span class="line">        if (lastUndoOperation?.isDeleteOperation == true) &#123;</span><br><span class="line">            val ids = lastUndoOperation?.overlayDataList?.map &#123; overlayData -&gt; overlayData.id &#125;</span><br><span class="line">            if (ids != null) &#123;</span><br><span class="line">                for (data in overlayDataList) &#123;</span><br><span class="line">                    if (data.id in ids) &#123;</span><br><span class="line">                        data.isDeleted = false</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; else &#123;</span><br><span class="line">            val id = lastUndoOperation?.overlayDataList?.first()?.id</span><br><span class="line">            overlayDataList.removeAt(overlayDataList.indexOfFirst &#123; overlayData -&gt; overlayData.id == id &#125;)</span><br><span class="line">        &#125;</span><br><span class="line">        invalidate()</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>For more sample code , see the <a href="https://github.com/diwakarsinghdiwakar/Draw-Line" target="_blank" rel="noopener" title="Draw Line">Draw-Line</a></p><h2 id="And-we’re-done"><a href="#And-we’re-done" class="headerlink" title="And we’re done!"></a>And we’re done!</h2><h4 id="Screenshots"><a href="#Screenshots" class="headerlink" title="Screenshots:"></a>Screenshots:</h4><img src="/blog/2019/12/02/Images/SC1.jpg" class="" title="Image_1"><img src="/blog/2019/12/02/Images/SC2.jpg" class="" title="Image_2"><img src="/blog/2019/12/02/Images/SC3.jpg" class="" title="Image_3"><img src="/blog/2019/12/02/Images/SC4.jpg" class="" title="Image_4">]]></content>
      
      
      <categories>
          
          <category> Android </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Android Testing Strategy</title>
      <link href="/blog//2019/11/29/Android-Testing-Strategy/"/>
      <url>/blog//2019/11/29/Android-Testing-Strategy/</url>
      
        <content type="html"><![CDATA[<img src="/blog/2019/11/29/Android-Testing-Strategy/testing_strategy.jpeg" class="" title="Testing Strategy"><p>Testing android application is quite hard. There is no set guidelines for us to follow. When ever I started thinking of testing my application I always get confused where to start, should I write unit tests or instrumentation tests and should I start with integration and End to end tests. There is also a lot of confusion on frameworks available for android testing.<br>This article has been written in a sense to address this confusion and show us as developers what kind of testing is most preferred and what frameworks are available.</p><h2 id="Kinds-of-Tests"><a href="#Kinds-of-Tests" class="headerlink" title="Kinds of Tests"></a>Kinds of Tests</h2><h3 id="Unit-Test"><a href="#Unit-Test" class="headerlink" title="Unit Test"></a>Unit Test</h3><p>This is often referred to as local tests and  doesn’t require a device or emulator for running them. These can be classified broadly into categories</p><ul><li>Local/ Pure Unit tests - tests which can run on JVM, mainly written for testing business logic. On android JUnit, Mockito, the mockable Android JARs give us a nice combination of tools to run fast unit-style tests in the local JVM.</li><li>Android  Unit Tests -  tests which  requires the Android system (ART) and for this we need to replace android dependencies using Roboelectric<br>Guidelines</li><li>If you have dependencies on the Android framework, particularly those that create complex interactions with the framework, it’s better to include framework dependencies using Robolectric.</li><li>If your tests have minimal dependencies on the Android framework, or if the tests depend only on your own objects, it’s fine to include mock dependencies using a mocking framework like Mockito and PowerMock.</li></ul><p>Reference Url : <a href="https://developer.android.com/training/testing/unit-testing/local-unit-tests" target="_blank" rel="noopener">https://developer.android.com/training/testing/unit-testing/local-unit-tests</a></p><h3 id="Instrumentation-Tests"><a href="#Instrumentation-Tests" class="headerlink" title="Instrumentation Tests"></a>Instrumentation Tests</h3><p>These tests requires device or an emulator for running. This is mostly used for UI testing but it can be used to test none UI logic as well.</p><ul><li><p>This is useful when you need to test code that has a dependency on context. </p></li><li><p>UI tests can be an essential component of any testing strategy since they can uncover issues related to UI, hardware, firmware, and backwards compatibility</p></li></ul><p>Reference Url: <a href="https://developer.android.com/training/testing/unit-testing/instrumented-unit-tests" target="_blank" rel="noopener">https://developer.android.com/training/testing/unit-testing/instrumented-unit-tests</a></p><h2 id="Testing-Frameworks"><a href="#Testing-Frameworks" class="headerlink" title="Testing Frameworks"></a>Testing Frameworks</h2><h3 id="Android-X-Test-Framework"><a href="#Android-X-Test-Framework" class="headerlink" title="Android X Test Framework"></a>Android X Test Framework</h3><p> It is testing framework and APIs provided  by android team for writing unit tests.</p><h3 id="Roboelectric"><a href="#Roboelectric" class="headerlink" title="Roboelectric"></a>Roboelectric</h3><p>It is a framework that brings fast and reliable unit tests to android. Runs inside JVM or your workstation in seconds. This is usually used to Integration testing. Integration tests validate how your code interacts with other parts of the system but without the added complexity of a UI framework.</p><p>app/src/test/java - for any unit test which can run on the JVM</p><p><strong>Question</strong> : How does it work?<br><strong>Answer</strong> : Unlike traditional emulators-based androids tests, it tests run inside a sandbox which allows the android environment to be precisely configured to the desired conditions for each test. It lets you run your tests on your workstation, or on your continuous integration environment in a regular JVM, without an emulator. It handles inflation of views, resource loading, and lots of other stuff that’s implemented in native C code on Android devices. This allows tests to do most things you could do on a real device. It’s easy to provide  our own implementation for specific SDK methods too, so you could simulate error conditions or real-world sensor behaviour. It allows a test style that is closer to black box testing, making the tests more effective for refactoring and allowing the tests to focus on the behaviour of the application instead of the implementation of Android</p><p><strong>Question</strong> : Why should be prefer this?<br><strong>Answer</strong> : In order for this to run tests it needs regular JVM,  Because of this, the dexing, packaging, and installing-on-the emulator steps aren’t necessary, reducing test cycles from minutes to seconds so you can iterate quickly and refactor your code with confidence. Robolectric executes your code against real (not mock) Android JARs in the local JVM.</p><h3 id="Espresso"><a href="#Espresso" class="headerlink" title="Espresso"></a>Espresso</h3><p> Use it to write concise, beautiful, and reliable Android UI tests. These tests are called Instrumentation tests and unlike unit tests takes more time to run them.</p><p>app/src/androidTest/java - for any instrumentation test which should run on an Android</p><p><strong>Question</strong> : How does it work?<br><strong>Answer</strong> : it requires an emulator or a real device to run tests. At the time of execution along with the main application, A testing application is also installed in the device which controls main application automatically.</p><h3 id="UI-Automator"><a href="#UI-Automator" class="headerlink" title="UI Automator"></a>UI Automator</h3><p>It allows us to write cross application functional tests ( End to End) . Example, Sharing messages via Text intent or sending email via locally installed email clients.</p><h3 id="Monkey-Runner-CL"><a href="#Monkey-Runner-CL" class="headerlink" title="Monkey Runner CL"></a>Monkey Runner CL</h3><p> Monkey is a command line tool which sends pseudo random events to your device. You can restrict Monkey to run only for a certain package and therefore instruct Monkey to test only your application. it can be used for Stress testing for android.</p><p>Reference Url : <a href="https://developer.android.com/studio/test/monkey" target="_blank" rel="noopener">https://developer.android.com/studio/test/monkey</a>            </p><h2 id="Recommandations"><a href="#Recommandations" class="headerlink" title="Recommandations"></a>Recommandations</h2><ul><li><p>Creating test groups - @SmallTest. @MediumTest and @LargeTest annotation allows us to classify tests. Allows you to run, for example, only short running tests for development cycle. You may run your long running tests on a continuous integration server.<br>This can be easily configured this via InstrumentationTestRunner in user build.gradle    (app)</p></li><li><p>We can use a three tiered approach</p><p><strong>Pure Unit tests :</strong>   These can be written for our business logic which are completely android independent of API and can run on JVM. These can be written using Junit Framework. Roboelectric Unit tests: where code has only small dependencies on android APIs and can be easily mocked with Roborelectric.</p><p><strong>Android Instrumentation tests :</strong> where code heavily interact with device hardware, sensors and android APIs. These tests will usually take most time to run.</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Android </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Instrumentation Test </tag>
            
            <tag> Unit Test </tag>
            
            <tag> Android Testing </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Tracking Deep Learning experiments using Keras,MlFlow and MongoDB</title>
      <link href="/blog//2019/11/29/Tracking-Deep-Learning-Experiments-using-Keras-MlFlow-and-MongoDb/"/>
      <url>/blog//2019/11/29/Tracking-Deep-Learning-Experiments-using-Keras-MlFlow-and-MongoDb/</url>
      
        <content type="html"><![CDATA[<img src="/blog/2019/11/29/Tracking-Deep-Learning-Experiments-using-Keras-MlFlow-and-MongoDb/banner.jpeg" class="" title="Banner"><p>It is late 2019 and <strong><em>Deep Learning</em></strong> is not a buzzword anymore. It is significantly used in the technology industry to attain feats of wonders which traditional machine learning and logic based techniques would take a longer time to achieve.</p><p>The main ingredient in Deep Learning are <strong><em>Neural Networks</em></strong>, which are computation units called neurons, connected in a specific fashion to perform the task of learning and understanding data. When these networks become extremely deep and sophisticated, they are referred to as Deep Neural Networks and thus Deep Learning is performed.</p><p>Neural Networks are so called because they are speculated to be imitating the human brain in some manner. Though it is not entirely true, but the learning mechanism is mostly similar in nature.</p><p>A human brain learns about an object or concept when it visually experiences it for a longer amount of time. Similar to that, a neural network learns about objects and what they actually represent when it is fed with a large amount of data.</p><p>For example, let us consider the <strong><em>LeNet architecture</em></strong> . It is a small two layered CNN (Convolution Neural Network). Convolution Neural Networks are a special kind of neural network where the mathematical computation being done at every layer are convolution operations.</p><p>If enough images of a certain kind are fed to the <strong><em>LeNet</em></strong> architecture, it starts to understand and classify those images better.</p><p>That was a simple introduction to what neural networks are and how they behave.</p><p>In this article we will be mostly looking into three main frameworks which can ease out the developer experience of building these neural networks and tracking there performance efficiently.</p><p>Nowadays, neural networks are heavily used for classifying objects, predicting data and other similar tasks by many companies out there. When it comes it to training neural networks and keeping track of their performance, the experience is not too subtle.</p><p>When building a neural network, a developer would be trying out multiple datasets and experimenting with different hyperparameters. It is essential to keep a track a of these parameters and how they affect the output of the neural networks.</p><p>Also debugging neural networks is an extremely cumbersome task. The output performance of different neural networks may vary due to different reasons. Some of the possible causes maybe inadequate data pre-processing, incorrect optimizer, a learning rate which is too low or too high. The number of variables which affect the performance of a neural network are quite a few. Hence it is essential that every parameter is properly tracked and maintained.</p><p>Some of the available options present out there include the infamous <strong>*Tensorboard, Bokeh *</strong>to name a few.</p><p>In this project we will be using <strong>MlFlow</strong> ,an open source platform to manage the entire deep learning development cycle. <strong>MlFlow</strong> allows developers to log and track the outputs of every experiment performed with great precision. We will be looking into <strong>MlFlow’s</strong> components with more detail in the subsequent sections.</p><p>The framework we would be using for writing our neural networks and training them is <strong>Keras</strong>.</p><p>We will be using the <strong>FashionMNIST</strong> dataset. It contains a total of 70000 gray scale images (training:test = 60000:10000) , each scaled at 28x28 associated with one from 10 classes. (Fig 1)</p><p><img src="https://cdn-images-1.medium.com/max/2000/1*vIpkolZZ3jACO529n4QaJg.jpeg" alt="Fig 1: Fashion Mnist Dataset"></p><p>The folder structure of our project looks as shown in Fig 2 below.</p><p><img src="https://cdn-images-1.medium.com/max/2000/1*NaIzdpiaiWAge7khYyRpmQ.png" alt="Fig 2: Project folder structure"></p><p>The data folder contains our <strong><em>fashion mnist</em></strong> dataset files which will be used for training the model. The db folder contains the python driver code to perform operations on <strong><em>MongoDb</em></strong> collections. <strong><em>MongoDB</em></strong> is an extremely easy to use <strong><em>NoSql</em></strong> database which has been built keeping in developer satisfaction. It is easily integrable with modern day applications and has a large developer community contributing to it’s extensions regularly. The model folder contains piece of code with the neural network model definition. The <strong><em>mlruns</em></strong> folder is created automatically once <strong>*mlflow *</strong>is invoked in the main code.</p><p><em>The aim of the project is to track multiple deep learning experiments and check how the outputs are affected when parameters are changed and data is changed.</em> Since we have only one dataset, we will split it into equal parts in order to simulate a multiple dataset scenario.</p><p>Let’s start off with the <strong><em>create_dataset</em></strong> script, which is used to split the fashion mnist into equal parts and store them inside the data folder with proper serial number.</p><p>In Fig 3 shown below, we import fashion mnist from <strong>*keras.datasets *</strong>and perform the necessary normalization step</p><p><img src="https://cdn-images-1.medium.com/max/2452/1*_GWdTw2PkJDBl5HW3-Yjbw.png" alt="Fig 3: Loading Fashion Mnist Dataset from the keras library"></p><p>Next, we write the methods to split the dataset into equal parts and save them with proper incremental serial numbers inside the data folder, inside the root project directory (Fig 4)</p><p><img src="https://cdn-images-1.medium.com/max/2060/1*NVW44h1BejUe25D0SwUYog.png" alt="Fig 4: The large dataset is equally split into 12 equal parts for training"> equal parts for training*</p><p>After this we go ahead and write the necessary driver code to manage our newly created dataset using <strong><em>MongoDb</em></strong>. Some might say that using <strong><em>MongoDb</em></strong> for such a small project might be an overkill, but personally I find <em>MongoDb</em> to be an excellent tool for managing data with flexibility. Given <strong><em>NoSql</em></strong>’s schema-less nature, managing collections and documents is a breeze.</p><p>The ease with which any document can be edited in <strong><em>MongoDB</em></strong> is superb. The best part is, whenever a collection is queried , the result returned is a <strong>*json *</strong>making it extremely easy to be parsed using any programming language. Aggregation queries in mongo are also very simple and allows users to cross reference collections in a swift manner.</p><p>In order to connect our python scripts with MongoDb we will be using <strong>pymongo</strong> which can be easily installed using the <strong><em>pip install pymongo</em></strong>. To install MongoDb, follow this tutorial <a href="https://docs.mongodb.com/manual/tutorial/install-mongodb-on-ubuntu/" target="_blank" rel="noopener">***https://docs.mongodb.com/manual/tutorial/install-mongodb-on-ubuntu/</a>***</p><p>Once MongoDb is installed and tested to be running properly on your system, create a new database called fashion_mnist. Inside the database create a new collection named dataset as shown in Fig 5 below.</p><p><img src="https://cdn-images-1.medium.com/max/2000/0*o3BTUrrPN6AeIogC" alt="Fig 5: Collection named as Dataset created in MongoDb"></p><p>A great GUI to interact with <strong><em>MongoDb</em></strong> is <strong><em>robo3t</em></strong>. It’s free and easy to use. It can be downloaded from the following link <a href="https://robomongo.org/download" target="_blank" rel="noopener">***https://robomongo.org/download</a>.***Since our DB is setup and datasets are created, we can progress with the task of inserting necessary information into the dataset collection</p><p><img src="https://cdn-images-1.medium.com/max/2000/1*ZRvtaMxnM8cP1tyeHtvUDA.png" alt="Fig 6: MongoClient is configured in db_ops python script"></p><p>In Fig 6 shown above, we are importing <strong><em>MongoClient</em></strong> from the <strong><em>pymongo</em></strong> library which will essentially be used to connect to our <strong><em>mongoDB</em></strong> database.</p><p>Fig 7 below describes <strong><em>mongoQueue</em></strong> class which has been written in order to interact with our dataset collection. In line 18 and 19, the collection name is initialized, which is used in all the member functions. The <strong>Enqueue*</strong> <em>method in line 6 is used to insert dataset information into the dataset collection. The <strong>*Dequeue</strong> <em>method in line 10 fetches the first dataset which has a <em>status <em>field of ‘</em></em></em>Not** <strong>*Processed</strong>’</em>.* The <strong>setAsProcessing*</strong> and <strong><em>setAsProcessed</em></strong> *methods are used to set the status field of respective dataset documents in the collection.</p><p><img src="https://cdn-images-1.medium.com/max/3824/1*fI-pJh6D3dgLZJ5CfzNUow.png" alt="Fig 7: MongoQueue class for handling operations on the database"></p><p><img src="https://cdn-images-1.medium.com/max/4096/1*YKzl3umNaRPxJsbSK8Q1lg.png" alt="Fig 8: Methods to insert data into MongoDB"><em>Fig 8: Methods to insert data into MongoDB</em></p><p>We use the <strong><em>insert_into_db</em></strong> method shown in Fig 8, line 1, to insert information about our newly created datasets into our <strong>mongoDb</strong> dataset collection. In line 23 of the <em>main</em> function, we iterate over the dataset folder and call <strong>*insert_into_db</strong> *to insert the necessary information for that dataset into the collection. Once every dataset is successfully inserted into the collection, the fields appear as shown in Fig 9 below.</p><p><img src="https://cdn-images-1.medium.com/max/2452/0*qbkKHLQgdPkW-UNp" alt="Fig 9: Status of document in Dataset collection after data insertion"></p><p>We can now define our model for training our deep learning network. Inside <strong>*model/model.py</strong> <em>we import all necessary **</em>keras*** packages to build our CNN network (shown in Fig 10a)</p><p><img src="https://cdn-images-1.medium.com/max/2312/1*XnED-4kpiekoGLMgVMRESA.png" alt="Fig 10a: Importing all necessary keras packages"></p><p>Fig 10b shows the model architecture. It is a simple two layer CNN, with two <strong>MaxPool</strong> layers and <strong>RelU</strong> activation in between. Two <strong>Dense</strong> layers are also added with 32 and 10 neurons respectively. I have also added a <strong>Dropout</strong> of 0.5 before the last Dense layer.</p><p><img src="https://cdn-images-1.medium.com/max/4096/1*lmfvVEc3LxgxqeCGCCD-MQ.png" alt="Fig 10b: CNN model definintion"></p><p>Now, in our <strong><em>train.py</em></strong> script we import all the necessary modules needed from the <strong><em>keras</em></strong> library to get on with our training. Along with all the <strong><em>keras</em></strong> libraries we import <strong>*mlflow *</strong>as well (Fig 11)</p><p><img src="https://cdn-images-1.medium.com/max/2000/1*roE7f-YhPmBl5h9l1j_IMQ.png" alt="Fig 11: Importing packages in the main script"></p><p>All the hyperparameters which will be used for training is stored in a config file named as <strong><em>train_config.json</em></strong>. This file is read (Fig 12a) and used for defining training parameters</p><p><img src="https://cdn-images-1.medium.com/max/2000/1*A1cPbWr1ggscrwGyKNo-kw.png" alt="Fig 12a: parameters required for training are loaded"></p><p>In Fig 12b, we have defined out training function , which takes arguments <strong>*trainX</strong> (<em>our training set) <em>,*</em>trainY** (*training set labels</em>) <em>and the **</em>model** *(CNN model)</p><p><img src="https://cdn-images-1.medium.com/max/3040/1*aq-dCVGk-s4nBAg6KLOxPg.png" alt="Fig 12b: Function to start training"></p><p>From line 38 (In Fig 13), the <strong>main</strong> function starts where we define our <strong>*MongoQueue</strong> <em>object using the **</em>MongoQueue** <em>class defined inside script mentioned before, **</em>db_ops.py <strong>. *As it can be seen in line 41, *</strong>mq*** is our object.</p><p><img src="https://cdn-images-1.medium.com/max/2396/1*jbtG75-OOC9yLrlAHJ-A2Q.png" alt="Fig 13: Main function which trains and also logs data using MlFlow"></p><p>In line 42 (Fig 13) , a new CNN model is created by calling the <strong>*model</strong> <em>function which accepts the optimizer type as input. In this experiment we would be using the ‘*</em>SGD<strong>’ (</strong>Stochastic Gradient Descent**) to train the network .</p><p>Everytime we invoke <strong><em>mlflow</em></strong> in our training code for logging, it is known as an <strong><em>mlflow</em></strong> run. <strong><em>MlFlow</em></strong> provides us with an API for starting and managing <strong><em>MlFlow</em></strong> runs. For example, Fig 14a and 14b</p><p><img src="https://cdn-images-1.medium.com/max/2000/0*gBpvWcli4WNQxv5a" alt="Fig 14a: Mlflow example (importing and logging params)"></p><p><img src="https://cdn-images-1.medium.com/max/2000/0*-QzXE_uwytc_IEmQ" alt="Fig 14b: Context managers can be used to declare and Mlflow run"></p><p>In our code we start the <strong><em>mlflow</em></strong> run using the python <strong><em>context manager</em></strong> as shown in Fig 14b.</p><p>At line 46 in Fig 13, we define our our <strong><em>mlflow</em></strong> run with the run name as ‘<strong><em>fashion mnist</em></strong>’.* <em>All data and metrics will be logged under this run name on the **</em>mlflow*** dashboard.</p><p>From line 47 we start a while loop, which continuously invokes the <strong>*dequeue</strong> <em>function from the *</em>MongoQeueue** class. What this does is fetches every row corresponding to a particular dataset from the dataset collection which has a <strong>status field = *Not Processed</strong> <em>(Fig 9). As soon as this dataset is fetched, *</em>setAsProcessing<strong>* <em>function is called in line 51 which sets the status of that dataset to *Processing *in *</em>MongoDb</strong>. This enables us to understand which dataset is currently being trained by our system. This is particularly helpful is large systems where there are multiple datasets and many training instances running in parallel.</p><p>In lines 54 and 55, the datasets are loaded from the <strong>*data</strong> <em>folder corresponding to the **</em>dataset_id** *fetched from the db.</p><p><img src="https://cdn-images-1.medium.com/max/2004/1*5lWcOwjfGzwNATxdkJmmgA.png" alt="Fig 15: Loading Test data and evaluating on it"></p><p>Lines 57 and 58 loads the test sets and the training is started at line 59 by calling the *train *function. We then use the trained model to predict our scores as shown in line 60 in Fig 15.</p><p>The training output looks as shown below (Fig 16a and Fig 16b)</p><p><img src="https://cdn-images-1.medium.com/max/2000/0*BCmu4INlyL7gPnxX" alt="Fig 16a: Model summary when starting to train"></p><p><img src="https://cdn-images-1.medium.com/max/2628/0*eRC8Zm0avYe38JlE" alt="Fig 16b : Outputs after first epoch"></p><p>As shown in Fig 16b, the training happens for an epoch and the evaluation metrics for the test dataset gets logged.</p><p>All outputs of the evaluation done using our trained model can be logged using the <strong><em>MlFlow</em></strong> tracking api (as shown in Fig 17). The <strong><em>tracking</em></strong> <strong><em>API</em></strong> comes with functions such as <strong><em>log_param</em></strong> and <strong><em>log_metric</em></strong> which enables us to log every hyperparameter and output values into <strong><em>mlflow</em></strong>.</p><p><img src="https://cdn-images-1.medium.com/max/2676/1*9IeXuffAu2mnwh21O35NUg.png" alt="Fig 17: Using Mlflow’s tracking API to log metrics and params"></p><p>The best feature about <strong><em>mlflow</em></strong> is the <strong><em>dashboard</em></strong> it provides. It has a very intuitive UI and can be used efficiently for tracking our experiments. The <strong><em>dashboard</em></strong> can be started easily by simply hitting <strong>*mlflow</strong> <strong>ui</strong> *in your terminal as shown in Fig 18 below.</p><p><img src="https://cdn-images-1.medium.com/max/2000/0*vhmPMa30-Gqnw6CQ" alt="Fig 18 : Starting the mlflow ui from the command line"></p><p>To access the dashboard, just type <a href="http://localhost:5000/" target="_blank" rel="noopener">***http://localhost:5000</a>** *in your browser and hit enter.</p><p><img src="https://cdn-images-1.medium.com/max/3200/0*rHX5D660ryqeUf7B" alt="Fig 19: Mlflow dashboard view."></p><p>Fig 19 shows how the dashboard looks like. Each <strong><em>MlFlow</em></strong> run is logged using a <strong><em>run ID</em></strong> and a <strong><em>run name</em></strong>. The Parameters and the Metrics column log display the parameters and the metrics which were logged while we were training our model</p><p><img src="https://cdn-images-1.medium.com/max/2288/0*DMhMMtoc1_ULtloJ" alt="Fig 20: Individual run information"></p><p>Further clicking on a particular run, takes us to another page where we can display all information about our run (Fig 20)</p><p><img src="https://cdn-images-1.medium.com/max/3200/0*t5qk_JlbTdNG89sy" alt="Fig 21a: Visualizing the test accuracy plot in mlflow"></p><p><strong><em>MlFlow</em></strong> provides us with this amazing feature to generate plots for our results. As you can see in Fig 21a, the test accuracy change can be visualized across different training datasets and time. We can also choose to display other metrics such as the <strong><em>eval</em></strong> loss, as shown in Fig 21b. The smoothness of the curve can also be controlled using the slider.</p><p><img src="https://cdn-images-1.medium.com/max/3200/0*mPZ1nM4I0kAY2yUU" alt="Fig 21b: Visualizing the eval loss plot in mlflow"></p><p>We can also log important files or scripts in our project to MlFlow using the <strong><em>mlflow.log_artifact</em></strong> command . Fig 22a shows how to use it in your training script and Fig 22b shows how it is displayed on the mlflow dashboard.</p><p><img src="https://cdn-images-1.medium.com/max/2000/1*JV0scppcjBZy_isq7HJOpQ.png" alt="Fig 22a: Logging files as artifact on mlflow"></p><p><img src="https://cdn-images-1.medium.com/max/2682/0*iEQvDCKtnOFCidSS" alt="Fig 22b: model file and db_ops file logged as artifact on mlflow"></p><p>MlFlow also allows users to compare two runs simultaneously and generate plots for it. Just tick the check-boxes against the runs you want to compare and press on the blue <strong><em>Compare</em></strong> button (Fig 23)</p><p><img src="https://cdn-images-1.medium.com/max/2096/0*5It-A9LbYnVJh9KM" alt="Fig 23: Mlflow dashboard which lists all the mlflow runs sequentially"></p><p>Once you click on compare, another page pops up where all metrics and parameters of two different runs can be viewed and studied in parallel (Fig 24a)</p><p><img src="https://cdn-images-1.medium.com/max/2976/0*0sNCbZBb3U61SzlQ" alt="Fig 24a: Comparing multiple mlflow runs in parallel"></p><p>The user can also choose to display metrics such as accuracy and loss in parallel charts as shown in Fig 24b.</p><p><img src="https://cdn-images-1.medium.com/max/3200/0*NQKwq3wMobxCLImI" alt="FIg 24b: Comparing multiple mlflow runs using visual plots based on metrics"></p><p>Users can add an MlFlow Project file (a text file in <strong><em>YAML</em></strong> syntax) to their MlFlow project allowing them to package their code better and run and recreate results from any machine. The MlFlow Project file for our <strong><em>fashion_mnsit</em></strong> project looks as shown below in Fig 25a</p><p><img src="https://cdn-images-1.medium.com/max/2000/0*JkMKeWYPlReYbuVn" alt="Fig 25a: MLfow project file containing entry points and also conda path"></p><p>We can also specify a <strong><em>conda</em></strong> environment for our <strong><em>MlFlow</em></strong> project and specify a <strong><em>conda.yaml</em></strong> file (Fig 25b).</p><p><img src="https://cdn-images-1.medium.com/max/2000/0*Bb3n9hCnxU6hblOm" alt="Fig 25b: Conda file specifying packages and dependencies in the specific environment"></p><p>Hence, with this we conclude our project. Developers face several on-demand requirements for monitoring metrics during training a neural network. These metrics can be extremely critical for predicting the output of their neural networks and are also critical in understanding how to modify a neural network for better performance. Traditionally when starting off with deep learning experiments, many developers are unaware of proper tools to help them. I hope this was piece of writing was helpful in understanding how deep learning experiments can be conducted in a proper manner during production when large numbers of datasets are needed to be managed and multiple training and evaluation instances are required to be monitored.</p><p><strong><em>MlFlow</em></strong> also has multiple other features which is beyond the scope of this tutorial and can be covered later. For any information on how to use <strong><em>MlFlow</em></strong> one can head to the <a href="https://mlflow.org/" target="_blank" rel="noopener">***https://mlflow.org/</a>***</p>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Deep Learning </tag>
            
            <tag> Keras </tag>
            
            <tag> Mlflow </tag>
            
            <tag> MongoDB </tag>
            
            <tag> Tracking </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>License plate recognition using Attention based OCR</title>
      <link href="/blog//2019/11/28/License-plate-recognition-using-Attention-based-OCR/"/>
      <url>/blog//2019/11/28/License-plate-recognition-using-Attention-based-OCR/</url>
      
        <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>If you clicked on this post title then it is certain that you are working on some kind of License plate recognition task and working on a specific kind of License Plate, if that’s the case you have landed in right post. In this post i will explain how to train Attention based OCR (AOCR) model for a specific License Plate.</p><p>I will first share some brief information of AOCR model followed by steps which will help you train the model and after that we will use the trained model to test its performance.</p><h2 id="Attention-OCR-Model-architecture"><a href="#Attention-OCR-Model-architecture" class="headerlink" title="Attention OCR Model architecture"></a>Attention OCR Model architecture</h2><p>First of all the source code of this model is available on this <a href="https://github.com/tensorflow/models/tree/master/research/attention_ocr" target="_blank" rel="noopener">Tensorflow</a> Github repository. I will suggest you to try <a href="https://github.com/emedvedev/attention-ocr" target="_blank" rel="noopener">this</a> repository if you want/can modify code.</p><img src="/blog/2019/11/28/License-plate-recognition-using-Attention-based-OCR/p1.png" class=""> <center>Figure 1.  AOCR model architecture</center><p>Source: <a href="https://arxiv.org/pdf/1609.04938v2.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1609.04938v2.pdf</a></p><p>Now the model structure, it has 7 Convolutional layer and 3 bi-directional Long short-term memory (LSTM) layers followed by a Seq2Seq model which translates image features to characters(act as a decoder). Convolutional layers can be seen in the below image.</p><img src="/blog/2019/11/28/License-plate-recognition-using-Attention-based-OCR/p2.png" class=""> <center>Figure 2. CNN architecture </center><br><h2 id="How-to-train-this-model"><a href="#How-to-train-this-model" class="headerlink" title="How to train this model?"></a>How to train this model?</h2><p>First of all use this pip command to install aocr on your system.</p><pre><code>pip install aocr</code></pre><p>Now you need dataset of License Plates images with its annotation. Annotation file should have file path and its label in a text file.</p><pre><code>datasets/image1.jpg label1datasets/image2.jpg label2</code></pre><p>After the dataset is ready along with annotation file you have to run this command to create tfrecords file for training of AOCR model. Separate some images for testing and create separate tfrecords file using test annotation file which contains paths of test images and corresponding labels.</p><pre><code>aocr dataset /path/to/training_annotation.txt path/to/training.tfrecordsaocr dataset /path/to/testing_annotation.txt path/to/testing.tfrecords</code></pre><p>The above command need annotation file path as input and creates tfrecords file on the given path i.e. last argument of above command. Now just run the below command to start training procedure.</p><p>By default maximum width is set to 160 and maximum height is set to 60. If your images has width or height more than the default maximum then model will not use those images for training. Either you resize all your images or you can set maximum width and height just make sure all the images are below those values.</p><p>Default checkpoint directory is ‘./checkpoints’ and it will create this where you will execute below command(You can set this too). Just make sure when you test you are giving correct checkpoint path, width and height.</p><p>Maximum prediction length is 8 by default and again you can change it according to your License plates. Default epoch is 1000 change it according to quantity of your dataset if it is small run it for default value otherwise you can set it to 500.</p><pre><code>aocr train /path/to/training.tfrecords --max-width 200 --max-height 100 --model-dir /path/to/checkpoint --max-prediction 6 --num-epoch 500</code></pre><h2 id="Test-the-model"><a href="#Test-the-model" class="headerlink" title="Test the model"></a>Test the model</h2><p>Once the training procedure is finished use this command to test the model. Just make sure checkpoint directory is created when the training starts.</p><pre><code>aocr test /path/to/testing.tfrecords --visualize --max-width 200 --max-height 100 --model-dir /path/to/checkpoint --max-prediction 6 --output-dir ./results</code></pre><p>Now you can see all the prediction inside result folder. For each file there will be one folder which will contain the GIF which will have attention mapped on the images and a text file which will have prediction in the first line and label in the second line. Prediction is placed on the folder name too by default.</p><img src="/blog/2019/11/28/License-plate-recognition-using-Attention-based-OCR/p3.png" class=""> <center>Figure 3.  Result folder directory</center><br><img src="/blog/2019/11/28/License-plate-recognition-using-Attention-based-OCR/p4.png" class=""> <center>Figure 4.  Text file which contains prediction and label</center><br><img src="/blog/2019/11/28/License-plate-recognition-using-Attention-based-OCR/p5.gif" class=""> <center>Gif 1.  GIF with attention map</center><br><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>If you have successfully trained and test the model then you can skip this part. If you have all the training images in one folder and their labels are in the filename itself then you can run this simple script to train your model.</p><h3 id="Note"><a href="#Note" class="headerlink" title="Note:"></a>Note:</h3><p>In case of same label for different images filename will be label_1.extension, label_2.extension etc.</p><p>Execute this script using this command “python3 Train_AOCR.py -d /home/some/path/”</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">import cv2</span><br><span class="line">import os</span><br><span class="line">import shutil</span><br><span class="line">import sys</span><br><span class="line">from pathlib import Path</span><br><span class="line">import optparse</span><br><span class="line"></span><br><span class="line">#python3 Train_AOCR.py -d /DIR_PATH/</span><br><span class="line"></span><br><span class="line"># Give checkpoint path, steps per checkpoints and number of epoch in line 61</span><br><span class="line"></span><br><span class="line">#give images max width and height here</span><br><span class="line">dim = (210, 70)</span><br><span class="line"></span><br><span class="line">parser = optparse.OptionParser()</span><br><span class="line">parser.add_option(&apos;-d&apos;, &apos;--dir_path&apos;,</span><br><span class="line">    action=&quot;store&quot;, dest=&quot;dirpath&quot;,</span><br><span class="line">    help=&quot;Enter test image directory&quot;, default=&quot;Empty&quot;)</span><br><span class="line"></span><br><span class="line">parser.add_option(&apos;-i&apos;, &apos;--image&apos;,</span><br><span class="line">    action=&quot;store&quot;, dest=&quot;image&quot;,</span><br><span class="line">    help=&quot;Input image&quot;, default=&quot;Empty&quot;)</span><br><span class="line"></span><br><span class="line">options, args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">if os.path.exists(&quot;/home/username/path/annotations-training.txt&quot;):</span><br><span class="line">  os.remove(&quot;/home/username/path/annotations-training.txt&quot;)</span><br><span class="line"></span><br><span class="line">if os.path.exists(&quot;/home/username/path/train.tfrecords&quot;):</span><br><span class="line">  os.remove(&quot;/home/username/path/train.tfrecords&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">f_veh = open(&apos;/home/username/path/annotations-training.txt&apos;, &apos;w+&apos;)</span><br><span class="line"></span><br><span class="line">if options.dirpath != &apos;Empty&apos;:</span><br><span class="line">for filename in os.listdir(options.dirpath):</span><br><span class="line"></span><br><span class="line">name, ext = os.path.splitext(filename)</span><br><span class="line">name = name.split(&apos;_&apos;)</span><br><span class="line">img = cv2.imread(options.dirpath+filename)</span><br><span class="line">img = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)</span><br><span class="line">cv2.imwrite(options.dirpath+filename,img)</span><br><span class="line">#os.rename(options.dirpath+filename,options.dirpath+temp[0]+ext)</span><br><span class="line">if ext in [&apos;.png&apos;, &apos;.jpg&apos;,&apos;.jpeg&apos;]:</span><br><span class="line">f_veh.write(options.dirpath+filename+ &apos; &apos;+name[0]+ &apos;\n&apos;)</span><br><span class="line"></span><br><span class="line">comm = &apos;aocr dataset /home/username/path/annotations-training.txt /home/username/path/train.tfrecords&apos;</span><br><span class="line">comm1 = &apos;aocr train /home/username/path/train.tfrecords --model-dir /home/username/path/checkpoints --max-height 70 --max-width 210 --max-prediction 6 --num-epoch 1000&apos; </span><br><span class="line">os.system(comm)</span><br><span class="line">os.system(comm1)</span><br></pre></td></tr></table></figure><p>Now you can test the model on a test set using the above code only same format goes as for the training set and its label.</p><p>You just need to run below code using this command “python3 Run_AOCR.py -d /home/some/test_set_path/”</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">import cv2</span><br><span class="line">import os</span><br><span class="line">import shutil</span><br><span class="line">import sys</span><br><span class="line">from pathlib import Path</span><br><span class="line">import optparse</span><br><span class="line"></span><br><span class="line">#python3 Run_AOCR.py -d /DIR_PATH/</span><br><span class="line"></span><br><span class="line"># Give checkpoint path, steps per checkpoints and number of epoch in line 61</span><br><span class="line"></span><br><span class="line">#give images max width and height here</span><br><span class="line">dim = (210, 70)</span><br><span class="line"></span><br><span class="line">parser = optparse.OptionParser()</span><br><span class="line">parser.add_option(&apos;-d&apos;, &apos;--dir_path&apos;,</span><br><span class="line">    action=&quot;store&quot;, dest=&quot;dirpath&quot;,</span><br><span class="line">    help=&quot;Enter test image directory&quot;, default=&quot;Empty&quot;)</span><br><span class="line"></span><br><span class="line">parser.add_option(&apos;-i&apos;, &apos;--image&apos;,</span><br><span class="line">    action=&quot;store&quot;, dest=&quot;image&quot;,</span><br><span class="line">    help=&quot;Input image&quot;, default=&quot;Empty&quot;)</span><br><span class="line"></span><br><span class="line">options, args = parser.parse_args()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">p = Path(&quot;/home/username/path/results&quot;)</span><br><span class="line">if p.is_dir():</span><br><span class="line">shutil.rmtree(&apos;/home/username/path/results&apos;)</span><br><span class="line"></span><br><span class="line">if os.path.exists(&quot;/home/username/path/annotations-testing.txt&quot;):</span><br><span class="line">  os.remove(&quot;/home/username/path/annotations-testing.txt&quot;)</span><br><span class="line"></span><br><span class="line">if os.path.exists(&quot;/home/username/path/test.tfrecords&quot;):</span><br><span class="line">  os.remove(&quot;/home/username/path/test.tfrecords&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">f_veh = open(&apos;/home/username/path/annotations-testing.txt&apos;, &apos;w+&apos;)</span><br><span class="line"></span><br><span class="line">if options.dirpath != &apos;Empty&apos;:</span><br><span class="line">for filename in os.listdir(options.dirpath):</span><br><span class="line"></span><br><span class="line">name, ext = os.path.splitext(filename)</span><br><span class="line">name = name.split(&apos;_&apos;)</span><br><span class="line">img = cv2.imread(options.dirpath+filename)</span><br><span class="line">img = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)</span><br><span class="line">cv2.imwrite(options.dirpath+filename,img)</span><br><span class="line">#os.rename(options.dirpath+filename,options.dirpath+temp[0]+ext)</span><br><span class="line">if ext in [&apos;.png&apos;, &apos;.jpg&apos;,&apos;.jpeg&apos;]:</span><br><span class="line">f_veh.write(options.dirpath+filename+ &apos; &apos;+name[0]+ &apos;\n&apos;)</span><br><span class="line"></span><br><span class="line">comm = &apos;aocr dataset /home/username/path/annotations-testing.txt /home/username/path/test.tfrecords&apos;</span><br><span class="line">comm1 = &apos;aocr test --visualize /home/username/path/test.tfrecords --model-dir /home/username/path/checkpoints --max-height 70 --max-width 210 --max-prediction 6 --output-dir ./results&apos; </span><br><span class="line">os.system(comm)</span><br><span class="line">os.system(comm1)</span><br></pre></td></tr></table></figure><p>After running this script you can find all the prediction in the output directory.</p>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Deep Learning </tag>
            
            <tag> OCR </tag>
            
            <tag> AOCR </tag>
            
            <tag> License Plates </tag>
            
            <tag> License Plates Recognition </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Sharing modules across Android apps</title>
      <link href="/blog//2019/11/27/sharing-modules-across-android-apps/"/>
      <url>/blog//2019/11/27/sharing-modules-across-android-apps/</url>
      
        <content type="html"><![CDATA[<h2 id="Sharing-modules-across-Android-apps"><a href="#Sharing-modules-across-Android-apps" class="headerlink" title="Sharing modules across Android apps"></a>Sharing modules across Android apps</h2><p>While most android apps are created with a single default <code>app</code> module, in the last few years people have started<br>moving to a multi module structure for their Android apps. Having multiple smaller modules have a few distinct<br>advantages</p><ul><li>Build times are noticeably faster</li><li>Your code is decoupled with clear dependencies</li><li>Better distribution of ownership across different parts of the app</li><li>Allows modules to be reused across apps</li></ul><h4 id="Example-modules"><a href="#Example-modules" class="headerlink" title="Example modules"></a>Example modules</h4><p>A possible strategy is to have one module per feature.</p><ul><li><code>app</code>: This is the main module which will be the entry point into your app. It acts mainly as a coordinator between<br>other modules</li><li><code>core</code>: This will contain the model definitions that are core to your app and will be required across modules</li><li><code>networking</code>: This provides the networking code for the other modules</li><li><code>login</code>: Login/Signup logic goes here</li><li><code>dashboard</code>: User dashboard will be here</li></ul><img src="/blog/2019/11/27/sharing-modules-across-android-apps/modules.jpg" class="" title="Modules"><p>There are many <a href="https://medium.com/google-developer-experts/modularizing-android-applications-9e2d18f244a0" target="_blank" rel="noopener">posts</a> on the advantages and strategies for modularizing your Android apps. For this post, we will<br>focus on the strategy to reuse modules across apps.</p><h4 id="Reusing-modules-across-apps"><a href="#Reusing-modules-across-apps" class="headerlink" title="Reusing modules across apps"></a>Reusing modules across apps</h4><p>We have multiple apps in our company that share the <code>core</code> and <code>login</code> logic. So we decided to share these modules among<br>the two applications.</p><p><img src="hierarchy.png" alt="hierarchy" title="Hierarchy"></p><p>One obvious way to share Android Library modules would be to share the generated <code>.aar</code> files and add them as<br>dependencies to the different apps. While this is simpler, the main applications and the library modules will be<br>different Android Studio projects. If any change needs to be done in the library, the <code>.aar</code> will have to be regenerated<br>and manually updated. There has to be a better way.</p><p>The solution we decided to use for sharing modules is <a href="https://git-scm.com/book/en/v2/Git-Tools-Submodules" target="_blank" rel="noopener">git submodules</a>. Though it had a small overhead in bringing<br>the entire team up to speed with submodules, it has worked exceptionally well for us.</p><p>In the above example, we have two git submodules, core and login.</p><p><img src="submodules.png" alt="submodules" title="Submodules"></p><p>And the submodules will be added as dependencies just as any Android module,</p><p><img src="dependencies.png" alt="dependencies" title="Dependencies"></p><h4 id="Creating-a-new-submodules"><a href="#Creating-a-new-submodules" class="headerlink" title="Creating a new submodules"></a>Creating a new submodules</h4><p>To create a new submodule, we follow the following process</p><ul><li>Create an empty repo on Github and initialize with a README</li><li>Add a new submodule to our app <code>git submodule add git@github.com:username/core.git</code></li><li>Create a new Android Library module in the new directory</li><li>Commit the changes in the library module and push</li><li>Commit the changes in the main repo and push</li></ul><p>Next time we need to use this submodule in another app, we only need to</p><ul><li>Add a new submodule to our app <code>git submodule add git@github.com:username/core.git</code></li><li>Add the newly added module to <code>settings.gradle</code> and a dependency in <code>build.gradle</code></li></ul><h4 id="Committing-changes-to-a-submodule"><a href="#Committing-changes-to-a-submodule" class="headerlink" title="Committing changes to a submodule"></a>Committing changes to a submodule</h4><p>Every time we make some changes to a submodule, we just need to make sure that we commit and push those changes<br>before committing the changes in the main repo.</p><p>Submodule:</p><ul><li>If you are on a detached head, <code>git checkout -b new-branch</code></li><li><code>git add . &amp;&amp; git commit -am &quot;commit message&quot;</code></li><li><code>git push origin new-branch</code></li></ul><p>Main repo:</p><ul><li><code>git add . &amp;&amp; git commit -am &quot;commit message&quot;</code></li><li><code>git push</code></li></ul><h4 id="Fetching-remote-changes"><a href="#Fetching-remote-changes" class="headerlink" title="Fetching remote changes"></a>Fetching remote changes</h4><p>Every time we do a git pull, we just need to remember to update the submodules as well</p><p><code>git submodule update</code></p><p>and that’s it. We have the latest version of the submodule locally!</p><p>Hope this works for you.</p><p>Happy coding!</p>]]></content>
      
      
      <categories>
          
          <category> Android </category>
          
      </categories>
      
      
        <tags>
            
            <tag> android </tag>
            
            <tag> git submodules </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Text detection in number plates</title>
      <link href="/blog//2019/11/26/Text-detection-in-number-plates/"/>
      <url>/blog//2019/11/26/Text-detection-in-number-plates/</url>
      
        <content type="html"><![CDATA[<h1 id="Text-detection-in-number-plates"><a href="#Text-detection-in-number-plates" class="headerlink" title="Text detection in number plates"></a>Text detection in number plates</h1><p>One of the vital modules in the optical character recognition(OCR) pipeline is text detectionand segmentation which is also called text localization. In this post, we will apply variedpreprocessing techniques to the input image and find out how to localize text in theenhanced image, so that we can feed the segments to our text recognition network.</p><h2 id="Image-Preprocessing"><a href="#Image-Preprocessing" class="headerlink" title="Image Preprocessing"></a>Image Preprocessing</h2><p>Sometimes images can be distorted, noisy and other problems that can scale back the OCRaccuracy. To make a better OCR pipeline, we need to do some image preprocessing.</p><ul><li>Grayscale the image: Generally you will get an image which is having 3channels(color images), we need to convert this image into a grayscale form whichcontains only one channel. We can also process images with three channels but itonly increases the complexity of the model and increases the processing time.OpenCV provides a built-in function that can do it for you.</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import cv2</span><br><span class="line">grayscale_image = cv2.cvtColor(image, cv2.COLOR_BRG2GRAY)</span><br></pre></td></tr></table></figure><p>Or you can convert the image to grayscale while reading the image.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#opencv reads image in BGR format</span><br><span class="line">graysacle_image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)</span><br></pre></td></tr></table></figure><ul><li>Noise reduction: Images come with various types of noises. OpenCV provides a lot ofnoise reduction function. I am using the Non-local Means Denoising algorithm.</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">denoised_image = cv2.fastNlMeansDenoising(grayscale_img, None, 10, 7, 21)</span><br></pre></td></tr></table></figure><img src="/blog/2019/11/26/Text-detection-in-number-plates/Figure_2.png" class="" title="Denoising"><ul><li>Contrast adjustment: Sometimes we have low contrast images. This makes it difficultto separate text from the image background. We need high contrast text images forthe localization process. We can increase image contrast using Contrast LimitedAdaptive Histogram Equalization (CLAHE) among many other contrast enhancementmethods provided by skimage.</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from  skimage import exposure</span><br><span class="line">contrast_enhanced_image = exposure.equalize_adapthist(denoised, clip_limit=0.03)</span><br></pre></td></tr></table></figure><img src="/blog/2019/11/26/Text-detection-in-number-plates/Figure_3.png" class="" title="Contrast Adjustment"><p>So now we are done with image preprocessing let us move on to the second part, textlocalization.</p><h2 id="Text-Localization"><a href="#Text-Localization" class="headerlink" title="Text Localization"></a>Text Localization</h2><p>In this part, we will see how to detect a large number of text region candidates andprogressively removes those less likely to contain text. Using the MSER feature descriptor tofind text candidates in the image. It works well for text because the consistent color and highcontrast of text lead to stable intensity profiles.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#constructor for MSER detector</span><br><span class="line">mser = cv2.MSER_create()</span><br><span class="line">regions, mser_bboxes = mser.detectRegions(contrast_enhance_image)</span><br></pre></td></tr></table></figure><p>Along with the text MSER picked up many other stable regions that are not text. Now, thegeometric properties of text can be used to filter out non-text regions using simplethresholds.</p><p>Before moving on with the filtering process, let’s write some functions to display the results ina comprehensible manner.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">#display images</span><br><span class="line">def pltShow(*images):</span><br><span class="line">    #count number of images to show</span><br><span class="line">    count = len(images)</span><br><span class="line">    #three images per columnn</span><br><span class="line">    Row = np.ceil(count / 3.)</span><br><span class="line">    for i in range(count):</span><br><span class="line">        plt.subplot(nRow, 3, i+1)</span><br><span class="line">        if len(images[i][0], cmap=’gray’)</span><br><span class="line">            plt.imshow(images[i][0], cmap=’gray’)</span><br><span class="line">        else:</span><br><span class="line">            plt.imshow(images[i][0])</span><br><span class="line">        #remove x-y axis from subplots</span><br><span class="line">        plt.xticks([])</span><br><span class="line">        plt.yticks([])</span><br><span class="line">        plt.title(images[i][1])</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">#color each MSER region in image</span><br><span class="line">def colorRegion(image_like_arr, region):</span><br><span class="line">    image_like_arr[region[:, 1], region[:, 0], 0] = np.random.randint(low=100, high=256)</span><br><span class="line">    image_like_arr[region[:, 1], region[:, 0], 1] = np.random.randint(low=100, high=256)</span><br><span class="line">    image_like_arr[region[:, 1], region[:, 0], 2] = np.random.randint(low=100, high=256)</span><br><span class="line"></span><br><span class="line">    return image</span><br></pre></td></tr></table></figure><p>The geometric properties we are going to use to discriminate between text and non-textregion are:</p><ul><li>Region area</li><li>Region perimeter</li><li>Aspect ratio</li><li>Occupancy</li><li>Compactness</li></ul><p>We will apply simple thresholds over these parameters to eliminate non-text regions. Firstlet’s write method to compute these parameters.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">#values for the parameters</span><br><span class="line">AREA_LIM = 2.0e-4</span><br><span class="line">PERIMETER_LIM = 1e-4</span><br><span class="line">ASPECT_RATIO_LIM = 5.0</span><br><span class="line">OCCUPATION_LIM = (0.23, 0.90)</span><br><span class="line">COMPACTNESS_LIM = (3e-3, 1e-1)</span><br><span class="line"></span><br><span class="line">def getRegionShape(self, region): </span><br><span class="line">    return (max(region[:, 1]) - min(region[:, 1]), max(region[:, 0]) - min(region[:, 0]))</span><br><span class="line">    </span><br><span class="line">#compute area</span><br><span class="line">def getRegionArea(region):</span><br><span class="line">    return len(list(region))</span><br><span class="line"></span><br><span class="line">#compute perimeter</span><br><span class="line">def getRegionPerimeter(image, region):</span><br><span class="line">    #get top-left coordinate, width and height of the box enclosing the region</span><br><span class="line">    x, y, w, h = cv2.boundingRect(region)</span><br><span class="line">    return len(np.where(image[y:y+h, x:x+w] != 0)[0]))</span><br><span class="line">    </span><br><span class="line">#compute aspect ratio</span><br><span class="line">def getAspectRatio(region):    </span><br><span class="line">    return (1.0 * max(getRegionShape(region))) / (min(getRegionShape(region)) + 1e-4)</span><br><span class="line"></span><br><span class="line">#compute area occupied by the region area in the shape</span><br><span class="line">def getOccupyRate(region):</span><br><span class="line">    return (1.0 * getRegionArea(region)) / (getRegionShape(region)[0] *  \getRegionShape(region)[1] + 1.0e-10)</span><br><span class="line">    </span><br><span class="line">#compute compactness of the regio</span><br><span class="line">ndef getCompactness(region):    </span><br><span class="line">    return (1.0 * getRegionArea(region)) / (1.0 * getRegionPerimeter(region) ** 2)</span><br></pre></td></tr></table></figure><p>Now apply these methods to filter out text regions  as follows:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">#total number of MSER regions</span><br><span class="line">n1 = len(regions)</span><br><span class="line">bboxes=[]</span><br><span class="line">for i, region in enumerate(regions):</span><br><span class="line">    self.colorRegion(res, region)</span><br><span class="line">    if self.getRegionArea(region) &gt; self.grayImg.shape[0] * self.grayImg.shape[1] * AREA_LIM:</span><br><span class="line">       #number of regions meeting the area criteria</span><br><span class="line">    n2 += 1</span><br><span class="line">    self.colorRegion(res2, region)</span><br><span class="line"></span><br><span class="line">    if self.getRegionPerimeter(region) &gt; 2 * (self.grayImg.shape[0] + \</span><br><span class="line">        self.grayImg.shape[1]) * PERIMETER_LIM:</span><br><span class="line">   #number of regions meeting the perimeter criteria</span><br><span class="line">        n3 += 1</span><br><span class="line">   self.colorRegion(res3, region)</span><br><span class="line"> </span><br><span class="line">        if self.getAspectRatio(region) &lt; ASPECT_RATIO_LIM:</span><br><span class="line">   #number of regions meeting the aspect ratio criteria </span><br><span class="line">                n4 += 1</span><br><span class="line">   self.colorRegion(res4, region)</span><br><span class="line"></span><br><span class="line">   if (self.getOccupyRate(region) &gt; OCCUPATION_LIM[0]) and \ (self.getOccupyRate(region) &lt; OCCUPATION_LIM[1]):</span><br><span class="line">   n5 += 1</span><br><span class="line">   self.colorRegion(res5, region)</span><br><span class="line"></span><br><span class="line">   if (self.getCompactness(region) &gt; \COMPACTNESS_LIM[0]) and \(self.getCompactness(region) &lt; \COMPACTNESS_LIM[1]):</span><br><span class="line">   #final number of regions left </span><br><span class="line">                        n6 += 1</span><br><span class="line">   self.colorRegion(res6, region)</span><br><span class="line">                        bboxes.append(mser_bboxes[i])</span><br></pre></td></tr></table></figure><p>After eliminating non-text regions, I draw bounding boxes on the remaining regions andvoila, we have successfully detected and segmented the characters on the number plate.<br>Note: Apply NMS to remove overlapping bounding boxes.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">for bbox in bboxes:</span><br><span class="line">   cv2.rectangle(img,(bbox[0]-1,bbox[1]-1),(bbox[0]+bbox[2]+1,box[1]+bbox[3]+1),(255,0,0), 1)</span><br></pre></td></tr></table></figure><p>Enough coding. Let’s see some results.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">pltShow(&quot;MSER Result Analysis&quot;, \</span><br><span class="line">      (self.img, &quot;Image&quot;), \</span><br><span class="line">      (self.cannyImg, &quot;Canny&quot;), \</span><br><span class="line">      (res, &quot;MSER,(&#123;&#125; regions)&quot;.format(n1)), \</span><br><span class="line">      (res2, &quot;Area=&#123;&#125;,(&#123;&#125; regions)&quot;.format(config.mser_areaLimit, n2)), \</span><br><span class="line">      (res3, &quot;Perimeter=&#123;&#125;,(&#123;&#125; regions)&quot;.format(config.mser_perimeterLimit, n3)), \</span><br><span class="line">      (res4, &quot;Aspect Ratio=&#123;&#125;,(&#123;&#125; regions)&quot;.format(config.mser_aspectRatioLimit, n4)), \</span><br><span class="line">      (res5, &quot;Occupation=&#123;&#125;,(&#123;&#125; regions)&quot;.format(config.mser_occupationLimit, n5)), \</span><br><span class="line">      (res6, &quot;Compactness=&#123;&#125;,(&#123;&#125; regions)&quot;.format(config.mser_compactnessLimit, n6)), \</span><br><span class="line">      (boxRes, &quot;Segmented Image&quot;) \</span><br><span class="line">   )</span><br></pre></td></tr></table></figure><img src="/blog/2019/11/26/Text-detection-in-number-plates/Figure_1.png" class="" title="MSER Result Analysis"><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>In this post, we covered the various image preprocessing techniques and learned about howto perform text localization on number plates.</p>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Detecting Lanes using Deep Neural Networks</title>
      <link href="/blog//2019/11/26/Detecting-Lanes-using-Deep-Neural-Networks/"/>
      <url>/blog//2019/11/26/Detecting-Lanes-using-Deep-Neural-Networks/</url>
      
        <content type="html"><![CDATA[<blockquote><p>This post explains how to use deep neural networks to detect highway lanes. Lane markings are the main static component on highways.<strong>They instruct the vehicles to interactively and safely drive on the highways.</strong> Lane detection is also an important task in autonomous driving, which provides localization information to the control of the car. It is also used in <strong>ADAS(Advanced Driver Assistance System)</strong>.</p></blockquote><p>For the task of lane detection, we have two open-source datasets available. One is the Tusimple dataset and the other is the CULane dataset. Let’s have a brief look at one of the datasets.</p><h2 id="Tusimple-Dataset"><a href="#Tusimple-Dataset" class="headerlink" title="Tusimple Dataset"></a>Tusimple Dataset</h2><p>This dataset was released as part of the Tusimple Lane Detection Challenge. It contains 3626 video clips of 1 sec duration each. Each of these video clips contains 20 frames of which, the last frame is annotated. These videos were captured by mounting the cameras on a vehicle dashboard. You can download the dataset from <a href="https://github.com/TuSimple/tusimple-benchmark/issues/3" target="_blank" rel="noopener">here</a>.</p><p>The directory structure looks like the figure below,</p><img src="/blog/2019/11/26/Detecting-Lanes-using-Deep-Neural-Networks/dir_structure.png" class="" title="Dataset directory structure"><p>Each sub-directory contains 20 sequential images of which, the last frame is annotated. label_data_(date).json contains labels in JSON format for the last frame. Each line in the JSON file is a dictionary with key values…</p><p><strong>raw_file</strong>: string type. the file path in the clip</p><p><strong>lanes</strong>: it is a list of list of lanes. Each list corresponds to a lane and each element of the inner list is x-coordinate of ground truth lane.</p><p><strong>h_samples</strong>: it is a list of height values corresponding to the lanes. Each element in this list is y-coordinate of ground truth lane</p><p>In this dataset, at most four lanes are annotated - the two ego lanes (two lane boundaries in which the vehicle is currently located) and the lanes to the right and left of ego lanes. All the lanes are annotated at an equal interval of height, therefore h_samples contain only one list whose elements correspond to y-coordinates for all lists in lanes. For a point in h_samples, if there is no lane at the location, its corresponding x-coordinate has -2. For example, a line in the JSON file looks like :</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;lanes&quot;: [</span><br><span class="line">        [-2, -2, -2, -2, 632, 625, 617, 609, 601, 594, 586, 578, 570, 563, 555, 547, 539, 532, 524, 516, 508, 501, 493, 485, 477, 469, 462, 454, 446, 438, 431, 423, 415, 407, 400, 392, 384, 376, 369, 361, 353, 345, 338, 330, 322, 314, 307, 299],</span><br><span class="line">        [-2, -2, -2, -2, 719, 734, 748, 762, 777, 791, 805, 820, 834, 848, 863, 877, 891, 906, 920, 934, 949, 963, 978, 992, 1006, 1021, 1035, 1049, 1064, 1078, 1092, 1107, 1121, 1135, 1150, 1164, 1178, 1193, 1207, 1221, 1236, 1250, 1265, -2, -2, -2, -2, -2],</span><br><span class="line">        [-2, -2, -2, -2, -2, 532, 503, 474, 445, 416, 387, 358, 329, 300, 271, 241, 212, 183, 154, 125, 96, 67, 38, 9, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2],</span><br><span class="line">        [-2, -2, -2, 781, 822, 862, 903, 944, 984, 1025, 1066, 1107, 1147, 1188, 1229, 1269, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]</span><br><span class="line">       ],</span><br><span class="line">  &quot;h_samples&quot;: [240, 250, 260, 270, 280, 290, 300, 310, 320, 330, 340, 350, 360, 370, 380, 390, 400, 410, 420, 430, 440, 450, 460, 470, 480, 490, 500, 510, 520, 530, 540, 550, 560, 570, 580, 590, 600, 610, 620, 630, 640, 650, 660, 670, 680, 690, 700, 710],</span><br><span class="line">  &quot;raw_file&quot;: &quot;path_to_clip&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>It says that there are four lanes in the image, and the first lane starts at (632,280), the second lane starts at (719,280), the third lane starts at (532,290) and the fourth lane starts at (781,270).</p><h2 id="DataSet-Visualization"><a href="#DataSet-Visualization" class="headerlink" title="DataSet Visualization"></a>DataSet Visualization</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># import required packages</span><br><span class="line">import json</span><br><span class="line">import numpy as np</span><br><span class="line">import cv2</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"># read each line of json file</span><br><span class="line">json_gt = [json.loads(line) for line in open(&apos;label_data.json&apos;)]</span><br><span class="line">gt = json_gt[0]</span><br><span class="line">gt_lanes = gt[&apos;lanes&apos;]</span><br><span class="line">y_samples = gt[&apos;h_samples&apos;]</span><br><span class="line">raw_file = gt[&apos;raw_file&apos;]</span><br><span class="line"># see the image</span><br><span class="line">img = cv2.imread(raw_file)</span><br><span class="line">cv2.imshow(&apos;image&apos;,img)</span><br><span class="line">cv2.WaitKey(0)</span><br><span class="line">cv2.destroyAllWindows()</span><br></pre></td></tr></table></figure><img src="/blog/2019/11/26/Detecting-Lanes-using-Deep-Neural-Networks/image_1.jpg" class="" title="Image 1. Raw image from the dataset"><p>Now see the JSON points visualization on the image</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">gt_lanes_vis = [[(x, y) for (x, y) in zip(lane, y_samples)</span><br><span class="line">                  if x &gt;= 0] for lane in gt_lanes]</span><br><span class="line">img_vis = img.copy()</span><br><span class="line"></span><br><span class="line">for lane in gt_lanes_vis:</span><br><span class="line">    cv2.polylines(img_vis, np.int32([lane]), isClosed=False,</span><br><span class="line">                   color=(0,255,0), thickness=5)</span><br></pre></td></tr></table></figure><img src="/blog/2019/11/26/Detecting-Lanes-using-Deep-Neural-Networks/image_2.jpg" class="" title="Image 2. Label visualications of image"><p>Now, we have understood the dataset, but we can not pass the above image as a label for the neural network since <strong>grayscale images with values ranging from zero to num_classes -1 are to be passed to the deep convolution neural network to outputs an image containing predicted lanes</strong>. So, we need to generate label images for the JSON files. Label images can be generated using <strong>OpenCV</strong> by drawing lines passing through the points in the JSON file.</p><p>OpenCV has an inbuilt function to draw multiple lines through a set of points. OpenCV’s Polylines method can be used here. First, create a mask of all zeros with height and width equal to the raw file’s height and width using numpy. <strong>The image size can be reduced to maintain lesser computations during training, but do not forget to maintain the same aspect ratio</strong>.</p><h2 id="Generating-Labels"><a href="#Generating-Labels" class="headerlink" title="Generating Labels"></a>Generating Labels</h2><p>The label should be a grayscale image. Generate one label for each clip from the JSON file. First, create a mask of black pixels with a shape similar to the raw_file image from the JSON file. Now, using OpenCV’s polylines method draw lines with different colors (each corresponding to each lane in lanes) on the mask image using the lanes and h_samples from the JSON file. From the three channeled mask image generate a gray scale mask image with values as class numbers. Likewise, create labels for all the images in the JSON file. You can resize the image and its label to a smaller size for lesser computations.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">mask = np.zeros_like(img)</span><br><span class="line">colors = [[255,0,0],[0,255,0],[0,0,255],[0,255,255]]</span><br><span class="line">for i in range(len(gt_lanes_vis)):</span><br><span class="line">    cv2.polylines(mask, np.int32([gt_lanes_vis[i]]), isClosed=False,color=colors[i], thickness=5)</span><br><span class="line">!! create grey-scale label image</span><br><span class="line">label = np.zeros((720,1280),dtype = np.uint8)</span><br><span class="line">for i in range(len(colors)):</span><br><span class="line">   label[np.where((mask == colors[i]).all(axis = 2))] = i+1</span><br></pre></td></tr></table></figure><img src="/blog/2019/11/26/Detecting-Lanes-using-Deep-Neural-Networks/image_3.jpg" class="" title="Image 3. Generated mask image"><h2 id="Build-and-Train-Model"><a href="#Build-and-Train-Model" class="headerlink" title="Build and Train Model"></a>Build and Train Model</h2><p>Lane Detection is essentially an image segmentation problem. So I am using the <strong>ERFNET model</strong> for this task, which is efficient and fast. Originally ERFNET was proposed for semantic segmentation problems, but it can also be extended to other image segmentation problems. You can check out for its paper <a href="https://ieeexplore.ieee.org/abstract/document/8063438" target="_blank" rel="noopener">here</a>. It is a CNN with Encoder, Decoder and dilated convolutions along with non-bottleneck residual layers. See Fig.1 for model architecture.</p><img src="/blog/2019/11/26/Detecting-Lanes-using-Deep-Neural-Networks/figure_1.png" class="" title="Figure 1. Model Architecture"><p>Build and create an object of the model. Train it over the dataset created above, for a sufficient number of epochs with Binary Cross Entropy loss or custom loss function which minimizes the per-pixel error. For better memory usage, create a dataset generator and train the model over it. Generators remove the burden of loading all the images into memory (if your dataset is of large size, you should use a generator) which leads to eating up of all memory and the other processes can’t work properly. Fig 2 shows the layers of ERFNET with input and output dimensions.</p><img src="/blog/2019/11/26/Detecting-Lanes-using-Deep-Neural-Networks/figure_2.png" class="" title="Figure 2. Model layers with input and output shapes"><h2 id="Evaluate-Model"><a href="#Evaluate-Model" class="headerlink" title="Evaluate Model"></a>Evaluate Model</h2><p>After training, get the model’s predictions using the code snippet below. I have implemented this in Pytorch. I use the color_lanes method to convert output images from the model (which are two channeled with values as class numbers) to three channeled images. im_seg is the final overlayed image shown in Image 4.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">#using pytorch</span><br><span class="line">import torch</span><br><span class="line">from torchvision.transforms import ToTensor</span><br><span class="line">def color_lanes(image, classes, i, color, HEIGHT, WIDTH):</span><br><span class="line">    buffer_c1 = np.zeros((HEIGHT, WIDTH), dtype=np.uint8)</span><br><span class="line">    buffer_c1[classes == i] = color[0]</span><br><span class="line">    image[:, :, 0] += buffer_c1</span><br><span class="line">    buffer_c2 = np.zeros((HEIGHT, WIDTH), dtype=np.uint8)</span><br><span class="line">    buffer_c2[classes == i] = color[1]</span><br><span class="line">    image[:, :, 1] += buffer_c2</span><br><span class="line">    buffer_c3 = np.zeros((HEIGHT, WIDTH), dtype=np.uint8)</span><br><span class="line">    buffer_c3[classes == i] = color[2]</span><br><span class="line">    image[:, :, 2] += buffer_c3</span><br><span class="line">    return image</span><br><span class="line">img = cv2.imread(&apos;images/test.jpg&apos;) </span><br><span class="line">img = cv2.resize(img,(WIDTH, HEIGHT),interpolation = cv2.INETR_CUBIC)</span><br><span class="line">op_transforms = transforms.Compose([transforms.ToTensor()])</span><br><span class="line">device = torch.device(&apos;cuda&apos; if torch.cuda.is_available() else &apos;cpu&apos;)</span><br><span class="line">im_tensor = torch.unsqueeze(op_transforms(img), dim=0)</span><br><span class="line">im_tensor = im_tensor.to(device)</span><br><span class="line">model = ERFNET(5)</span><br><span class="line">model = model.to(device)</span><br><span class="line">model = model.eval()</span><br><span class="line">out = model(im_tensor)</span><br><span class="line">out = out.max(dim=1)[1]</span><br><span class="line">out_np = out.cpu().numpy()[0]</span><br><span class="line">out_viz = np.zeros((HEIGHT, WIDTH, 3))</span><br><span class="line">for i in range(1, NUM_LD_CLASSES):</span><br><span class="line">    rand_c1 = random.randint(1, 255)</span><br><span class="line">    rand_c2 = random.randint(1, 255)</span><br><span class="line">    rand_c3 = random.randint(1, 255)</span><br><span class="line">    out_viz = color_lanes(</span><br><span class="line">            out_viz, out_np,</span><br><span class="line">            i, (rand_c1, rand_c2, rand_c3), HEIGHT, WIDTH)</span><br><span class="line">instance_im = out_viz.astype(np.uint8)</span><br><span class="line">im_seg = cv2.addWeighted(img, 1, instance_im, 1, 0)</span><br></pre></td></tr></table></figure><img src="/blog/2019/11/26/Detecting-Lanes-using-Deep-Neural-Networks/image_4.jpg" class="" title="Image 4. Final predicted image"><p>Thanks for reading it…</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol><li>ERFNet: Efficient Residual Factorized ConvNet for Real-time Semantic 2. Segmentation.</li><li>Lane Detection and Classification using CNNs.</li><li><a href="https://www.mdpi.com/sensors/sensors-19-00503/article_deploy/html/images/sensors-19-00503-g004.png" target="_blank" rel="noopener">https://www.mdpi.com/sensors/sensors-19-00503/article_deploy/html/images/sensors-19-00503-g004.png</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Deep Learning </tag>
            
            <tag> Lane Detection </tag>
            
            <tag> Advance Driver Assistance </tag>
            
            <tag> Autonomous Driving </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
