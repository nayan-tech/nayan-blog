<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Nayan Blog</title>
  
  
  <link href="/blog/atom.xml" rel="self"/>
  
  <link href="https://nayan.co/blog/"/>
  <updated>2019-11-28T08:12:01.416Z</updated>
  <id>https://nayan.co/blog/</id>
  
  <author>
    <name>NayanTech</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>License plate recognition using Attention based OCR</title>
    <link href="https://nayan.co/blog/2019/11/28/License-plate-recognition-using-Attention-based-OCR/"/>
    <id>https://nayan.co/blog/2019/11/28/License-plate-recognition-using-Attention-based-OCR/</id>
    <published>2019-11-28T12:06:56.000Z</published>
    <updated>2019-11-28T08:12:01.416Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>If you clicked on this post title then it is certain that you are working on some kind of License plate recognition task and working on a specific kind of License Plate, if that’s the case you have landed in right post. In this post i will explain how to train Attention based OCR (AOCR) model for a specific License Plate.</p><p>I will first share some brief information of AOCR model followed by steps which will help you train the model and after that we will use the trained model to test its performance.</p><h2 id="Attention-OCR-Model-architecture"><a href="#Attention-OCR-Model-architecture" class="headerlink" title="Attention OCR Model architecture"></a>Attention OCR Model architecture</h2><p>First of all the source code of this model is available on this <a href="https://github.com/tensorflow/models/tree/master/research/attention_ocr" target="_blank" rel="noopener">Tensorflow</a> Github repository. I will suggest you to try <a href="https://github.com/emedvedev/attention-ocr" target="_blank" rel="noopener">this</a> repository if you want/can modify code.</p><img src="/blog/2019/11/28/License-plate-recognition-using-Attention-based-OCR/p1.png" class=""> <center>Figure 1.  AOCR model architecture</center><p>Source: <a href="https://arxiv.org/pdf/1609.04938v2.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1609.04938v2.pdf</a></p><p>Now the model structure, it has 7 Convolutional layer and 3 bi-directional Long short-term memory (LSTM) layers followed by a Seq2Seq model which translates image features to characters(act as a decoder). Convolutional layers can be seen in the below image.</p><img src="/blog/2019/11/28/License-plate-recognition-using-Attention-based-OCR/p2.png" class=""> <center>Figure 2. CNN architecture </center><br><h2 id="How-to-train-this-model"><a href="#How-to-train-this-model" class="headerlink" title="How to train this model?"></a>How to train this model?</h2><p>First of all use this pip command to install aocr on your system.</p><pre><code>pip install aocr</code></pre><p>Now you need dataset of License Plates images with its annotation. Annotation file should have file path and its label in a text file.</p><pre><code>datasets/image1.jpg label1datasets/image2.jpg label2</code></pre><p>After the dataset is ready along with annotation file you have to run this command to create tfrecords file for training of AOCR model. Separate some images for testing and create separate tfrecords file using test annotation file which contains paths of test images and corresponding labels.</p><pre><code>aocr dataset /path/to/training_annotation.txt path/to/training.tfrecordsaocr dataset /path/to/testing_annotation.txt path/to/testing.tfrecords</code></pre><p>The above command need annotation file path as input and creates tfrecords file on the given path i.e. last argument of above command. Now just run the below command to start training procedure.</p><p>By default maximum width is set to 160 and maximum height is set to 60. If your images has width or height more than the default maximum then model will not use those images for training. Either you resize all your images or you can set maximum width and height just make sure all the images are below those values.</p><p>Default checkpoint directory is ‘./checkpoints’ and it will create this where you will execute below command(You can set this too). Just make sure when you test you are giving correct checkpoint path, width and height.</p><p>Maximum prediction length is 8 by default and again you can change it according to your License plates. Default epoch is 1000 change it according to quantity of your dataset if it is small run it for default value otherwise you can set it to 500.</p><pre><code>aocr train /path/to/training.tfrecords --max-width 200 --max-height 100 --model-dir /path/to/checkpoint --max-prediction 6 --num-epoch 500</code></pre><h2 id="Test-the-model"><a href="#Test-the-model" class="headerlink" title="Test the model"></a>Test the model</h2><p>Once the training procedure is finished use this command to test the model. Just make sure checkpoint directory is created when the training starts.</p><pre><code>aocr test /path/to/testing.tfrecords --visualize --max-width 200 --max-height 100 --model-dir /path/to/checkpoint --max-prediction 6 --output-dir ./results</code></pre><p>Now you can see all the prediction inside result folder. For each file there will be one folder which will contain the GIF which will have attention mapped on the images and a text file which will have prediction in the first line and label in the second line. Prediction is placed on the folder name too by default.</p><img src="/blog/2019/11/28/License-plate-recognition-using-Attention-based-OCR/p3.png" class=""> <center>Figure 3.  Result folder directory</center><br><img src="/blog/2019/11/28/License-plate-recognition-using-Attention-based-OCR/p4.png" class=""> <center>Figure 4.  Text file which contains prediction and label</center><br><img src="/blog/2019/11/28/License-plate-recognition-using-Attention-based-OCR/p5.gif" class=""> <center>Gif 1.  GIF with attention map</center><br><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>If you have successfully trained and test the model then you can skip this part. If you have all the training images in one folder and their labels are in the filename itself then you can run this simple script to train your model.</p><h3 id="Note"><a href="#Note" class="headerlink" title="Note:"></a>Note:</h3><p>In case of same label for different images filename will be label_1.extension, label_2.extension etc.</p><p>Execute this script using this command “python3 Train_AOCR.py -d /home/some/path/”</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">import cv2</span><br><span class="line">import os</span><br><span class="line">import shutil</span><br><span class="line">import sys</span><br><span class="line">from pathlib import Path</span><br><span class="line">import optparse</span><br><span class="line"></span><br><span class="line">#python3 Train_AOCR.py -d /DIR_PATH/</span><br><span class="line"></span><br><span class="line"># Give checkpoint path, steps per checkpoints and number of epoch in line 61</span><br><span class="line"></span><br><span class="line">#give images max width and height here</span><br><span class="line">dim = (210, 70)</span><br><span class="line"></span><br><span class="line">parser = optparse.OptionParser()</span><br><span class="line">parser.add_option(&apos;-d&apos;, &apos;--dir_path&apos;,</span><br><span class="line">    action=&quot;store&quot;, dest=&quot;dirpath&quot;,</span><br><span class="line">    help=&quot;Enter test image directory&quot;, default=&quot;Empty&quot;)</span><br><span class="line"></span><br><span class="line">parser.add_option(&apos;-i&apos;, &apos;--image&apos;,</span><br><span class="line">    action=&quot;store&quot;, dest=&quot;image&quot;,</span><br><span class="line">    help=&quot;Input image&quot;, default=&quot;Empty&quot;)</span><br><span class="line"></span><br><span class="line">options, args = parser.parse_args()</span><br><span class="line"></span><br><span class="line">if os.path.exists(&quot;/home/username/path/annotations-training.txt&quot;):</span><br><span class="line">  os.remove(&quot;/home/username/path/annotations-training.txt&quot;)</span><br><span class="line"></span><br><span class="line">if os.path.exists(&quot;/home/username/path/train.tfrecords&quot;):</span><br><span class="line">  os.remove(&quot;/home/username/path/train.tfrecords&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">f_veh = open(&apos;/home/username/path/annotations-training.txt&apos;, &apos;w+&apos;)</span><br><span class="line"></span><br><span class="line">if options.dirpath != &apos;Empty&apos;:</span><br><span class="line">for filename in os.listdir(options.dirpath):</span><br><span class="line"></span><br><span class="line">name, ext = os.path.splitext(filename)</span><br><span class="line">name = name.split(&apos;_&apos;)</span><br><span class="line">img = cv2.imread(options.dirpath+filename)</span><br><span class="line">img = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)</span><br><span class="line">cv2.imwrite(options.dirpath+filename,img)</span><br><span class="line">#os.rename(options.dirpath+filename,options.dirpath+temp[0]+ext)</span><br><span class="line">if ext in [&apos;.png&apos;, &apos;.jpg&apos;,&apos;.jpeg&apos;]:</span><br><span class="line">f_veh.write(options.dirpath+filename+ &apos; &apos;+name[0]+ &apos;\n&apos;)</span><br><span class="line"></span><br><span class="line">comm = &apos;aocr dataset /home/username/path/annotations-training.txt /home/username/path/train.tfrecords&apos;</span><br><span class="line">comm1 = &apos;aocr train /home/username/path/train.tfrecords --model-dir /home/username/path/checkpoints --max-height 70 --max-width 210 --max-prediction 6 --num-epoch 1000&apos; </span><br><span class="line">os.system(comm)</span><br><span class="line">os.system(comm1)</span><br></pre></td></tr></table></figure><p>Now you can test the model on a test set using the above code only same format goes as for the training set and its label.</p><p>You just need to run below code using this command “python3 Run_AOCR.py -d /home/some/test_set_path/”</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">import cv2</span><br><span class="line">import os</span><br><span class="line">import shutil</span><br><span class="line">import sys</span><br><span class="line">from pathlib import Path</span><br><span class="line">import optparse</span><br><span class="line"></span><br><span class="line">#python3 Run_AOCR.py -d /DIR_PATH/</span><br><span class="line"></span><br><span class="line"># Give checkpoint path, steps per checkpoints and number of epoch in line 61</span><br><span class="line"></span><br><span class="line">#give images max width and height here</span><br><span class="line">dim = (210, 70)</span><br><span class="line"></span><br><span class="line">parser = optparse.OptionParser()</span><br><span class="line">parser.add_option(&apos;-d&apos;, &apos;--dir_path&apos;,</span><br><span class="line">    action=&quot;store&quot;, dest=&quot;dirpath&quot;,</span><br><span class="line">    help=&quot;Enter test image directory&quot;, default=&quot;Empty&quot;)</span><br><span class="line"></span><br><span class="line">parser.add_option(&apos;-i&apos;, &apos;--image&apos;,</span><br><span class="line">    action=&quot;store&quot;, dest=&quot;image&quot;,</span><br><span class="line">    help=&quot;Input image&quot;, default=&quot;Empty&quot;)</span><br><span class="line"></span><br><span class="line">options, args = parser.parse_args()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">p = Path(&quot;/home/username/path/results&quot;)</span><br><span class="line">if p.is_dir():</span><br><span class="line">shutil.rmtree(&apos;/home/username/path/results&apos;)</span><br><span class="line"></span><br><span class="line">if os.path.exists(&quot;/home/username/path/annotations-testing.txt&quot;):</span><br><span class="line">  os.remove(&quot;/home/username/path/annotations-testing.txt&quot;)</span><br><span class="line"></span><br><span class="line">if os.path.exists(&quot;/home/username/path/test.tfrecords&quot;):</span><br><span class="line">  os.remove(&quot;/home/username/path/test.tfrecords&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">f_veh = open(&apos;/home/username/path/annotations-testing.txt&apos;, &apos;w+&apos;)</span><br><span class="line"></span><br><span class="line">if options.dirpath != &apos;Empty&apos;:</span><br><span class="line">for filename in os.listdir(options.dirpath):</span><br><span class="line"></span><br><span class="line">name, ext = os.path.splitext(filename)</span><br><span class="line">name = name.split(&apos;_&apos;)</span><br><span class="line">img = cv2.imread(options.dirpath+filename)</span><br><span class="line">img = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)</span><br><span class="line">cv2.imwrite(options.dirpath+filename,img)</span><br><span class="line">#os.rename(options.dirpath+filename,options.dirpath+temp[0]+ext)</span><br><span class="line">if ext in [&apos;.png&apos;, &apos;.jpg&apos;,&apos;.jpeg&apos;]:</span><br><span class="line">f_veh.write(options.dirpath+filename+ &apos; &apos;+name[0]+ &apos;\n&apos;)</span><br><span class="line"></span><br><span class="line">comm = &apos;aocr dataset /home/username/path/annotations-testing.txt /home/username/path/test.tfrecords&apos;</span><br><span class="line">comm1 = &apos;aocr test --visualize /home/username/path/test.tfrecords --model-dir /home/username/path/checkpoints --max-height 70 --max-width 210 --max-prediction 6 --output-dir ./results&apos; </span><br><span class="line">os.system(comm)</span><br><span class="line">os.system(comm1)</span><br></pre></td></tr></table></figure><p>After running this script you can find all the prediction in the output directory.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h2&gt;&lt;p&gt;If you clicked on this post ti
      
    
    </summary>
    
    
      <category term="AI" scheme="https://nayan.co/blog/categories/AI/"/>
    
    
      <category term="Deep Learning" scheme="https://nayan.co/blog/tags/Deep-Learning/"/>
    
      <category term="Machine Learning" scheme="https://nayan.co/blog/tags/Machine-Learning/"/>
    
      <category term="OCR" scheme="https://nayan.co/blog/tags/OCR/"/>
    
      <category term="AOCR" scheme="https://nayan.co/blog/tags/AOCR/"/>
    
      <category term="License Plates" scheme="https://nayan.co/blog/tags/License-Plates/"/>
    
      <category term="License Plates Recognition" scheme="https://nayan.co/blog/tags/License-Plates-Recognition/"/>
    
  </entry>
  
  <entry>
    <title>Sharing modules across Android apps</title>
    <link href="https://nayan.co/blog/2019/11/27/sharing-modules-across-android-apps/"/>
    <id>https://nayan.co/blog/2019/11/27/sharing-modules-across-android-apps/</id>
    <published>2019-11-27T13:00:00.000Z</published>
    <updated>2019-11-28T08:12:01.420Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Sharing-modules-across-Android-apps"><a href="#Sharing-modules-across-Android-apps" class="headerlink" title="Sharing modules across Android apps"></a>Sharing modules across Android apps</h2><p>While most android apps are created with a single default <code>app</code> module, in the last few years people have started<br>moving to a multi module structure for their Android apps. Having multiple smaller modules have a few distinct<br>advantages</p><ul><li>Build times are noticeably faster</li><li>Your code is decoupled with clear dependencies</li><li>Better distribution of ownership across different parts of the app</li><li>Allows modules to be reused across apps</li></ul><h4 id="Example-modules"><a href="#Example-modules" class="headerlink" title="Example modules"></a>Example modules</h4><p>A possible strategy is to have one module per feature.</p><ul><li><code>app</code>: This is the main module which will be the entry point into your app. It acts mainly as a coordinator between<br>other modules</li><li><code>core</code>: This will contain the model definitions that are core to your app and will be required across modules</li><li><code>networking</code>: This provides the networking code for the other modules</li><li><code>login</code>: Login/Signup logic goes here</li><li><code>dashboard</code>: User dashboard will be here</li></ul><img src="/blog/2019/11/27/sharing-modules-across-android-apps/modules.jpg" class="" title="Modules"><p>There are many <a href="https://medium.com/google-developer-experts/modularizing-android-applications-9e2d18f244a0" target="_blank" rel="noopener">posts</a> on the advantages and strategies for modularizing your Android apps. For this post, we will<br>focus on the strategy to reuse modules across apps.</p><h4 id="Reusing-modules-across-apps"><a href="#Reusing-modules-across-apps" class="headerlink" title="Reusing modules across apps"></a>Reusing modules across apps</h4><p>We have multiple apps in our company that share the <code>core</code> and <code>login</code> logic. So we decided to share these modules among<br>the two applications.</p><p><img src="hierarchy.png" alt="hierarchy" title="Hierarchy"></p><p>One obvious way to share Android Library modules would be to share the generated <code>.aar</code> files and add them as<br>dependencies to the different apps. While this is simpler, the main applications and the library modules will be<br>different Android Studio projects. If any change needs to be done in the library, the <code>.aar</code> will have to be regenerated<br>and manually updated. There has to be a better way.</p><p>The solution we decided to use for sharing modules is <a href="https://git-scm.com/book/en/v2/Git-Tools-Submodules" target="_blank" rel="noopener">git submodules</a>. Though it had a small overhead in bringing<br>the entire team up to speed with submodules, it has worked exceptionally well for us.</p><p>In the above example, we have two git submodules, core and login.</p><p><img src="submodules.png" alt="submodules" title="Submodules"></p><p>And the submodules will be added as dependencies just as any Android module,</p><p><img src="dependencies.png" alt="dependencies" title="Dependencies"></p><h4 id="Creating-a-new-submodules"><a href="#Creating-a-new-submodules" class="headerlink" title="Creating a new submodules"></a>Creating a new submodules</h4><p>To create a new submodule, we follow the following process</p><ul><li>Create an empty repo on Github and initialize with a README</li><li>Add a new submodule to our app <code>git submodule add git@github.com:username/core.git</code></li><li>Create a new Android Library module in the new directory</li><li>Commit the changes in the library module and push</li><li>Commit the changes in the main repo and push</li></ul><p>Next time we need to use this submodule in another app, we only need to</p><ul><li>Add a new submodule to our app <code>git submodule add git@github.com:username/core.git</code></li><li>Add the newly added module to <code>settings.gradle</code> and a dependency in <code>build.gradle</code></li></ul><h4 id="Committing-changes-to-a-submodule"><a href="#Committing-changes-to-a-submodule" class="headerlink" title="Committing changes to a submodule"></a>Committing changes to a submodule</h4><p>Every time we make some changes to a submodule, we just need to make sure that we commit and push those changes<br>before committing the changes in the main repo.</p><p>Submodule:</p><ul><li>If you are on a detached head, <code>git checkout -b new-branch</code></li><li><code>git add . &amp;&amp; git commit -am &quot;commit message&quot;</code></li><li><code>git push origin new-branch</code></li></ul><p>Main repo:</p><ul><li><code>git add . &amp;&amp; git commit -am &quot;commit message&quot;</code></li><li><code>git push</code></li></ul><h4 id="Fetching-remote-changes"><a href="#Fetching-remote-changes" class="headerlink" title="Fetching remote changes"></a>Fetching remote changes</h4><p>Every time we do a git pull, we just need to remember to update the submodules as well</p><p><code>git submodule update</code></p><p>and that’s it. We have the latest version of the submodule locally!</p><p>Hope this works for you.</p><p>Happy coding!</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Sharing-modules-across-Android-apps&quot;&gt;&lt;a href=&quot;#Sharing-modules-across-Android-apps&quot; class=&quot;headerlink&quot; title=&quot;Sharing modules across
      
    
    </summary>
    
    
      <category term="Android" scheme="https://nayan.co/blog/categories/Android/"/>
    
    
      <category term="android" scheme="https://nayan.co/blog/tags/android/"/>
    
      <category term="git submodules" scheme="https://nayan.co/blog/tags/git-submodules/"/>
    
  </entry>
  
  <entry>
    <title>Text detection in number plates</title>
    <link href="https://nayan.co/blog/2019/11/26/Text-detection-in-number-plates/"/>
    <id>https://nayan.co/blog/2019/11/26/Text-detection-in-number-plates/</id>
    <published>2019-11-26T15:21:29.000Z</published>
    <updated>2019-11-28T08:12:01.416Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Text-detection-in-number-plates"><a href="#Text-detection-in-number-plates" class="headerlink" title="Text detection in number plates"></a>Text detection in number plates</h1><p>One of the vital modules in the optical character recognition(OCR) pipeline is text detectionand segmentation which is also called text localization. In this post, we will apply variedpreprocessing techniques to the input image and find out how to localize text in theenhanced image, so that we can feed the segments to our text recognition network.</p><h2 id="Image-Preprocessing"><a href="#Image-Preprocessing" class="headerlink" title="Image Preprocessing"></a>Image Preprocessing</h2><p>Sometimes images can be distorted, noisy and other problems that can scale back the OCRaccuracy. To make a better OCR pipeline, we need to do some image preprocessing.</p><ul><li>Grayscale the image: Generally you will get an image which is having 3channels(color images), we need to convert this image into a grayscale form whichcontains only one channel. We can also process images with three channels but itonly increases the complexity of the model and increases the processing time.OpenCV provides a built-in function that can do it for you.</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">import cv2</span><br><span class="line">grayscale_image = cv2.cvtColor(image, cv2.COLOR_BRG2GRAY)</span><br></pre></td></tr></table></figure><p>Or you can convert the image to grayscale while reading the image.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#opencv reads image in BGR format</span><br><span class="line">graysacle_image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)</span><br></pre></td></tr></table></figure><ul><li>Noise reduction: Images come with various types of noises. OpenCV provides a lot ofnoise reduction function. I am using the Non-local Means Denoising algorithm.</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">denoised_image = cv2.fastNlMeansDenoising(grayscale_img, None, 10, 7, 21)</span><br></pre></td></tr></table></figure><img src="/blog/2019/11/26/Text-detection-in-number-plates/Figure_2.png" class="" title="Denoising"><ul><li>Contrast adjustment: Sometimes we have low contrast images. This makes it difficultto separate text from the image background. We need high contrast text images forthe localization process. We can increase image contrast using Contrast LimitedAdaptive Histogram Equalization (CLAHE) among many other contrast enhancementmethods provided by skimage.</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from  skimage import exposure</span><br><span class="line">contrast_enhanced_image = exposure.equalize_adapthist(denoised, clip_limit=0.03)</span><br></pre></td></tr></table></figure><img src="/blog/2019/11/26/Text-detection-in-number-plates/Figure_3.png" class="" title="Contrast Adjustment"><p>So now we are done with image preprocessing let us move on to the second part, textlocalization.</p><h2 id="Text-Localization"><a href="#Text-Localization" class="headerlink" title="Text Localization"></a>Text Localization</h2><p>In this part, we will see how to detect a large number of text region candidates andprogressively removes those less likely to contain text. Using the MSER feature descriptor tofind text candidates in the image. It works well for text because the consistent color and highcontrast of text lead to stable intensity profiles.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#constructor for MSER detector</span><br><span class="line">mser = cv2.MSER_create()</span><br><span class="line">regions, mser_bboxes = mser.detectRegions(contrast_enhance_image)</span><br></pre></td></tr></table></figure><p>Along with the text MSER picked up many other stable regions that are not text. Now, thegeometric properties of text can be used to filter out non-text regions using simplethresholds.</p><p>Before moving on with the filtering process, let’s write some functions to display the results ina comprehensible manner.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">#display images</span><br><span class="line">def pltShow(*images):</span><br><span class="line">    #count number of images to show</span><br><span class="line">    count = len(images)</span><br><span class="line">    #three images per columnn</span><br><span class="line">    Row = np.ceil(count / 3.)</span><br><span class="line">    for i in range(count):</span><br><span class="line">        plt.subplot(nRow, 3, i+1)</span><br><span class="line">        if len(images[i][0], cmap=’gray’)</span><br><span class="line">            plt.imshow(images[i][0], cmap=’gray’)</span><br><span class="line">        else:</span><br><span class="line">            plt.imshow(images[i][0])</span><br><span class="line">        #remove x-y axis from subplots</span><br><span class="line">        plt.xticks([])</span><br><span class="line">        plt.yticks([])</span><br><span class="line">        plt.title(images[i][1])</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">#color each MSER region in image</span><br><span class="line">def colorRegion(image_like_arr, region):</span><br><span class="line">    image_like_arr[region[:, 1], region[:, 0], 0] = np.random.randint(low=100, high=256)</span><br><span class="line">    image_like_arr[region[:, 1], region[:, 0], 1] = np.random.randint(low=100, high=256)</span><br><span class="line">    image_like_arr[region[:, 1], region[:, 0], 2] = np.random.randint(low=100, high=256)</span><br><span class="line"></span><br><span class="line">    return image</span><br></pre></td></tr></table></figure><p>The geometric properties we are going to use to discriminate between text and non-textregion are:</p><ul><li>Region area</li><li>Region perimeter</li><li>Aspect ratio</li><li>Occupancy</li><li>Compactness</li></ul><p>We will apply simple thresholds over these parameters to eliminate non-text regions. Firstlet’s write method to compute these parameters.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">#values for the parameters</span><br><span class="line">AREA_LIM = 2.0e-4</span><br><span class="line">PERIMETER_LIM = 1e-4</span><br><span class="line">ASPECT_RATIO_LIM = 5.0</span><br><span class="line">OCCUPATION_LIM = (0.23, 0.90)</span><br><span class="line">COMPACTNESS_LIM = (3e-3, 1e-1)</span><br><span class="line"></span><br><span class="line">def getRegionShape(self, region): </span><br><span class="line">    return (max(region[:, 1]) - min(region[:, 1]), max(region[:, 0]) - min(region[:, 0]))</span><br><span class="line">    </span><br><span class="line">#compute area</span><br><span class="line">def getRegionArea(region):</span><br><span class="line">    return len(list(region))</span><br><span class="line"></span><br><span class="line">#compute perimeter</span><br><span class="line">def getRegionPerimeter(image, region):</span><br><span class="line">    #get top-left coordinate, width and height of the box enclosing the region</span><br><span class="line">    x, y, w, h = cv2.boundingRect(region)</span><br><span class="line">    return len(np.where(image[y:y+h, x:x+w] != 0)[0]))</span><br><span class="line">    </span><br><span class="line">#compute aspect ratio</span><br><span class="line">def getAspectRatio(region):    </span><br><span class="line">    return (1.0 * max(getRegionShape(region))) / (min(getRegionShape(region)) + 1e-4)</span><br><span class="line"></span><br><span class="line">#compute area occupied by the region area in the shape</span><br><span class="line">def getOccupyRate(region):</span><br><span class="line">    return (1.0 * getRegionArea(region)) / (getRegionShape(region)[0] *  \getRegionShape(region)[1] + 1.0e-10)</span><br><span class="line">    </span><br><span class="line">#compute compactness of the regio</span><br><span class="line">ndef getCompactness(region):    </span><br><span class="line">    return (1.0 * getRegionArea(region)) / (1.0 * getRegionPerimeter(region) ** 2)</span><br></pre></td></tr></table></figure><p>Now apply these methods to filter out text regions  as follows:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">#total number of MSER regions</span><br><span class="line">n1 = len(regions)</span><br><span class="line">bboxes=[]</span><br><span class="line">for i, region in enumerate(regions):</span><br><span class="line">    self.colorRegion(res, region)</span><br><span class="line">    if self.getRegionArea(region) &gt; self.grayImg.shape[0] * self.grayImg.shape[1] * AREA_LIM:</span><br><span class="line">       #number of regions meeting the area criteria</span><br><span class="line">    n2 += 1</span><br><span class="line">    self.colorRegion(res2, region)</span><br><span class="line"></span><br><span class="line">    if self.getRegionPerimeter(region) &gt; 2 * (self.grayImg.shape[0] + \</span><br><span class="line">        self.grayImg.shape[1]) * PERIMETER_LIM:</span><br><span class="line">   #number of regions meeting the perimeter criteria</span><br><span class="line">        n3 += 1</span><br><span class="line">   self.colorRegion(res3, region)</span><br><span class="line"> </span><br><span class="line">        if self.getAspectRatio(region) &lt; ASPECT_RATIO_LIM:</span><br><span class="line">   #number of regions meeting the aspect ratio criteria </span><br><span class="line">                n4 += 1</span><br><span class="line">   self.colorRegion(res4, region)</span><br><span class="line"></span><br><span class="line">   if (self.getOccupyRate(region) &gt; OCCUPATION_LIM[0]) and \ (self.getOccupyRate(region) &lt; OCCUPATION_LIM[1]):</span><br><span class="line">   n5 += 1</span><br><span class="line">   self.colorRegion(res5, region)</span><br><span class="line"></span><br><span class="line">   if (self.getCompactness(region) &gt; \COMPACTNESS_LIM[0]) and \(self.getCompactness(region) &lt; \COMPACTNESS_LIM[1]):</span><br><span class="line">   #final number of regions left </span><br><span class="line">                        n6 += 1</span><br><span class="line">   self.colorRegion(res6, region)</span><br><span class="line">                        bboxes.append(mser_bboxes[i])</span><br></pre></td></tr></table></figure><p>After eliminating non-text regions, I draw bounding boxes on the remaining regions andvoila, we have successfully detected and segmented the characters on the number plate.<br>Note: Apply NMS to remove overlapping bounding boxes.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">for bbox in bboxes:</span><br><span class="line">   cv2.rectangle(img,(bbox[0]-1,bbox[1]-1),(bbox[0]+bbox[2]+1,box[1]+bbox[3]+1),(255,0,0), 1)</span><br></pre></td></tr></table></figure><p>Enough coding. Let’s see some results.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">pltShow(&quot;MSER Result Analysis&quot;, \</span><br><span class="line">      (self.img, &quot;Image&quot;), \</span><br><span class="line">      (self.cannyImg, &quot;Canny&quot;), \</span><br><span class="line">      (res, &quot;MSER,(&#123;&#125; regions)&quot;.format(n1)), \</span><br><span class="line">      (res2, &quot;Area=&#123;&#125;,(&#123;&#125; regions)&quot;.format(config.mser_areaLimit, n2)), \</span><br><span class="line">      (res3, &quot;Perimeter=&#123;&#125;,(&#123;&#125; regions)&quot;.format(config.mser_perimeterLimit, n3)), \</span><br><span class="line">      (res4, &quot;Aspect Ratio=&#123;&#125;,(&#123;&#125; regions)&quot;.format(config.mser_aspectRatioLimit, n4)), \</span><br><span class="line">      (res5, &quot;Occupation=&#123;&#125;,(&#123;&#125; regions)&quot;.format(config.mser_occupationLimit, n5)), \</span><br><span class="line">      (res6, &quot;Compactness=&#123;&#125;,(&#123;&#125; regions)&quot;.format(config.mser_compactnessLimit, n6)), \</span><br><span class="line">      (boxRes, &quot;Segmented Image&quot;) \</span><br><span class="line">   )</span><br></pre></td></tr></table></figure><img src="/blog/2019/11/26/Text-detection-in-number-plates/Figure_1.png" class="" title="MSER Result Analysis"><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>In this post, we covered the various image preprocessing techniques and learned about howto perform text localization on number plates.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Text-detection-in-number-plates&quot;&gt;&lt;a href=&quot;#Text-detection-in-number-plates&quot; class=&quot;headerlink&quot; title=&quot;Text detection in number plate
      
    
    </summary>
    
    
      <category term="AI" scheme="https://nayan.co/blog/categories/AI/"/>
    
    
      <category term="Deep Learning" scheme="https://nayan.co/blog/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Detecting Lanes using Deep Neural Networks</title>
    <link href="https://nayan.co/blog/2019/11/26/Detecting-Lanes-using-Deep-Neural-Networks/"/>
    <id>https://nayan.co/blog/2019/11/26/Detecting-Lanes-using-Deep-Neural-Networks/</id>
    <published>2019-11-26T11:17:10.000Z</published>
    <updated>2019-11-28T08:12:01.408Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>This post explains how to use deep neural networks to detect highway lanes. Lane markings are the main static component on highways.<strong>They instruct the vehicles to interactively and safely drive on the highways.</strong> Lane detection is also an important task in autonomous driving, which provides localization information to the control of the car. It is also used in <strong>ADAS(Advanced Driver Assistance System)</strong>.</p></blockquote><p>For the task of lane detection, we have two open-source datasets available. One is the Tusimple dataset and the other is the CULane dataset. Let’s have a brief look at one of the datasets.</p><h2 id="Tusimple-Dataset"><a href="#Tusimple-Dataset" class="headerlink" title="Tusimple Dataset"></a>Tusimple Dataset</h2><p>This dataset was released as part of the Tusimple Lane Detection Challenge. It contains 3626 video clips of 1 sec duration each. Each of these video clips contains 20 frames of which, the last frame is annotated. These videos were captured by mounting the cameras on a vehicle dashboard. You can download the dataset from <a href="https://github.com/TuSimple/tusimple-benchmark/issues/3" target="_blank" rel="noopener">here</a>.</p><p>The directory structure looks like the figure below,</p><img src="/blog/2019/11/26/Detecting-Lanes-using-Deep-Neural-Networks/dir_structure.png" class="" title="Dataset directory structure"><p>Each sub-directory contains 20 sequential images of which, the last frame is annotated. label_data_(date).json contains labels in JSON format for the last frame. Each line in the JSON file is a dictionary with key values…</p><p><strong>raw_file</strong>: string type. the file path in the clip</p><p><strong>lanes</strong>: it is a list of list of lanes. Each list corresponds to a lane and each element of the inner list is x-coordinate of ground truth lane.</p><p><strong>h_samples</strong>: it is a list of height values corresponding to the lanes. Each element in this list is y-coordinate of ground truth lane</p><p>In this dataset, at most four lanes are annotated - the two ego lanes (two lane boundaries in which the vehicle is currently located) and the lanes to the right and left of ego lanes. All the lanes are annotated at an equal interval of height, therefore h_samples contain only one list whose elements correspond to y-coordinates for all lists in lanes. For a point in h_samples, if there is no lane at the location, its corresponding x-coordinate has -2. For example, a line in the JSON file looks like :</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;lanes&quot;: [</span><br><span class="line">        [-2, -2, -2, -2, 632, 625, 617, 609, 601, 594, 586, 578, 570, 563, 555, 547, 539, 532, 524, 516, 508, 501, 493, 485, 477, 469, 462, 454, 446, 438, 431, 423, 415, 407, 400, 392, 384, 376, 369, 361, 353, 345, 338, 330, 322, 314, 307, 299],</span><br><span class="line">        [-2, -2, -2, -2, 719, 734, 748, 762, 777, 791, 805, 820, 834, 848, 863, 877, 891, 906, 920, 934, 949, 963, 978, 992, 1006, 1021, 1035, 1049, 1064, 1078, 1092, 1107, 1121, 1135, 1150, 1164, 1178, 1193, 1207, 1221, 1236, 1250, 1265, -2, -2, -2, -2, -2],</span><br><span class="line">        [-2, -2, -2, -2, -2, 532, 503, 474, 445, 416, 387, 358, 329, 300, 271, 241, 212, 183, 154, 125, 96, 67, 38, 9, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2],</span><br><span class="line">        [-2, -2, -2, 781, 822, 862, 903, 944, 984, 1025, 1066, 1107, 1147, 1188, 1229, 1269, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2]</span><br><span class="line">       ],</span><br><span class="line">  &quot;h_samples&quot;: [240, 250, 260, 270, 280, 290, 300, 310, 320, 330, 340, 350, 360, 370, 380, 390, 400, 410, 420, 430, 440, 450, 460, 470, 480, 490, 500, 510, 520, 530, 540, 550, 560, 570, 580, 590, 600, 610, 620, 630, 640, 650, 660, 670, 680, 690, 700, 710],</span><br><span class="line">  &quot;raw_file&quot;: &quot;path_to_clip&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>It says that there are four lanes in the image, and the first lane starts at (632,280), the second lane starts at (719,280), the third lane starts at (532,290) and the fourth lane starts at (781,270).</p><h2 id="DataSet-Visualization"><a href="#DataSet-Visualization" class="headerlink" title="DataSet Visualization"></a>DataSet Visualization</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># import required packages</span><br><span class="line">import json</span><br><span class="line">import numpy as np</span><br><span class="line">import cv2</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"># read each line of json file</span><br><span class="line">json_gt = [json.loads(line) for line in open(&apos;label_data.json&apos;)]</span><br><span class="line">gt = json_gt[0]</span><br><span class="line">gt_lanes = gt[&apos;lanes&apos;]</span><br><span class="line">y_samples = gt[&apos;h_samples&apos;]</span><br><span class="line">raw_file = gt[&apos;raw_file&apos;]</span><br><span class="line"># see the image</span><br><span class="line">img = cv2.imread(raw_file)</span><br><span class="line">cv2.imshow(&apos;image&apos;,img)</span><br><span class="line">cv2.WaitKey(0)</span><br><span class="line">cv2.destroyAllWindows()</span><br></pre></td></tr></table></figure><img src="/blog/2019/11/26/Detecting-Lanes-using-Deep-Neural-Networks/image_1.jpg" class="" title="Image 1. Raw image from the dataset"><p>Now see the JSON points visualization on the image</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">gt_lanes_vis = [[(x, y) for (x, y) in zip(lane, y_samples)</span><br><span class="line">                  if x &gt;= 0] for lane in gt_lanes]</span><br><span class="line">img_vis = img.copy()</span><br><span class="line"></span><br><span class="line">for lane in gt_lanes_vis:</span><br><span class="line">    cv2.polylines(img_vis, np.int32([lane]), isClosed=False,</span><br><span class="line">                   color=(0,255,0), thickness=5)</span><br></pre></td></tr></table></figure><img src="/blog/2019/11/26/Detecting-Lanes-using-Deep-Neural-Networks/image_2.jpg" class="" title="Image 2. Label visualications of image"><p>Now, we have understood the dataset, but we can not pass the above image as a label for the neural network since <strong>grayscale images with values ranging from zero to num_classes -1 are to be passed to the deep convolution neural network to outputs an image containing predicted lanes</strong>. So, we need to generate label images for the JSON files. Label images can be generated using <strong>OpenCV</strong> by drawing lines passing through the points in the JSON file.</p><p>OpenCV has an inbuilt function to draw multiple lines through a set of points. OpenCV’s Polylines method can be used here. First, create a mask of all zeros with height and width equal to the raw file’s height and width using numpy. <strong>The image size can be reduced to maintain lesser computations during training, but do not forget to maintain the same aspect ratio</strong>.</p><h2 id="Generating-Labels"><a href="#Generating-Labels" class="headerlink" title="Generating Labels"></a>Generating Labels</h2><p>The label should be a grayscale image. Generate one label for each clip from the JSON file. First, create a mask of black pixels with a shape similar to the raw_file image from the JSON file. Now, using OpenCV’s polylines method draw lines with different colors (each corresponding to each lane in lanes) on the mask image using the lanes and h_samples from the JSON file. From the three channeled mask image generate a gray scale mask image with values as class numbers. Likewise, create labels for all the images in the JSON file. You can resize the image and its label to a smaller size for lesser computations.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">mask = np.zeros_like(img)</span><br><span class="line">colors = [[255,0,0],[0,255,0],[0,0,255],[0,255,255]]</span><br><span class="line">for i in range(len(gt_lanes_vis)):</span><br><span class="line">    cv2.polylines(mask, np.int32([gt_lanes_vis[i]]), isClosed=False,color=colors[i], thickness=5)</span><br><span class="line">!! create grey-scale label image</span><br><span class="line">label = np.zeros((720,1280),dtype = np.uint8)</span><br><span class="line">for i in range(len(colors)):</span><br><span class="line">   label[np.where((mask == colors[i]).all(axis = 2))] = i+1</span><br></pre></td></tr></table></figure><img src="/blog/2019/11/26/Detecting-Lanes-using-Deep-Neural-Networks/image_3.jpg" class="" title="Image 3. Generated mask image"><h2 id="Build-and-Train-Model"><a href="#Build-and-Train-Model" class="headerlink" title="Build and Train Model"></a>Build and Train Model</h2><p>Lane Detection is essentially an image segmentation problem. So I am using the <strong>ERFNET model</strong> for this task, which is efficient and fast. Originally ERFNET was proposed for semantic segmentation problems, but it can also be extended to other image segmentation problems. You can check out for its paper <a href="https://ieeexplore.ieee.org/abstract/document/8063438" target="_blank" rel="noopener">here</a>. It is a CNN with Encoder, Decoder and dilated convolutions along with non-bottleneck residual layers. See Fig.1 for model architecture.</p><img src="/blog/2019/11/26/Detecting-Lanes-using-Deep-Neural-Networks/figure_1.png" class="" title="Figure 1. Model Architecture"><p>Build and create an object of the model. Train it over the dataset created above, for a sufficient number of epochs with Binary Cross Entropy loss or custom loss function which minimizes the per-pixel error. For better memory usage, create a dataset generator and train the model over it. Generators remove the burden of loading all the images into memory (if your dataset is of large size, you should use a generator) which leads to eating up of all memory and the other processes can’t work properly. Fig 2 shows the layers of ERFNET with input and output dimensions.</p><img src="/blog/2019/11/26/Detecting-Lanes-using-Deep-Neural-Networks/figure_2.png" class="" title="Figure 2. Model layers with input and output shapes"><h2 id="Evaluate-Model"><a href="#Evaluate-Model" class="headerlink" title="Evaluate Model"></a>Evaluate Model</h2><p>After training, get the model’s predictions using the code snippet below. I have implemented this in Pytorch. I use the color_lanes method to convert output images from the model (which are two channeled with values as class numbers) to three channeled images. im_seg is the final overlayed image shown in Image 4.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">#using pytorch</span><br><span class="line">import torch</span><br><span class="line">from torchvision.transforms import ToTensor</span><br><span class="line">def color_lanes(image, classes, i, color, HEIGHT, WIDTH):</span><br><span class="line">    buffer_c1 = np.zeros((HEIGHT, WIDTH), dtype=np.uint8)</span><br><span class="line">    buffer_c1[classes == i] = color[0]</span><br><span class="line">    image[:, :, 0] += buffer_c1</span><br><span class="line">    buffer_c2 = np.zeros((HEIGHT, WIDTH), dtype=np.uint8)</span><br><span class="line">    buffer_c2[classes == i] = color[1]</span><br><span class="line">    image[:, :, 1] += buffer_c2</span><br><span class="line">    buffer_c3 = np.zeros((HEIGHT, WIDTH), dtype=np.uint8)</span><br><span class="line">    buffer_c3[classes == i] = color[2]</span><br><span class="line">    image[:, :, 2] += buffer_c3</span><br><span class="line">    return image</span><br><span class="line">img = cv2.imread(&apos;images/test.jpg&apos;) </span><br><span class="line">img = cv2.resize(img,(WIDTH, HEIGHT),interpolation = cv2.INETR_CUBIC)</span><br><span class="line">op_transforms = transforms.Compose([transforms.ToTensor()])</span><br><span class="line">device = torch.device(&apos;cuda&apos; if torch.cuda.is_available() else &apos;cpu&apos;)</span><br><span class="line">im_tensor = torch.unsqueeze(op_transforms(img), dim=0)</span><br><span class="line">im_tensor = im_tensor.to(device)</span><br><span class="line">model = ERFNET(5)</span><br><span class="line">model = model.to(device)</span><br><span class="line">model = model.eval()</span><br><span class="line">out = model(im_tensor)</span><br><span class="line">out = out.max(dim=1)[1]</span><br><span class="line">out_np = out.cpu().numpy()[0]</span><br><span class="line">out_viz = np.zeros((HEIGHT, WIDTH, 3))</span><br><span class="line">for i in range(1, NUM_LD_CLASSES):</span><br><span class="line">    rand_c1 = random.randint(1, 255)</span><br><span class="line">    rand_c2 = random.randint(1, 255)</span><br><span class="line">    rand_c3 = random.randint(1, 255)</span><br><span class="line">    out_viz = color_lanes(</span><br><span class="line">            out_viz, out_np,</span><br><span class="line">            i, (rand_c1, rand_c2, rand_c3), HEIGHT, WIDTH)</span><br><span class="line">instance_im = out_viz.astype(np.uint8)</span><br><span class="line">im_seg = cv2.addWeighted(img, 1, instance_im, 1, 0)</span><br></pre></td></tr></table></figure><img src="/blog/2019/11/26/Detecting-Lanes-using-Deep-Neural-Networks/image_4.jpg" class="" title="Image 4. Final predicted image"><p>Thanks for reading it…</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol><li>ERFNet: Efficient Residual Factorized ConvNet for Real-time Semantic 2. Segmentation.</li><li>Lane Detection and Classification using CNNs.</li><li><a href="https://www.mdpi.com/sensors/sensors-19-00503/article_deploy/html/images/sensors-19-00503-g004.png" target="_blank" rel="noopener">https://www.mdpi.com/sensors/sensors-19-00503/article_deploy/html/images/sensors-19-00503-g004.png</a></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;This post explains how to use deep neural networks to detect highway lanes. Lane markings are the main static component on h
      
    
    </summary>
    
    
      <category term="AI" scheme="https://nayan.co/blog/categories/AI/"/>
    
    
      <category term="Deep Learning" scheme="https://nayan.co/blog/tags/Deep-Learning/"/>
    
      <category term="Machine Learning" scheme="https://nayan.co/blog/tags/Machine-Learning/"/>
    
      <category term="Lane Detection" scheme="https://nayan.co/blog/tags/Lane-Detection/"/>
    
      <category term="Advance Driver Assistance" scheme="https://nayan.co/blog/tags/Advance-Driver-Assistance/"/>
    
      <category term="Autonomous Driving" scheme="https://nayan.co/blog/tags/Autonomous-Driving/"/>
    
  </entry>
  
</feed>
