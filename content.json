{"meta":{"title":"Nayan Blog","subtitle":"","description":"","author":"NayanTech","url":"https://nayan.co","root":"/blog/"},"pages":[],"posts":[{"title":"tensorflow-docker","slug":"tensorflow-docker","date":"2020-08-26T05:49:07.516Z","updated":"2020-08-26T05:49:07.516Z","comments":true,"path":"/uncategorized/tensorflow-docker/","link":"","permalink":"https://nayan.co/uncategorized/tensorflow-docker/","excerpt":"","text":"Installing Tensorflow 2.0 on Ubuntu 18.04 using docker. Run all experiments from a container.I have come across many developers who face serious issues when it comes to installing tensorflow on a linux distro such as Ubuntu. There are very few instances when the installation goes smoothly the first time itself. Mostly, the developer has to face a plethora of error messages which get quite tricky to solve. Some of the error messages are shown below: [...\\stream_executor\\dso_loader.cc] Couldn&apos;t open CUDA library nvcuda.dll [...\\stream_executor\\cuda\\cuda_dnn.cc] Unable to load cuDNN DSOSimilar error messages can be found in this official tensorflow link → https://www.tensorflow.org/install/errors Let’s make things easier and lives simpler: Enter Docker:Using Docker containers the life of a developer becomes easier by a massive amount. Many developers shy away from using docker containers thinking that it introduces extra dependencies into the system followed by maintenance issues, but that’s a misconception. Docker containers actually reduce the time spent on figuring out different library versions to be installed and how they would communicate with one another. Containers solve multiple issues which arise with incompatibility of libraries and version mismatch. A container is completely independent from it’s host and reduces the chances of ruining environments on the host machine Installing Docker and NVIDIA Docker :In order to run tensorflow as a container we would obviously need the latest version of docker to be installed and configured. Along with that we would also need NVIDIA Docker v2 to be installed on the host machine. NVIDIA Docker plays a beautiful role of exposing the host machine’s GPU and GPU drivers to a container. Hence the developer only has to worry about installing the correct NVIDIA GPU driver on this machine. The NVIDIA Docker v2 does the task of making it available for the container. In order to learn how to install the latest version of Docker and NVIDIA Docker v2 , head over to my earlier post which describes this is in detail. Link given below: Deep Learning for Production: Deploying YOLO using Docker. Installing tensorflow using Docker:Once your docker and NVIDIA docker v2 installation is complete with nvidia-smi giving you the output as shown in Fig 1, when run inside a docker container, we can move ahead with pulling the correct image for tensorflow. Simply doing a docker pull tensorflow/tensorflow would download the latest version of tensorflow image. This can be run using the following command docker run -it -rm --runtime=nvidia --name=tensorflow_container ***tensorflow_image_name***Executing the command given above will run the tensorflow container in an interactive shell along with the availability of the NVIDIA gpus inside the container. Now there are certain modifications which can be performed to get the tensorflow version required along with other libraries. Let’s say you want the latest version of tensorflow along with gpu support and python 3 pre-installed. The image for this customization can be easily pulled using the following command: docker pull tensorflow/tensorflow:latest-gpu-py3You can find many other such images in the following link →https://hub.docker.com/r/tensorflow/tensorflow/tags Just do a docker pull on the one which suits your requirement. If you want other libraries along with tensorflow, you can put them in a dockerfile and perform build command. Fig 2 above shows a custom dockerfile with tensorflow v1 image being used along with installation of other libraries such as OpenCV,Moviepy,Keras and Apache Kafka for python Once inside the container invoked using docker run, you can setup code to use tensorflow easily as you would done on the host machine without the container. I would encourage all AI/ML practitioners to increase the use of docker containers to increase their research and development efficiency by reducing the time spent in managing multiple libraries and dueling with incompatibility errors.","categories":[],"tags":[]},{"title":"Setup CI/CD on React app using GitHub Actions","slug":"react-cicd","date":"2020-08-20T15:08:25.000Z","updated":"2020-08-26T05:49:07.516Z","comments":true,"path":"/Web/react-cicd/","link":"","permalink":"https://nayan.co/Web/react-cicd/","excerpt":"","text":"In this tutorial, I’m going to show you how to create a simple workflow that I use on my personal projects with React. This workflow created on GitHub Actions will be responsible for automatically test the source code, generate a test coverage report and upload it on Codecov, build and deploy the project on GitHub Pages. All these jobs are activated by a push or pull request event on master branch. Getting startedFirst, on your React App GitHub repository, navigate to the main page of the repository, click Actions. Then, you’ll see suggestions of CI templates that are the best fit for your project. You can use workflow templates as a starting place to build your custom workflow.In this case, click Set up this workflow, under the name of the template Node.js. Finally, you’ll see a default YAML file like this: 1234567891011121314151617181920212223242526name: Node.js CIon: push: branches: [ master ] pull_request: branches: [ master ]jobs: build: runs-on: ubuntu-latest strategy: matrix: node-version: [10.x, 12.x, 14.x] steps: - uses: actions/checkout@v2 - name: Use Node.js $&#123;&#123; matrix.node-version &#125;&#125; uses: actions/setup-node@v1 with: node-version: $&#123;&#123; matrix.node-version &#125;&#125; - run: npm ci - run: npm run build --if-present - run: npm test Test coverage report generationLets update workflow abit to add test coverage step in our workflow 12345678910111213141516171819202122232425262728293031323334name: CI/CDon: push: branches: [ master ] pull_request: branches: [ master ]jobs: build: runs-on: ubuntu-latest strategy: matrix: node-version: [12.x] steps: - name: Checkout repository uses: actions/checkout@v2 - name: Set up Node.js $&#123;&#123; matrix.node-version &#125;&#125; uses: actions/setup-node@v1 with: node-version: $&#123;&#123; matrix.node-version &#125;&#125; - name: Install dependencies run: npm install - name: Run the tests run: npm test - name: Build run: npm run build Setup codecov test coverageFirst, go to the Codecov website and create an account, you can use your GitHub account to sign up. Next, access your account on the website, click Repositories, after, click Add new repository and choose the repository you want to use. (we will setup secret in abit) Deployment on GitHub PagesInstall gh-pages and add deploy to scripts in package.json.Run: 1$ npm install --save gh-pages Add the following scripts in your package.json: 12&quot;predeploy&quot;: &quot;npm run build&quot;,&quot;deploy&quot;: &quot;gh-pages -d build&quot;, Now we are going to create a new access token, in order to deploy our application through the workflow. Go to the Personal access tokens area in the Developer settings of your GitHub profile and click Generate new token. Copy the generated token. On GitHub, navigate to the main page of the repository, under your repository name, click Settings. In the left sidebar, click Secrets. And finally, Click Add a new secret. Type a name for your secret in the Name input box, like ACTIONS_DEPLOY_ACCESS_TOKEN. Enter the value for your secret (which should be the personal access token we just created). Click Add secret. Your last step on your workflow should be some like this: 1234567891011- name: Deploy run: | git config --global user.name $user_name git config --global user.email $user_email git remote set-url origin https://$&#123;github_token&#125;@github.com/$&#123;repository&#125; npm run deploy env: user_name: &apos;github-actions[bot]&apos; user_email: &apos;github-actions[bot]@users.noreply.github.com&apos; github_token: $&#123;&#123; secrets.ACTIONS_DEPLOY_ACCESS_TOKEN &#125;&#125; repository: $&#123;&#123; github.repository &#125;&#125; The name and email information need not necessarily be your real information. And you must replace ACTIONS_DEPLOY_ACCESS_TOKEN with the name of the secret you just created. After adding the deploy command on your workflow, click Start commit, and click Commit new file. Your final workflow file should be like this: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849name: CI/CDon: push: branches: [ master ] pull_request: branches: [ master ]jobs: build: runs-on: ubuntu-latest strategy: matrix: node-version: [12.x] steps: - name: Checkout repository uses: actions/checkout@v2 - name: Set up Node.js $&#123;&#123; matrix.node-version &#125;&#125; uses: actions/setup-node@v1 with: node-version: $&#123;&#123; matrix.node-version &#125;&#125; - name: Install dependencies run: npm install - name: Run the tests and generate coverage report run: npm test -- --coverage - name: Upload coverage to Codecov uses: codecov/codecov-action@v1 - name: Build run: npm run build - name: Deploy run: | git config --global user.name $user_name git config --global user.email $user_email git remote set-url origin https://$&#123;github_token&#125;@github.com/$&#123;repository&#125; npm run deploy env: user_name: &apos;github-actions[bot]&apos; user_email: &apos;github-actions[bot]@users.noreply.github.com&apos; github_token: $&#123;&#123; secrets.ACTIONS_DEPLOY_ACCESS_TOKEN &#125;&#125; repository: $&#123;&#123; github.repository &#125;&#125; Now, in every push or pull request event on master branch, the CI/CD workflow will be activated. And you will be able to see if all steps have passed or not. References:- GitHub actions Actions docs for JS Some good reads you may like:- Angular Youtube integration Angular maps and clusters","categories":[{"name":"Web","slug":"Web","permalink":"https://nayan.co/categories/Web/"}],"tags":[{"name":"web","slug":"web","permalink":"https://nayan.co/tags/web/"},{"name":"react","slug":"react","permalink":"https://nayan.co/tags/react/"},{"name":"javascript","slug":"javascript","permalink":"https://nayan.co/tags/javascript/"},{"name":"CICD","slug":"CICD","permalink":"https://nayan.co/tags/CICD/"},{"name":"Github-actions","slug":"Github-actions","permalink":"https://nayan.co/tags/Github-actions/"},{"name":"Code coverage","slug":"Code-coverage","permalink":"https://nayan.co/tags/Code-coverage/"}],"author":"Abhishek Rana"},{"title":"Quick guide for dynamic feature delivery using android application bundle","slug":"Dynamic-feature-deployment-in-android","date":"2020-08-12T10:00:00.000Z","updated":"2020-08-26T05:49:07.412Z","comments":true,"path":"/Android/Dynamic-feature-deployment-in-android/","link":"","permalink":"https://nayan.co/Android/Dynamic-feature-deployment-in-android/","excerpt":"","text":"With Google Play’s Dynamic Delivery, your app can download dynamic feature modules on demand to devices running Android 5.0 (API level 21) and higher. This helps reducing initial apk size that user need to down first time. It has been observed that not all user uses all features of an mobile application. Hence this make sense that only the bare minimum functional feature to be present in initial apk which can be described as core funcitionality of your application. Note : For every 6 MB increase to an apk size, it has been observed that install rate deduces by 1 %. Considering this, we should use dynamic feature delivery aproach to reduce initial apk size, and if need those features can be downloaded on demand. Initially, the Google Play Store pushes only the code and resources needed for base module to the device, which is app module. For the rest of the modules, Your app simply needs to call APIs in the Play Core Library to download and install those modules as required, on demand for your Android Apps. Why you should be considering this?The benefit of split APKs is the ability to break up a monolithic APK—that is, an APK that includes code and resources for all features and device configurations your app supports—into smaller, discrete packages that are installed on a user’s device as required. Getting startedIn an order to implement dyanmic feature delivery, you might need to update your application structure to extract out code of an existing feature. If you are setting up a new project then, you can start right away. Add below mentioned dependency in app/build.gradle file 12// app/build.gradleimplementation 'com.google.android.play:core:1.8.0' Create a new dynamic feature module from menu. Let’s name it New_Feature. This “New Feature” just like your “app module” containing activities, resources and other library in it’s build.gradle file. The only different is, it uses “dynamic feature as plugin” and has “app” as module dependency. Meaning it inherits everything from app module. 12345678// New_Feature/build.gradleapply plugin: 'com.android.dynamic-feature'dependencies &#123; implementation project(':app') ...&#125; Before you try to open up any activity or try to call any utility function or code from New_Feature, you need download by executing below mentioned code from calling calling activity. 12345678910111213141516171819private void installNew_FeatureModule(String className) &#123; // Get a reference to Split APK Install Manager splitInstallManager = SplitInstallManagerFactory.create(App.getInstance()); // Code to download New_Feature on demand SplitInstallRequest request = SplitInstallRequest.newBuilder() .addModule(\"New_Feature\") .build(); splitInstallManager.startInstall(request) .addOnSuccessListener(integer -&gt; &#123; Toast.makeText(DashboardActivity.this, \"New Feature installed\",Toast.LENGTH_SHORT).show(); &#125;) .addOnFailureListener(e -&gt; &#123; Toast.makeText(DashboardActivity.this, \"unable to download New feature\",Toast.LENGTH_SHORT).show(); &#125;);&#125; To launch any activity from New_Feature, need to pass full class name prefixed with package name like com.example.New_Feature.TestActivity 123456// Launching Test activity of New Feature module from App moduleprivate void launchTestActivity() &#123; Intent i = new Intent(); i.setClassName(BuildConfig.APPLICATION_ID, \"com.example.New_Feature.TestActivity\"); startActivity(i);&#125; ConclusionAs you can see, implementing dynamic feature for a new project is quiet easy and simple. You should try it out for your next project. Also If you want to try out for one of your existing projects, you can but there is an effort involved in decoupling of your features. References:- Read more about Dynamic Delivery Learn how to restructure your project for dynamic delivery Some good reads you may like:- Read here to know more about Android Testing Strategy Checkout To integrate Paytm Gateway Integration","categories":[{"name":"Android","slug":"Android","permalink":"https://nayan.co/categories/Android/"}],"tags":[{"name":"android","slug":"android","permalink":"https://nayan.co/tags/android/"},{"name":"kotlin","slug":"kotlin","permalink":"https://nayan.co/tags/kotlin/"},{"name":"JAVA","slug":"JAVA","permalink":"https://nayan.co/tags/JAVA/"},{"name":"Android Architecture","slug":"Android-Architecture","permalink":"https://nayan.co/tags/Android-Architecture/"},{"name":"App Bundle","slug":"App-Bundle","permalink":"https://nayan.co/tags/App-Bundle/"},{"name":"Dynamic Feature","slug":"Dynamic-Feature","permalink":"https://nayan.co/tags/Dynamic-Feature/"},{"name":"Puneet","slug":"Puneet","permalink":"https://nayan.co/tags/Puneet/"}],"author":"Puneet"},{"title":"How to recognize Emotions using deep learning?","slug":"emotion-recognition-sagar","date":"2020-08-06T14:16:00.000Z","updated":"2020-08-26T05:49:07.480Z","comments":true,"path":"/AI/emotion-recognition-sagar/","link":"","permalink":"https://nayan.co/AI/emotion-recognition-sagar/","excerpt":"","text":"Emotion Recognition is used for categorizing the emotions into one of the 6 emotions through the images of face. Intially viola jones or other face detection algorithm is used for detecting the faces. Preprocessing is also used for standardizing the images into same sizes. Facial Feature extraction is used in a number of applications including emotion detection. In the following approach various popular feature descriptors, including Gabor features, HOG, DWT were computed. We have fused features using Multiview Distance Metric Learning (MDML) which utilizes complementary features of the images to extract every known detail while eliminating the redundant features. Moreover MDML maps the features extracted from the dataset to higher discriminative space. The features belonging to the same class are brought closer and those that are from different classes are forced away by the MDML thereby increasing the accuracy of the classifier employed. CK+ Dataset has been used to conduct the experiments. Experimental resultsrepresent the efficacy of the method is 93.5% displaying the potential of the recommended manner. The human facial expressions contain clues to the emotional state of a person. Computation of emotion of a human through facial expression has been a central topic of Human Computer Interaction (HCI) research and a concern for researchers. The machines can utilize an emotion detection tool to understand the human emotions better and respond accordingly. Facial Expressions have been analyzed and put to use by various researchers such as in [6] the pain level can be recognized by detecting the facial expression. It can be used in case of patients who are mute or are not able to speak because of a disorder. [4] judges the concentration level of a viewer watching an advertisement. It has been employed for detecting various mental disorders such as depression and anxiety in [6]. As Paul Ekman showed in [1], the emotions are universally expressed in the same way regardless of the culture, nationality etc. Subtlety, variance of facial expression and complex nature of emotion further complicate the process. Six emotions have been defined by [1] in his paper. They are happy, Anger, Sad, Surprise, disgust and fear. Emotion detection is accomplished in three basic steps. Firstly, the images are preprocessed and a high quality facial image is extracted from the original image from the dataset. Secondly, feature descriptors are applied to the image for feature extraction. After the dimension reduction by a dimension reduction algorithm such as PCA, LDA, etc a classifier is used for classifying the image into one of emotion. Since the Extended Cohn Kanade Dataset used is free from illumination variance, occlusion and head pose are some of the major concerns in many other datasets results shown are superior than most of the previous methods. The proposed approach is novel in detecting and computing the emotions through Multiview Distance Metric Learning (MDML) introduced in [8]. The similar looking data from different classes are often misclassified. MDML helps to reduce this misclassification and fuses the results of various feature descriptors and extracts complementary features from them. Moreover MDML maps the features extracted from the dataset to higher discriminative space. The features having affinity to the same emotion are brought closer while those having affinity to different emotion classes are moved as far as possible. The classifier is trained using this data. A. Emotion recognition The primary step in emotion detection is the localization of the face. CK+ dataset has been recorded in a controlled having illumination invariance and free from any occlusions. Hence, the facial image is always present in the image and priorly the face’s global position is known. Since wide variations are present in the scale and yaw, pitch, roll of the face, accurately localizing the face in different images is a very challenging task. The various obstructions and hindrances such as glasses and facial hair further complicates the task [1]. Viola-Jones [2] found an efficient answer for localizing the face. This method is based on AdaBoost learning and Haar-like features. Earlier it was developed for finding and detecting objects in an image [3]. It granted fast object detection for the task of real time usage. Keeping in mind the accuracy of the method, it is used for face localization. B. Feature Descriptor Feature extraction indicates deriving pertinent data from the preprocessed, gray scaled images from dataset, that will be employed for the intent of emotion detection. It is a defying task due to the above mentioned reasons. A comprehensive method is to derive descriptors from images is to use Log-Gabor filters as done in [2] by nominating the random bandwidth for making the Log-Gabor filter. Another attainable answer is to use model based method for the facial expression recognition [5]. The authors in [5] constructed a light source and subject independent global representation of expressions by discovering of the set of 322 image sequences in CK+ database. [7] utilizes LBP features and finds the emotion. C. Multiview Distance Metric Learning Motivated by the discriminative mapping and better results of MDML we are using MDML proposed by [8] which attracts the features of the same class together and pushes away that belong to the different class. The data is then mapped to higher discriminative space and complementary features are extracted from the dataset. This ensures all the details of the image are taken into account. MDML pulls the similar data together and pushes away the different data away. In [8] theauthors used MDML for Pain intensity classification. D. Facial Expression Classification Finally, after being mapped to higher discriminative space features are enforced into the classifier for identification of the six emotions. The most sought after classifiers include template matching, rule based classifier, neural networks and support vector machines (SVM). In SVM, a hyperplane is constructed with ample distance among the different emotion classes during the training phase. While during test phase, the trained SVM model classifies the test image to one of the emotion classes. In [7] the authors utilize SVM with linear kernel for expression classification. There are six basic emotion classes which are Anger, Disgust, Happy, Sad, Surprise and Fear for analysis of proposed algorithm. The author of this blog is working currently with Nayan Bibliography [1] Ekman, P., &amp; Friesen, W. V. (1971). Constants across cultures in the face and emotion. Journal of personality and social psychology, 17(2), 124. [2] L. S. Chen. Joint processing of audio-visual information for the recognition of emotional expressions in human-computer interaction. PhD thesis, University of Illinois at UrbanaChampaign, Dept. of Electrical Engineering, 2000. [3] Viola, Paul, and Michael Jones. “Rapid object detection using a boosted cascade of simple features.” Computer Vision and Pattern Recognition, 2001. CVPR 2001. Proceedings of the 2001 IEEE Computer Society Conference on. Vol. 1. IEEE, 2001. [4] S. Lajevardi and M. Lech, “Facial expression recognition from imagesequences using optimized feature selection,” Image and Vision Computing New Zealand, 2008. IVCNZ 2008. 23rd International Conference.pp. 1–6, 2008. J. T. Cacioppo and L.G. Tassinary. Inferring psychological significance from physiological signals. American Psychologist, 45:16–28, January 1990. [5] Kaltwang, S., Rudovic, O., &amp; Pantic, M. (2012). Continuous pain intensity estimation from facial expressions. Advances in visual computing, 368-377 [6] Rathee, N., Vaish, A., &amp; Gupta, S. (2016, April). Adaptive system to learn and recognize emotional state of mind. In Computing, Communication and Automation (ICCCA), 2016 International Conference on (pp. 32-36). IEEE. [7] Rathee, N., &amp; Ganotra, D. (2016). Multiview Distance Metric Learning on facial feature descriptors for automatic pain intensity detection. Computer Vision and Image Understanding, 147, 77-86. [8] Dalal, N., &amp; Triggs, B. (2005, June). Histograms of oriented gradients for human detection. In Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on (Vol. 1, pp. 886-893). IEEE.","categories":[{"name":"AI","slug":"AI","permalink":"https://nayan.co/categories/AI/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://nayan.co/tags/AI/"},{"name":"Emotion Recognition","slug":"Emotion-Recognition","permalink":"https://nayan.co/tags/Emotion-Recognition/"},{"name":"HCI","slug":"HCI","permalink":"https://nayan.co/tags/HCI/"}],"author":"Sagar Gupta"},{"title":"How to host any port to entire internet on any machine?","slug":"jupyter-hosting","date":"2020-07-31T08:00:00.000Z","updated":"2020-08-26T05:49:07.484Z","comments":true,"path":"/AI/jupyter-hosting/","link":"","permalink":"https://nayan.co/AI/jupyter-hosting/","excerpt":"","text":"In this short blog post, I will explain a great trick to expose various services like Jupyter-notebook, tensorboard, etc. to the entire internet. We generally use it at Nayan Before following the steps I want you to understand some fundamentals behind the hosting of different services. Some Basics:Whenever we host some services on a local server having URLs like (https://localhost:8888 or http://127.0.0.1:8888) they are behind a NAT or firewall of our computer(Most of the hackers work to breach these). To jump over the firewall, we will use ngrok. Ngrok:Ngrok allows you to expose a web server running on your local machine to the internet. Just tell ngrok what port your web server is listening on.something like this: Steps to host Jupyter-Notebook on AWS EC2: First, we need to install tmux for running processes(jupyter in our case) in background and jupyter notebook. 1x 1r 2) Download ngrok using 1wget [https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip](https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip) 3) Unzip to install ngrok 1unzip ngrok.zip 4) Create an account on ngrok and get authtoken. 1./ngrok authtoken &lt;your_auth_token&gt; Now on tmux we will run jupyter-notebook 1tmux 1jupyter-notebook --ip=0.0.0.0 --allow-root ``` Host the noted port using ngrok. For my case it is 8890 1./ngrok http 8890 Also, you can make a config file and can host multiple ports using the same account as mentioned here. Now note the URL you got on ngrok screen. Cheers and now hit the URL as many times as you can to access your favorite jupyter notebook. Conclusion:In this cool blogpost, we understood to host jupyter notebook on the local machine. We can host other services like tensorboard or anything you want.","categories":[{"name":"AI","slug":"AI","permalink":"https://nayan.co/categories/AI/"}],"tags":[{"name":"AI","slug":"AI","permalink":"https://nayan.co/tags/AI/"},{"name":"Deep learning","slug":"Deep-learning","permalink":"https://nayan.co/tags/Deep-learning/"},{"name":"jupyter","slug":"jupyter","permalink":"https://nayan.co/tags/jupyter/"},{"name":"ngrok","slug":"ngrok","permalink":"https://nayan.co/tags/ngrok/"},{"name":"Kunal Goyal","slug":"Kunal-Goyal","permalink":"https://nayan.co/tags/Kunal-Goyal/"}],"author":"Kunal Goyal"},{"title":"How to apply Proguard in an android application (Code obfuscation, shrinking, optimization)","slug":"Applying-Proguard-in-an-android-application","date":"2020-07-27T18:40:48.000Z","updated":"2020-08-26T05:49:07.408Z","comments":true,"path":"/Android/Applying-Proguard-in-an-android-application/","link":"","permalink":"https://nayan.co/Android/Applying-Proguard-in-an-android-application/","excerpt":"","text":"To make an application is not good enough, but it also needs to make secure and optimize. It’s the basic needs of an application. To make your app as small as possible, you should enable shrinking in your release build to remove unused code and resources. When enabling shrinking, you also benefit from obfuscation, which shortens the names of your app’s classes and members, and optimization, which applies more aggressive strategies to further reduce the size of your app. ProGuard is a tool used to do the following in an Android application: Minify the codeDetects and safely removes unused classes, fields, methods, and attributes from your app and its library dependencies Obfuscate the codeShortens the name of classes and members, which results in reduced DEX file sizes. Optimize the codeInspects and rewrites your code to further reduce the size of your app’s DEX files. To enable shrinking, obfuscation, and optimization, include the following in your project-level build.gradle file. In that data class, we will handle some sessions related tasks (check session is active or not, update session etc.). A session will have two values start time and end time. 123456789101112android &#123; buildTypes &#123; release &#123; shrinkResources true minifyEnabled true proguardFiles getDefaultProguardFile( &apos;proguard-android-optimize.txt&apos;), &apos;proguard-rules.pro&apos; &#125; &#125; ...&#125; While enabling proguard in your application there are some rules , that should be considered. Do not forget to add the Proguard rules in proguard-rules.pro file for any library that you have included in your project. Like this for classes, also can apply for members and fields 1-keepclassmembers class &lt;className with pakage&gt;.** &#123; *; &#125; For Warning : You need to take a look on stacktrace to find which classes gives those warnings 12-dontwarn &lt;classes_name&gt;-dontwarn java.nio.file.* Some stats related to APK size Referenceshttps://developer.android.com/studio/build/shrink-code Some good reads you may like:- App-Heartbeat","categories":[{"name":"Android","slug":"Android","permalink":"https://nayan.co/categories/Android/"}],"tags":[{"name":"android","slug":"android","permalink":"https://nayan.co/tags/android/"},{"name":"Diwakar Singh","slug":"Diwakar-Singh","permalink":"https://nayan.co/tags/Diwakar-Singh/"},{"name":"proguard","slug":"proguard","permalink":"https://nayan.co/tags/proguard/"},{"name":"kotlin","slug":"kotlin","permalink":"https://nayan.co/tags/kotlin/"},{"name":"shrinking","slug":"shrinking","permalink":"https://nayan.co/tags/shrinking/"},{"name":"obfuscation","slug":"obfuscation","permalink":"https://nayan.co/tags/obfuscation/"},{"name":"optimization","slug":"optimization","permalink":"https://nayan.co/tags/optimization/"}],"author":"Diwakar Singh"},{"title":"Deploying a Rails application to a Docker container","slug":"rails-docker","date":"2020-07-25T08:00:00.000Z","updated":"2020-08-26T05:49:07.516Z","comments":true,"path":"/Rails/rails-docker/","link":"","permalink":"https://nayan.co/Rails/rails-docker/","excerpt":"","text":"Docker has gained immense popularity over the past few years as a tool for depploying your applications to production. It allows you to package you application and all of its dependencies in an image, ready to be deployed anywhere. Deploying a Rails app to Docker can be a bit tricky, so here’s a handy step by step guide. Steps Install Docker Create Dockerfile Create docker-compose.yml Build the image Deploy Installing DockerIf you are on Mac or Windows, its best to install Docker Desktop which installs all the necessary tools for you. DockerfileIt all starts with the Dockerfile. This is where you define how your image needs to be built. TLDR; 1234567891011121314151617181920FROM ruby:2.5.3RUN apt-get update -qq &amp;&amp; apt-get install -y nodejs postgresql-clientRUN mkdir /srcWORKDIR /srcENV BUNDLER_VERSION=2.1.4RUN gem install bundler -v 2.1.4COPY Gemfile Gemfile.lock ./RUN bundle config build.nokogiri --use-system-librariesRUN bundle installCOPY . ./#COPY entrypoint.sh /usr/bin/RUN chmod +x entrypoint.shENTRYPOINT [&quot;./entrypoint.sh&quot;]EXPOSE 3000 You start by choosing the base image with your project Ruby version, and install the required libraries on the next line. Then you create the source directory for your code and set it as your working directory. Next, set the bundler version to the one from your Gemfile and install it. Copy the Gemfile and Gemfile.lock, and install all dependencies. Once the dependencies are installed, we copy the entire current directory to the image. Finally, we define an entrypoint for the image. This is a script that will be run when the image is first started. And then we expose the port on which our app will run. docker-compose.ymlWith the Dockerfile, we can build our application image. But it depends on other services as well, such as a database or maybe Redis. This is where Docker Compose comes into the picture. We’ll create another file docker-compose.yml 1234567891011121314151617version: &apos;3&apos;services: db: image: postgres environment: - POSTGRES_PASSWORD=password volumes: - ./tmp/db:/var/lib/postgresql/data web: build: . command: bash -c &quot;rm -f tmp/pids/server.pid &amp;&amp; bundle exec rails s -p 3000 -b &apos;0.0.0.0&apos;&quot; volumes: - .:/src ports: - &quot;3000:3000&quot; depends_on: - db We have two service, one for the database and one for the application. By defining the web service to be dependent on the db service, we tell Compose to start db before the web service. EntrypointFinally, we define the entrypoint file that describes the tasks to be run when starting a new image. entrypoint.sh 1234567891011#!/bin/bashset -e# Remove a potentially pre-existing server.pid for Rails.rm -f /src/tmp/pids/server.pid# Database migrationRUN bundle exec rails db:migrate# Then exec the container&apos;s main process (what&apos;s set as CMD in the Dockerfile).bundle exec rails s -b 0.0.0.0 One thing to note is that you should not have both the CMD and entrypoint in Dockerfile. Took me a few many hours to figure this out. BuildTo build the container, run the following command, 1$ docker-compose build DeployThe app can be run locally by 1docker-compose run web rails new . --force --no-deps --database=postgresql You can upload the image to a registry service such as GCR. First tag the image, then push it to GCR, 12docker tag &lt;image-name&gt;:latest gcr.io/&lt;organization&gt;/&lt;image-name&gt;:latestdocker push gcr.io/&lt;organization&gt;/&lt;image-name&gt;:latest And you are done. You can now deploy this image to GKE or any other Kubernetes engine.","categories":[{"name":"Rails","slug":"Rails","permalink":"https://nayan.co/categories/Rails/"}],"tags":[{"name":"Ruby","slug":"Ruby","permalink":"https://nayan.co/tags/Ruby/"},{"name":"Rails","slug":"Rails","permalink":"https://nayan.co/tags/Rails/"}],"author":"Anuj Middha"},{"title":"How to check whether your product causes potential patent infringement","slug":"potential-patent-infringement","date":"2020-07-24T18:16:09.000Z","updated":"2020-08-26T05:49:07.512Z","comments":true,"path":"/IP-Patents/potential-patent-infringement/","link":"","permalink":"https://nayan.co/IP-Patents/potential-patent-infringement/","excerpt":"","text":"Let’s imagine that as an entrepreneur or a product manager, you are going to launch a product (or service) commercially but you do not know what is patent infringement and how to avoid it (this article explains what are patents and how they work). For anyone facing this situation, it is important to understand the meaning of patent infringement, know if you are infringing on a patent and use necessary remedies to avoid infringement, in early stages of your product launch. You may want to take such preventive measures because if patent infringement is proven after your product hits the market, you may need to pay a share of your revenues (called ‘royalty’) to the patent owner who alleges infringement. This is especially important for start-ups since in many cases, the company’s business model is largely based on these products and if they were to get stuck in patent infringement, it can be damaging for the entire business. It, therefore, makes sense to take such risks into account before the product hits the market. To find whether your product infringes on a patent, let us first understand what is patent infringement. As we now already know, an important purpose of getting a patent is to protect an invention by excluding others from making it, using it, selling it, importing it or offering it for sale without the patent owner’s consent. Any such exploitation of the patented invention without the consent of the patent owner is called patent infringement. As an example, let’s assume that you have a technical solution or a product that you are going to commercialise. In this event, someone else with a patent on that product (or your technical solution) could approach you to solicit royalty (licensing fee) from you, since you are exploiting their patent without their consent. Well, such a situation cannot always be completely eliminated but its probability can be minimized to a great extent if, before launching your product, you can conduct (or get a patent professional to conduct) a thorough patent search (known as a Freedom-to-operate search in IP world). The objective of doing so is to find patents which have claims that directly map to the features of your product. This precautionary effort can make you aware at early stages of your product launch, regarding any patent risks that you may face later on and allows you sufficient time to take necessary precautions. Now we know that FTO search is used to find any third-party patents on which your product may infringe. An important aspect to remember here is that patent infringement occurs when a product feature directly maps to the ‘claims’ of a granted patent. For example, the very popular feature of “slide to unlock” on smartphones directly maps to this patent (refer claim 1 in the linked patent) and thus, this feature is considered infringing on this patent. Now that you know what is patent infringement and how it can affect you in the above situation, it logically follows that you would want to review every patent that is a potential risk and take precautionary measures accordingly. The simplest starting approach you can take as a beginner is to use this article in conjunction with the points covered below to search for patents by using relevant keywords (or patent classes). Patent is a territorial right - Any patent in any jurisdiction is an enforceable right that can be exercised only within the confines of that jurisdiction. For example, a product launched in U.S. (or any other country) cannot be considered infringing on a patent granted in Australia (or any country). This means if a patent owner has a patent granted in Australia, they can only enforce their patent within Australia and not outside. In fact, this is exactly why patent owners file patents in different jurisdictions separately and there is no such thing as global patent. Therefore, while searching you need to only stick to the patents that are granted in jurisdictions of your interest. This saves time and effort during your search. Independent claims must map entirely - An essential criteria of patent infringement is that at least one independent claim (any one) must map to the product features. If any of the dependent claims map to the product features without the independent claims getting mapped, it is not considered patent infringement. Another aspect to consider is that each feature of the independent claim must map to the product, for infringement to occur. If you refer to the ‘slide to unlock’ patent above, you will notice that the independent claim 1 of the patent maps entirely to the actual slide to unlock feature you use on your smartphone to unlock the phone. Moving on, if as a product owner, you can ascertain that any feature of the independent claim does not map, it is not considered patent infringement (but this can be subjective). Granted patent - Since only ‘granted’ patents can be enforced as per patent law in any country, any legal action can be taken by a third-party against you (as an infringing product owner) only if their patent is granted. Therefore, you should pay attention to the legal status of a patent before concluding that your product infringes on a patent. If a patent application is yet to be granted and its claims map to your product, it is likely that the scope of its claims would change during patent prosecution (and diverting from your product scope. However, you can watchlist such patents to track any change in their scope until (and if!) they get granted. This may affect your product strategy. If the patent is abandoned or expired (even after grant), you need not be concerned and can discard that particular patent from your consideration. Exhaustive search- A great aspect of patents is that they are published documents. This means that you will find most of the patents in public domain at one or another search tool. To make the search exhaustive though, you should try multiple search tools including Google Patents, Freepatentsonline, USPTO search tool, Espacenet, Patentscope and so on. If the budget permits, you can even try out a paid search tool as well for better patent coverage where high stakes are involved. However, no search completely eliminates the risk because the search tools have coverage limitations with respect to the jurisdictions and the number of patents covered. Additionally, most patent offices globally, publish patent applications after a few months from filing. Therefore, it is likely that at the time you are searching, a target patent has not been published and thus, could not be found but it does not mean that there is no infringement. However, doing an exhaustive search and repeating it periodically can minimize the risk of infringement to a great extent. All the above points translate to shortlisting at least one granted patent, which has at least one independent claim that completely maps to the product in consideration, in a jurisdiction of interest (where the product is going to get launched). During your review of the patents that show up during search, you can shortlist patents using the above criteria and find the most relevant ones based on their claim features. These are the patents that pose patent infringement risk to your product. Remember that merely adding new features to the product or believing that you already have additional features in the product may not help you escape infringement as long as the infringing features are already present in the product. Once you have done the above exercise, study the patents closely to find out any features in the independent claims that do not map. If that is not possible and you are stuck with a granted (enforceable) patent that does map to your product, you can try to modify your product features such that the product does not infringe on the shortlisted patent(s). Alternatively, you can approach the patent owner to strike a deal to license the patented invention. There are many more remedies available but I would avoid digressing from the scope of this article. Like analysis of all market risks before launching a product, patent infringement risk also should be given high importance and all necessary due diligence should be done to minimise it. If you are new to patents or are trying to learn this field in greater detail, do subscribe to this blog to get notified on similar articles on patent fundamentals. IP focus at Nayan TechnologiesNayan Technologies is a new-age start-up focussed on Artificial Intelligence-based Intelligent Mobility. At Nayan, we strongly believe in creating and expanding our Intellectual Property assets. Nayan Technologies has more than 40 patent applications filed across the world in jurisdictions such as USA, Europe, PCT, Middle-East (GCC), Canada, India, Singapore, Indonesia and Australia with 4 patent grants already received across USA, Australia and India. These patents protect various facets of the Intelligent Mobility framework that Nayan Technology works on. To avoid external patent risks, Nayan applies a two-pronged strategy – first, to create, expand and diversify its own patent portfolio and second, to be aware of third-party patents from which patent risk is anticipated and take appropriate strategies accordingly. While innovation one of the core strengths at Nayan, we also believe in ensuring that Nayan’s innovation does not wilfully infringe on third-party patents, by following the above mentioned practices.","categories":[{"name":"IP-Patents","slug":"IP-Patents","permalink":"https://nayan.co/categories/IP-Patents/"}],"tags":[{"name":"Patents","slug":"Patents","permalink":"https://nayan.co/tags/Patents/"},{"name":"Infringement","slug":"Infringement","permalink":"https://nayan.co/tags/Infringement/"},{"name":"Nayan IP","slug":"Nayan-IP","permalink":"https://nayan.co/tags/Nayan-IP/"},{"name":"Nayan data protection","slug":"Nayan-data-protection","permalink":"https://nayan.co/tags/Nayan-data-protection/"}],"author":"Abhinav Arora"},{"title":"How to implement Android RecyclerView Drag and Drop feature seamlessly","slug":"android-recycler-view-drag-and-drop","date":"2020-07-07T12:41:05.000Z","updated":"2020-08-26T05:49:07.472Z","comments":true,"path":"/Android/android-recycler-view-drag-and-drop/","link":"","permalink":"https://nayan.co/Android/android-recycler-view-drag-and-drop/","excerpt":"","text":"We some times want to implement Drag and Drop feature OR Swip to dismiss feature on our recycler view. For implementing that we usually go for a library that already have this implemented, and at this point of time most of those libraries are using old APIs and complex logic to handle the things. But now we have simple and better ItemTouchHelper in the Android Support Library itself, so now we don’t need those good old libraries. Lets start implementing. 1. Lets create an ItemTouchHelper.CallbackWe’ll create an ItemTouchHelper.Callback to handle the events. 123456789101112131415161718192021222324252627282930313233343536373839private val itemTouchHelperCallback = object: ItemTouchHelper.Callback() &#123; override fun getMovementFlags( recyclerView: RecyclerView, viewHolder: RecyclerView.ViewHolder ): Int &#123; // Specify the directions of movement return makeMovementFlags(null, 0) &#125; override fun onMove( recyclerView: RecyclerView, viewHolder: RecyclerView.ViewHolder, target: RecyclerView.ViewHolder ): Boolean &#123; // Notify your adapter that an item is moved from x position to y position return true &#125; override fun isLongPressDragEnabled(): Boolean &#123; // true: if you want to start dragging on long press // false: if you want to handle it yourself return true &#125; override fun onSwiped(viewHolder: RecyclerView.ViewHolder, direction: Int) &#123; &#125; override fun onSelectedChanged(viewHolder: RecyclerView.ViewHolder?, actionState: Int) &#123; super.onSelectedChanged(viewHolder, actionState) // Hanlde action state changes &#125; override fun clearView(recyclerView: RecyclerView, viewHolder: RecyclerView.ViewHolder) &#123; super.clearView(recyclerView, viewHolder) // Called by the ItemTouchHelper when the user interaction with an element is over and it also completed its animation // This is a good place to send update to your backend about changes &#125; &#125; 2. Set directionsAdd your movement flags for the directions you want to handle. 123456789101112private val itemTouchHelperCallback = object: ItemTouchHelper.Callback() &#123; override fun getMovementFlags( recyclerView: RecyclerView, viewHolder: RecyclerView.ViewHolder ): Int &#123; // Specify the directions of movement val dragFlags = ItemTouchHelper.UP or ItemTouchHelper.DOWN return makeMovementFlags(dragFlags, 0) &#125; ... &#125; 3. Update AdapterTell your adapter about the positions updates of the items. 123456789101112131415private val itemTouchHelperCallback = object: ItemTouchHelper.Callback() &#123; ... override fun onMove( recyclerView: RecyclerView, viewHolder: RecyclerView.ViewHolder, target: RecyclerView.ViewHolder ): Boolean &#123; // Notify your adapter that an item is moved from x position to y position yourAdapter.notifyItemMoved(viewHolder.adapterPosition, target.adapterPosition) return true &#125; ... &#125; 4. Create ItemTouchHelperFrom itemTouchHelperCallback we’ll create an ItemTouchHelper object to attach RecyclerView to it. 1val itemTouchHelper = ItemTouchHelper(itemTouchHelperCallback) 5. Attach to RecyclerView1itemTouchHelper.attachToRecyclerView(yourRecyclerView) By this time now you have Drag and Drop feature enabled in your RecyclerView you can just build the project and run the app. 6. Handle final stateNow when you want to send and update to you backend about the new order of the items, then just override the clearView method of itemTouchHelperCallback and you are good to go. 12345678910private val itemTouchHelperCallback = object: ItemTouchHelper.Callback() &#123; ... override fun clearView(recyclerView: RecyclerView, viewHolder: RecyclerView.ViewHolder) &#123; super.clearView(recyclerView, viewHolder) // Called by the ItemTouchHelper when the user interaction with an element is over and it also completed its animation // This is a good place to send update to your backend about changes // Your API call &#125; &#125; Make it working with SwipeRefreshLayoutNow comes a strange problem when you try to implement this feature in swipe refresh layout. The problem is, when you try to drag the item from bottom to top direction, then it will work, but when you try to drag from top to bottom, then it fails, as SwipeRefreshLayout intercepts the callback of dragging from top to bottom. We just have to disable the SwipeRefreshLayout‘s swipe to refresh feature for the drag time and enable it back when the user has dropped the item to its final position. So, lets override onSelectedChanged to handle this. 12345678910111213private val itemTouchHelperCallback = object: ItemTouchHelper.Callback() &#123; ... override fun onSelectedChanged(viewHolder: RecyclerView.ViewHolder?, actionState: Int) &#123; super.onSelectedChanged(viewHolder, actionState) // Hanlde action state changes val swiping = actionState == ItemTouchHelper.ACTION_STATE_DRAG pullToRefresh.isEnabled = !swiping &#125; ... &#125; That’s all you have to do for implementing Drag and Drop feature for your RecyclerView References:- Android RecyclerView drag and drop Drag and Swipe with RecyclerView ItemTouchHelper and SwipeRefreshLayout (RecyclerView) Some good reads you may like:- Paytm Gateway Integration Android Testing Strategy How to draw custom paths/lines in Android usign PathEffect","categories":[{"name":"Android","slug":"Android","permalink":"https://nayan.co/categories/Android/"}],"tags":[{"name":"android","slug":"android","permalink":"https://nayan.co/tags/android/"},{"name":"kotlin","slug":"kotlin","permalink":"https://nayan.co/tags/kotlin/"},{"name":"ItemTouchHelper","slug":"ItemTouchHelper","permalink":"https://nayan.co/tags/ItemTouchHelper/"},{"name":"Callback","slug":"Callback","permalink":"https://nayan.co/tags/Callback/"},{"name":"RecyclerView","slug":"RecyclerView","permalink":"https://nayan.co/tags/RecyclerView/"},{"name":"Drag and Drop","slug":"Drag-and-Drop","permalink":"https://nayan.co/tags/Drag-and-Drop/"},{"name":"Ashish Jajoria","slug":"Ashish-Jajoria","permalink":"https://nayan.co/tags/Ashish-Jajoria/"}],"author":"Ashish Jajoria"},{"title":"Youtube Data API in Angular app for realtime chanels video data","slug":"angular-youtube","date":"2020-06-21T16:36:18.000Z","updated":"2020-08-26T05:49:07.480Z","comments":true,"path":"/Web/angular-youtube/","link":"","permalink":"https://nayan.co/Web/angular-youtube/","excerpt":"","text":"Facing issues getting data of your Youtube channel on your website? This blog is a guide demonstrating on how to integrate Youtube data API in your angular web application with few easy steps. YouTube Data API v3The first step is to get the api key. By visiting https://developers.google.com/youtube/v3/getting-started you’ll find the procedures you need to get your authorization credentials.In a nutshell, you need: Go to the Google Developers Console. Select a project. In the left sidebar, select APIs and authorization. In the list of APIs, make sure the status is ON for the YouTube Data API v3. After the project is created the next step is to register the HttpClientModule module in the main module (app.module.ts). 1import &#123; HttpClientModule &#125; from &apos;@angular/common/http&apos;; And declare in imports: 12345imports: [BrowserModule,AppRoutingModule,HttpClientModule,NgxSpinnerModule I’ll use the NGX-Spinner library to display a spinner while loading the videos. Install the library with: 1$ npm install ngx-spinner --save And declare the module in the imports, as per the code above.We can now create a service to make calls to the Youtube API. In the terminal, write: 1$ ng g service youtube 123456789101112131415161718192021import &#123; Injectable &#125; from &apos;@angular/core&apos;;import &#123; HttpClient &#125; from &apos;@angular/common/http&apos;;import &#123; map &#125; from &apos;rxjs/operators&apos;;@Injectable(&#123; providedIn: &apos;root&apos;&#125;)export class YoutubeService &#123; apiKey : string = &apos;YOUR-APIKEY-YOUTUBE&apos;; constructor(public http: HttpClient) &#123; &#125; getVideosForChanel(channel, maxResults): Observable&lt;Object&gt; &#123; let url = &apos;https://www.googleapis.com/youtube/v3/search?key=&apos; + this.apiKey + &apos;&amp;channelId=&apos; + channel + &apos;&amp;order=date&amp;part=snippet &amp;type=video,id&amp;maxResults=&apos; + maxResults return this.http.get(url) .pipe(map((res) =&gt; &#123; return res; &#125;)) &#125;&#125; We create an apiKey variable that stores the value of the API obtained in the first step. Then we inject the HttpClient class into the constructor. It provides methods for performing HTTP requests. Let’s implement a method that returns a list of videos. We name it getVideosForChanel(). We pass two arguments, the first is the channel ID. The second limit the number of videos. We concatenate this information in the API URL, passing other parameters as the order (‘&amp; order = date), part = snippet that contains other properties that identify the title, the description, among others, and the type of resource (type = video). 1let url = ‘https://www.googleapis.com/youtube/v3/search?key=&apos; + this.apiKey + ‘&amp;channelId=’ + channel + ‘&amp;order=date&amp;part=snippet &amp;type=video,id&amp;maxResults=’ + maxResults In the component class (app.component.ts), we declare an array for the result of the videos: 12export class AppComponent &#123;videos: any[]; In the constructor method, we inject the service created for requesting videos (YoutubeService) and a class to display a spinner (NgxSpinnerService). 1constructor(private spinner: NgxSpinnerService, private youTubeService: YoutubeService) &#123; &#125; Then, in the ngOnInit( ) method, we invoke the method by passing the Channel ID, in this example the channel is my child’s :), and a maximum number of .getVideosForChanel results. 12345678910111213141516ngOnInit() &#123;this.spinner.show()setTimeout(()=&gt;&#123;this.spinner.hide()&#125;,3000)this.videos = [];this.youTubeService.getVideosForChanel(&apos;UC_LtA_EtCr7Jp5ofOsYt18g&apos;, 15).pipe(takeUntil(this.unsubscribe$)).subscribe(lista =&gt; &#123;for (let element of lista[&quot;items&quot;]) &#123;this.videos.push(element)&#125;&#125;);&#125; In the result, .subscribe (list =&gt; {, retrieve the items property and add each object in the created array. At the beginning of the function, we included a timeout of 3 seconds to close the spinner in. 1234setTimeout(()=&gt;&#123;this.spinner.hide()&#125;,3000) Let’s finalize, coding the component template (app.component.html: 12345678910111213&lt;div *ngFor=&quot;let video of videos&quot; class=&quot;col-xl-3 col-md-6 mb-4&quot;&gt;&lt;div class=&quot;card border-0 shadow vh-50&quot;&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=&#123;&#123;video.id.videoId&#125;&#125;&quot; target=&quot;_blank&quot;&gt;&lt;img [src]=&quot;video.snippet.thumbnails.medium.url&quot; class=&quot;card-img-top&quot; alt=&quot;...&quot;&gt;&lt;/a&gt;&lt;div class=&quot;card-body text-center&quot;&gt;&lt;h5 class=&quot;card-title mb-0&quot;&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=&#123;&#123;video.id.videoId&#125;&#125;&quot;&gt;&#123;&#123;video.snippet.title&#125;&#125;&lt;/a&gt;&lt;/h5&gt;&lt;div class=&quot;card-text text-black-50&quot;&gt;&#123;&#123;video.snippet.description.slice(0, 100)&#125;&#125;&lt;/div&gt;&lt;p class=&quot;card-text&quot;&gt;&#123;&#123;video.snippet.description&#125;&#125;&lt;/p&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt; We loop the array using the * ngFor directive. We have defined a link to view the video through videoID at href = “https://www.youtube.com/watch?v=“. Resources: https://developers.google.com/youtube/v3 https://developers.google.com/youtube/v3/docs/videos/list Previous blog: https://nayan.co/blog/Web/angular-maps/","categories":[{"name":"Web","slug":"Web","permalink":"https://nayan.co/categories/Web/"}],"tags":[{"name":"angular","slug":"angular","permalink":"https://nayan.co/tags/angular/"},{"name":"youtube","slug":"youtube","permalink":"https://nayan.co/tags/youtube/"},{"name":"Youtube data API","slug":"Youtube-data-API","permalink":"https://nayan.co/tags/Youtube-data-API/"},{"name":"Nayan","slug":"Nayan","permalink":"https://nayan.co/tags/Nayan/"},{"name":"Cloud Data","slug":"Cloud-Data","permalink":"https://nayan.co/tags/Cloud-Data/"}],"author":"Abhishek Rana"},{"title":"How to draw custom paths/lines in Android usign PathEffect","slug":"drawing-custom-paths-in-android","date":"2020-06-08T12:41:05.000Z","updated":"2020-08-26T05:49:07.480Z","comments":true,"path":"/Android/drawing-custom-paths-in-android/","link":"","permalink":"https://nayan.co/Android/drawing-custom-paths-in-android/","excerpt":"","text":"We can draw simple lines and shapes by using path.lineTo, path.moveTo etc. But sometimes we have requirements to draw a line in pattern, for example: simple dashed, two lines where 1 is continuous and other one is dashed etc. Getting startedFirst we’ll have to create a small path that we want to repeat in the final path 12345678910111213141516171819202122232425262728293031323334353637383940414243444546// These are some sample methods to generate a single block of path// which will be repeated in the final path that we'll draw on canvas// This will draw a small line with width 10px and length 30pxprivate fun makeDefaultLinePath(): Path &#123; val p = Path() p.moveTo(-15f, 5f) p.lineTo(15f, 5f) p.lineTo(15f, -5f) p.lineTo(-15f, -5f) return p &#125;// This will draw two small lines with width 4px and length 30px// They'll have 4px space between themprivate fun makeDoubleLinePath(): Path &#123; val p = Path() p.moveTo(-15f, 6f) p.lineTo(15f, 6f) p.lineTo(15f, 2f) p.lineTo(-15f, 2f) p.close() p.moveTo(-15f, -6f) p.lineTo(15f, -6f) p.lineTo(15f, -2f) p.lineTo(-15f, -2f) return p&#125;// This will draw two small lines// One with width 4px and length 15px// Other with width 4px and length 30px// They'll have 4px space between themprivate fun makeBrokenSolidLinePath(): Path &#123; val p = Path() p.moveTo(-15f, 6f) p.lineTo(0f, 6f) p.lineTo(0f, 2f) p.lineTo(-15f, 2f) p.close() p.moveTo(-15f, -6f) p.lineTo(15f, -6f) p.lineTo(15f, -2f) p.lineTo(-15f, -2f) return p&#125; We’ve create our building blocks of the final path. Now we’ll be setting PathEffect to the paint that will draw these blocks. 12345678910111213141516171819// First setup your paint objectval paint = Paint()paint.style = Paint.Style.STROKEpaint.strokeWidth = 10fpaint.color = Color.YELLOW// Declare your pathDashPathEffectvar pathDashPathEffect: PathDashPathEffect? = null// Define your pathDashPathEffectpathDashPathEffect = PathDashPathEffect(makeDoubleLanePath(), //Your building block 45f, //At how much distance the next block should be drawn from the current block's starting point 0f, //Phase value PathDashPathEffect.Style.MORPH) //EffectStyle// Set your defined pathDashPathEffect to your paint objectpathDashPathEffect?.let &#123; effect -&gt; paint.pathEffect = effect&#125; We have our paint object with pathEffect with us. Now we’ll be drawing a path that we actaully want to draw using this paint object. 123456789101112131415var lineX1 = 0fvar lineX2 = 0fvar lineY1 = 0fvar lineY2 = 0fval path = Path()override fun onDraw(canvas: Canvas?) &#123; super.onDraw(canvas) path.reset() path.moveTo(lineX1, lineY1) path.lineTo(lineX2, lineY2) canvas?.drawPath(path, //This final path that we are drawing now paint) //The one that we created earlier&#125; References:- Custom path line style when drawing on canvas what does path mean in PathDashPathEffect constructor PathDashPathEffect example Some good reads you may like:- Paytm Gateway Integration Android Testing Strategy","categories":[{"name":"Android","slug":"Android","permalink":"https://nayan.co/categories/Android/"}],"tags":[{"name":"android","slug":"android","permalink":"https://nayan.co/tags/android/"},{"name":"kotlin","slug":"kotlin","permalink":"https://nayan.co/tags/kotlin/"},{"name":"Ashish Jajoria","slug":"Ashish-Jajoria","permalink":"https://nayan.co/tags/Ashish-Jajoria/"},{"name":"canvas","slug":"canvas","permalink":"https://nayan.co/tags/canvas/"},{"name":"paint","slug":"paint","permalink":"https://nayan.co/tags/paint/"},{"name":"path","slug":"path","permalink":"https://nayan.co/tags/path/"},{"name":"PathDashPathEffect","slug":"PathDashPathEffect","permalink":"https://nayan.co/tags/PathDashPathEffect/"},{"name":"DashPathEffect","slug":"DashPathEffect","permalink":"https://nayan.co/tags/DashPathEffect/"},{"name":"CornerPathEffect","slug":"CornerPathEffect","permalink":"https://nayan.co/tags/CornerPathEffect/"},{"name":"ComposePathEffect","slug":"ComposePathEffect","permalink":"https://nayan.co/tags/ComposePathEffect/"},{"name":"draw custom path","slug":"draw-custom-path","permalink":"https://nayan.co/tags/draw-custom-path/"}],"author":"Ashish Jajoria"},{"title":"Detecting Whether a Parking Zone Is Vacant Or Occupied","slug":"Vehicle-Parking-Occupancy-Detection","date":"2020-06-01T17:22:29.000Z","updated":"2020-08-26T05:49:07.444Z","comments":true,"path":"/AI/Vehicle-Parking-Occupancy-Detection/","link":"","permalink":"https://nayan.co/AI/Vehicle-Parking-Occupancy-Detection/","excerpt":"","text":"Circling around the parking is one of the most annoying experiences mainly because it is time consuming and quite irritating too.However , there has been a major boost in the field of Deep Learning and Computer Vision in the past few years which enables us to create a robust and a real-time solution. Plan Of Attack A pipeline for the same has been created which consists of the following 5 steps : Finding suitable Images Parking zones detection Vehicle detection Plotting the vehicles as point objects on 2d-map Detecting whether a parking lot is vacant or occupied Each of the 5 steps have been described in detail in the following sections : 1) Finding Suitable ImagesThe first step to solve any problem in Deep Learning and Computer Vision is to collect data.In our approach , we just required 2 images throughout the entire process. The 2 images required are: * Image of actual camera view of the parking zones * Image of the top-view / 2d-map of the same areaWe obtained these 2 images in the following manner : * There are a lot of open CCTV IP cameras available freely on the internet. By using one of the websites , we were able to get our first image which is a camera view of parking zones. * The same website also provided us the longitude and latitude of the area which was then entered in Google Maps in order to get it’s top-view. We now have both the images and we are ready to proceed to the next step. 2) Parking Zones DetectionThis is one of the most important tasks in the entire pipeline as almost all the remaining tasks are dependent on this. Getting these inaccurate could impact our entire pipeline leading us to an incorrect outcome. Before jumping onto this , I had done some research and found that the following 3 methods are most commonly used to tackle this kind of problem : * Canny edge detection and hough line transform combined with image processing could detect the lines in the parking zones. * Manually drawing out the contours on the parking zones. * Directly perform vehicle detection and locate the stationary cars assuming that they are on the parking zones.Third method is based on assumptions and also not suitable for many scenarios , due to which we discarded it as an option. We used the first method wherein we detected the lines using canny edge detection and hough line transform combined with image processing. 3) Vehicle DetectionIn order to achieve this , we had used the state-of-the-art YOLOv3 Deep Learning object detection model. This is a crucial step as once we get the bounding boxes accurately, only then would we be able to detect whether a parking zone is vacant or occupied. 4) Representing the vehicles as point objects on 2d-mapFor this step , we would be using a very classical Computer Vision concept.This step can be broken to the following sub-steps : *Using Google API , we came to know the dimensions of the parking areas in both the images. *This helped us in restricting the images to particular areas which further aided us in automatically detecting key points in both images. *Distortion also plays a significant role in getting accurate key-points. So had there been some information about the planted camera , we could have undistorted the images and got more accurate key-points Using the key-points in the above 2 images , we calculate their homography. We can represent vehicles as point objects by the following steps : First step is to determine the 2d-point matrix by multiplying homography and the center coordinates of the bounding boxes The final step in finding the 2d coordinates is to divide the 2d-point matrix obtained by third element. Therefore the 2d coordinates for all the vehicles are derived in similar fashion and are plotted on the 2d-map image. Getting these points plotted at accurate locations implies that results and calculations we got from both step 3 and step 4 are perfect. 5) Detecting whether a parking lot is vacant or occupiedPoint Polygon test is used here to find whether the point is inside or outside the parking zone. Point polygon test checks whether a point is inside the polygon or not. It returns a negative value if the point is outside the polygon , 0 if the point is on the polygon and a positive value if the point is inside the polygon. Hence as it can be seen from the image above, with the help of Deep Learning and Computer Vision , we have successfully solved this problem by following a very simple and systematic approach. For more such exciting Deep Learning blog posts,click here. Also, click here to know how we are leveraging AI for traffic monitoring and road safety.","categories":[{"name":"AI","slug":"AI","permalink":"https://nayan.co/categories/AI/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"https://nayan.co/tags/Deep-Learning/"},{"name":"Computer Vision","slug":"Computer-Vision","permalink":"https://nayan.co/tags/Computer-Vision/"},{"name":"Parking Detection","slug":"Parking-Detection","permalink":"https://nayan.co/tags/Parking-Detection/"},{"name":"Vehicle Occupancy Detection","slug":"Vehicle-Occupancy-Detection","permalink":"https://nayan.co/tags/Vehicle-Occupancy-Detection/"}],"author":"Aman Goyal , Anubhav Rohatgi"},{"title":"Creating a very basic deep-learning model in Docker","slug":"Creating-deep-learning-models-in-Docker","date":"2020-05-16T11:09:31.000Z","updated":"2020-08-26T05:49:07.412Z","comments":true,"path":"/AI/Creating-deep-learning-models-in-Docker/","link":"","permalink":"https://nayan.co/AI/Creating-deep-learning-models-in-Docker/","excerpt":"","text":"Recently me and my team shifted our approach to build our models in the docker containers rather than creating and running a python file on the system. For me, it was a completely new experience in learning docker. If you are also quite new to docker then with this post you will be able to create your own basic deep-learning model in docker. What is Docker? Docker provides an image-based deployment model. This makes it easy to share an application, with all of their dependencies across multiple environments. Docker also automates deploying the application inside this container environment. Docker Images and Containers Docker Container is a standard unit which can be created on the fly to deploy a particular application or environment. A Docker image is a file, comprised of multiple layers, that is used to execute code in a Docker container. An image is essentially built from the instructions for a complete and executable version of an application, which relies on the host OS kernel. Multiple instances of a Docker container can be run on a single docker image. Why Docker? Since we get an idea about docker, lets discuss some of its important use-cases: Modularity : This means that with the use of docker if any of the part of the application needs to get update or repair, we can do that without shutting the whole application down. Rollback : Every image has layers. Don’t like the current iteration of an image? Roll it back to the previous version Installing Docker Before getting started, we firstly need to install docker on our machine. Firstly update the packages and installing the required packages $ sudo apt update $ sudo apt install apt-transport-https ca-certificates curl software-properties-commonThen add the GPG key for the official Docker repository to your system: $ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -Add the Docker repository to APT sources: $ sudo add-apt-repository &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu bionic stable&quot;Make sure you are about to install from the Docker repo instead of the default Ubuntu repo: $ apt-cache policy docker-ceNow, finally install docker $ sudo apt install docker-ceCheck, the docker is running by $ sudo systemctl status docker Creating a project with Docker Now, we are ready to create our first docker project. I had choose a simple CIFAR-10 dataset for the post. You can clone my git repository. Go the CIFARDocker folder. The folder structure is as below:- We here need to know mostly about two files Dockerfile and run_cifar.sh. Firstly going with Dockerfile. Firstly, we pull our base image from the public repositories. A Dockerfile must start with a FROM instruction. It initializes a new build stage and sets a base image for other instructions. The ENV instruction sets the environment variable. So, here we sets some enivronment for python. Then, we use RUN *instruction. This instruction is used to execute any commands just like we do on our machine. Here, I install some packages like ffmpeg, wget etc. The *ADD instruction is used to copy new files or directories local machine and adds them to the filesystem of the image at the path. Format for ADD instruction is as below: ADD &lt;src&gt; &lt;dst&gt;Here, we provide the file path as per our local machine as and our file path in our image in . Here, in our dockerfile, I copied a requirements.txt file and adds to the base path of the image. Then I install all the packages inside the requirements.txt file using the RUN instruction. Now, finally we define our WORKDIR instruction. This is the path to our working directory. So, specify the WORKDIR path as per your image path.Now, we will build our docker image. To build the image, type the below command $ docker build -t &apos;docker_containerized_cifar&apos; .Here, the docker_containerized_cifar is the name of our docker image. You can specify your custom image name for your dockerfile. Now we will make a container for this docker image. Open run_cifar.sh file. Its basic format is as like:- docker run -it -d --name=docker_container_name -v &lt;src&gt;:&lt;dst&gt; --network=docker_image_nameHere, -it: Instructs Docker to create an interactive bash shell in the container. –name: Gives the name of your container –v: Specify, the local machine folder to be mounted on the path (image path) –network: Docker image name associated with the container. In our case, the name will be docker_containerized_cifar. Now, starting the container using the below command. $ bash run_cifar.shNow, run our code in the docker container itself. For, this type the below command. $ docker exec -it docker_container_name bashHere, docker_container_name is the name of my docker container. When you hit the above command, a shell gets open and BOOM you are in your docker container. Now, run the code for cifar using the command $ python cifar.pyThat’s simple, now, your code is running on the docker container. If you want to stop/start the container again. Then type the below command. $ docker start container_name ##for starting a container $ docker stop container_name ##for stopping a containerConclusion Well, that’s enough in this post. Hope you liked it. I hope you get an idea about docker and also about how can we these deep-learning models with the docker. Some good reads you may like :) Flask + Mongo Integration","categories":[{"name":"AI","slug":"AI","permalink":"https://nayan.co/categories/AI/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://nayan.co/tags/docker/"},{"name":"deep-learning","slug":"deep-learning","permalink":"https://nayan.co/tags/deep-learning/"},{"name":"AI","slug":"AI","permalink":"https://nayan.co/tags/AI/"}],"author":"Himanshu Garg"},{"title":"Meta-Learning:Learning-to-learn-fast and its approaches","slug":"Meta-Learning-Learning-to-learn-fast","date":"2020-05-06T10:36:34.000Z","updated":"2020-08-26T05:49:07.420Z","comments":true,"path":"/AI/Meta-Learning-Learning-to-learn-fast/","link":"","permalink":"https://nayan.co/AI/Meta-Learning-Learning-to-learn-fast/","excerpt":"","text":"In this blog, I will explain meta-learning what is also known as learning to learn in machine learning. What is Meta-Learning?Current AI agents excel at mastering a particular skill like object classification, predicting a disaster, image segmentation etc. However, now AI is extensively used in day-to-day life. Almost every organization now uses AI in their products. So there is a need for a single AI agent that can solve a variety of problems. A good machine learning model often requires training with a large number of samples. Humans, in contrast, learn new concepts and skills much faster and more efficiently. Kids who have seen cats and birds only a few times can quickly tell them apart. Similar to humans, machine learning models need to be versatile and adapt to a new problem with the least number of samples. This essentially meta-learning aims to solve. Versatility is achieved by an intelligent amalgamation of meta-learning along with related techniques such as reinforcement learning (finding suitable actions to maximize a reward), transfer learning (re-purposing a trained model for a specific task on a second related task), and active learning (learning algorithm chooses the data it wants to learn from). Such different learning techniques provide an AI agent with the brains to do multiple tasks without the need to learn every new task from scratch. Thereby making it capable of adapting intelligently to a wide variety of new, unseen situations. Using Meta-Learning, AI agents learn how to learn new tasks by reusing prior experience, rather than examining each new task in isolation. Approaches for Meta-LearningThere are two popular approaches to meta-learning. The Simple ViewA good meta-learning model should be trained over a variety of learning tasks and optimized for the best performance on the distribution of tasks, including potentially unseen tasks. Each task is associated with a dataset D, containing both feature vectors and true labels. The optimal model parameters are: here one dataset is considered as one sample data point. Few-shot classificationFew-shot classification is an instantiation of meta-learning in the field of supervised learning. The dataset D is often split into two parts, support set S for learning and a prediction set B for training or testing, D=(S, B). Often we consider a K-shot N-class classification task: the support set contains K labeled examples for each of N classes. A dataset D contains pairs of feature vectors and labels, and each label belongs to a known label set L. Let’s say, our classifier f outputs a probability of a data point belonging to the class y given the feature vector x, Pθ(y|x). The optimal parameters should maximize the probability of true labels across multiple training batches B⊂D: In a few-shot classification, the goal is to reduce the prediction error on data samples with unknown labels given small support set for “fast learning” (think of how “fine-tuning” works). To make the training process mimics what happens during inference, we would like to “fake” datasets with a subset of labels to avoid exposing all the labels to the model and modify the optimization procedure accordingly to encourage fast learning: Sample a subset of labels. Sample a support set and a training batch. Both of them only contain data points with labels belonging to the sampled label set. The support set is part of the model input. The final optimization uses the mini-batch to compute the loss and update the model parameters through backpropagation, in the same way as how we use it in the supervised learning We may consider each pair of sampled datasets as one data point. The model is trained such that it can generalize to other datasets. Symbols in red are added for meta-learning in addition to the supervised learning objective. The Learner and Meta LearnerAnother popular view of meta-learning decomposes the model update into two stages: A classifier is the learner model, trained for operating a given task; In the meantime, an optimizer learns how to update the learner model’s parameters via the support set. Let’s consider what happens in normal supervised training. In the figure below, M is the neural network with initial weights (blue ■) and L calculates the loss function, and O is the optimizer with parameters (pink ★). For each data sample, the model predicts, finds loss, and optimizes the model weights. This happens in the cycle. In meta-learning, the model M is called the learner and the optimizer O is called the meta-learner. The meta-learner’s parameters are learned by back-propagating a meta-loss gradient along the training process itself, back to the initial weights of the model and/or to the parameters of the optimizer We now have two, nested, training processes: the meta-training process of the optimizer/meta-learner in which the (meta-)forward pass includes several training steps of the model (with forward, backward, and optimization steps). A single step of the meta-training process includes two steps of the training process of the model (vertically in the meta-forward and meta-backward boxes). The training process of the model is exactly the same training process as in supervised learning. The input of the meta-forward pass is a list of examples/labels (or a list of batches) that are used successively during the model training pass. Meta-loss is calculated to reduce the training error. It means, the lower the loss, the better the training was. A meta-optimizer is used to update the weights of the optimizer. Thanks for reading it… References https://lilianweng.github.io/lil-log/2018/11/30/meta-learning.html#a-simple-view from-zero-to-research-an-introduction-to-meta-learning http://localhost:4000/blog/2020/05/06/Meta-Learning-Learning-to-learn-fast/meta-learning.png","categories":[{"name":"AI","slug":"AI","permalink":"https://nayan.co/categories/AI/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://nayan.co/tags/Machine-Learning/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"https://nayan.co/tags/Deep-Learning/"},{"name":"Meta Learning","slug":"Meta-Learning","permalink":"https://nayan.co/tags/Meta-Learning/"},{"name":"Supervised Learning","slug":"Supervised-Learning","permalink":"https://nayan.co/tags/Supervised-Learning/"},{"name":"nayan","slug":"nayan","permalink":"https://nayan.co/tags/nayan/"},{"name":"nayanindia","slug":"nayanindia","permalink":"https://nayan.co/tags/nayanindia/"}],"author":"Anand Kummari"},{"title":"Python and Kafka: message passing and more","slug":"Pyton-and-Kafka-Message-Passing-and-More","date":"2020-05-03T14:50:04.000Z","updated":"2020-08-26T05:49:07.424Z","comments":true,"path":"/AI/Pyton-and-Kafka-Message-Passing-and-More/","link":"","permalink":"https://nayan.co/AI/Pyton-and-Kafka-Message-Passing-and-More/","excerpt":"","text":"Python and Kafka: message passing and moreAt many stages during a developer’s career, he/she has to build systems capable of communicating with another. For example, there may be two python programs and the second program’s operations might depend on the output of the first program. There can be multiple programs in that linear sequence, with every consecutive program depending on the output of the previous. The linear sequence may even branch out to two child programs depending on the output of the previous. An example dependency graph is shown below. As shown in Fig 1, program 2 depends on the output of program 1. Similarly, program 3 depends on the output of program 2. Program 3, in turn, branched out into two children programs accepting output from 3. If the objective of the developer is to make program 4 and program 5 run in parallel then he/she will have to make asynchronous calls to these programs in a multi-threaded fashion. Since python is not inherently thread safe, making scalable multi-threaded systems becomes a bit difficult. This is where Apache Kafka can be used. Let me give you brief introduction of Apache Kafka before moving into it’s implementation and integration with Python. A software platform developed by LinkedIn, Kafka *is an open-source stream processing pipeline which was later donated to the *Apache Software foundation. As mentioned on the official Apache site, Kafka can be used for creating data pipelines that are horizontally scalable, extremely fast and fault-tolerant. The use-case involving python programs given above is not an exact streaming pipeline but Kafka can be used to solve the problem of sending messages to programs for execution and can be also be used to horizontally scale the information between programs. Let’s move forward to see how this can be done. 1. Installing Kafka on Ubuntu and Confluent-Kafka for python:In order to install Kafka, just follow **this** installation tutorial for Ubuntu 18 given on **DigitalOcean**. ***Confluent-Kafka*** is a fully managed enterprise platform for Kafka services. Confluent also ships a Python Client for Kafka, which can be used to integrate Kafka directly with python. This client can be used to create topics, delete them and also send and receive messages. 2. Using Kafka with python:Let’s begin my making a project folder. We will be carrying out the experiments using Docker since it’s a wonderful piece of technology and makes a developer’s life extremely easy. A good practice would be to have two project folders, but for this experiment would be using just one. To execute the python programs we will be going inside the containers and running and them manually. If you want a pretty tutorial on using Docker with python checkout these blogs -&gt; **Docker and Tensorflow and also Docker and YOLO** Let’s go ahead and check each of these files now. Config.yml : *Contains the topic list and from which the two programs are going to read. Also contains the broker information. A kafka topic retains a certain message for a certain amount of topic. Kafka topics can be also be broken down into partitions. Users can then send specific messages in these partitions. To understand more about Kafka topics and partitions check this link out -&gt; [Kafka topics and partitions](http://cloudurable.com/blog/kafka-architecture-topics/index.html) . ***A Kafka consumer reads messages from a particular topic and the Kafka produces/sends messages to topics. KafaConsumer.py and KafkaProducer.py : **These two files contains the classes for the *Kafka Consumer* and *Kafka Producer.* For complete information please refer to the [*github repo](https://github.com/AbhishekBose/kafka_python). Link given below. These two files contains classes which have been created using the *Consumer and Producer classes from the confluent_kafka library.main(operation,x,y) Dockerfile: *Contains the configuration for the docker container which we will be spawning. This will take a *python:3.8 image and install all required libraries using the requirements.txt file. The project folder is added to container using the *ADD *command as shown in Fig 4 below. *Requirements.txt *: Contains a list of all the python libraries for this project python_1.py : *This file does the task of sending a message to a topic which will be read by the second python code. The *config file is read using the *PyYaml library. *The function , defined as *main(operation,x,y) *initializes the producer object named prod and sends a message to it. The message is in a json format which can be read the second program. In this example the second function would be performing two operations namely addition and subtraction. The operations and the operators are taken as command line arguments. python_2.py: *This code essentially will contain the consumer. The consumer will read from the topic in which *python_1.py **will be producing the message. The two functions are defined as *add(x,y) *and *subtract(x,y). These functions will be receiving the two operators to perform the operation on, shown in Fig 6.1. The main function defined in Fig 6.2 is while loop which continuously reads from the consumer topic. This function checks the *operation which was sent by the previous code by reading the message in a json format and calls the respective function. If the operator in the message is sum then the add function is called otherwise the subtract function is called. 3. Building the image and running the container:In order to build the image type the command: docker build -t &apos;image_name&apos; .Once the image is built, spawn a container by typing the following command: docker run -it -d --name=container_name --network=host image_name4. Executing the programs:Go inside the container by typing the following command: docker exec -it container_name bashThis will get you inside. The above container opens up the bash shell inside the container. Run the following commands in two separate shells to witness the output Run the consumer first cd src python python2.py 0The 0 here is the client id which gives a name to your consumer. You can have multiple consumers reading from the consumer topic. This ensures parallel processing as shown in Fig 7.1. Just assign a different client id to each consumer. Now execute the producer by : python python1.py 5 2 sumThe operation here is sum and the operators are 5 and 2 (Fig 7.2) The output can be immediately seen with the result of the addition operation on the previously shell as shown in Fig 7.3 5. Conclusion:This project was a basic example how Kafka can be extensively used for creating massive, parallel data pipelines. At **NAYAN Technologies** we use Kafka extensively for our deep learning inference and training data pipelines. Github repo: ***https://github.com/AbhishekBose/kafka_python***","categories":[{"name":"AI","slug":"AI","permalink":"https://nayan.co/categories/AI/"}],"tags":[{"name":"Python","slug":"Python","permalink":"https://nayan.co/tags/Python/"},{"name":"Queue","slug":"Queue","permalink":"https://nayan.co/tags/Queue/"},{"name":"Kafka","slug":"Kafka","permalink":"https://nayan.co/tags/Kafka/"},{"name":"Stream Processing","slug":"Stream-Processing","permalink":"https://nayan.co/tags/Stream-Processing/"},{"name":"Microservices","slug":"Microservices","permalink":"https://nayan.co/tags/Microservices/"}],"author":"Abhishek Bose"},{"title":"Angular Maps | MarkerCluster","slug":"angular-maps","date":"2020-03-04T11:18:37.000Z","updated":"2020-08-26T05:49:07.480Z","comments":true,"path":"/Web/angular-maps/","link":"","permalink":"https://nayan.co/Web/angular-maps/","excerpt":"","text":"Marker ClusterThe marker clustering utility helps you to manage multiple markers at different zoom levels.When a user views the map at a high zoom level, the individual markers show on the map. When the user zooms out, the markers gather together into clusters, to make viewing the map easier.If you have a lot of markers on the map, it’s better to use Marker Cluster setting to organize them better visually. Why marker clustering?The marker clustering utility helps you manage large number of google markers at different zoom levels. To be precise, the ‘markers’ are actually ‘items’ at this point, and only become ‘Markers’ when they’re rendered. Rendering large number of google markers on google map can be very resouce extensive tasks and UI experince is also not good even if we achieve to render them. When a user views the map at a high zoom level, the individual markers show on the map. When the user zooms out, the markers gather together into clusters, to make viewing the map easier. How marker clustering worksThe MarkerClustererPlus library uses the grid-based clustering technique that divides the map into squares of a certain size (the size changes at each zoom level), and groups the markers into each square grid. It creates a cluster at a particular marker, and adds markers that are in its bounds to the cluster. It repeats this process until all markers are allocated to the closest grid-based marker clusters based on the map’s zoom level. If markers are in the bounds of more than one existing cluster, the Maps JavaScript API determines the marker’s distance from each cluster, and adds it to the closest cluster. How to use Marker Cluster in Angular AppsInstallation of modulesWe need to install AGM (Angular Google Maps), js-marker-cluster(peer dependency) NPM: npm install js-marker-clusterer @agm/js-marker-clusterer --save Yarn: yarn add js-marker-clusterer @agm/js-marker-clusterer Module: https://www.npmjs.com/package/@agm/js-marker-clusterer Usage Import the module in module.ts file of your anulgar application 1234567891011121314151617181920212223import &#123; BrowserModule &#125; from &apos;@angular/platform-browser&apos;;import &#123; NgModule &#125; from &apos;@angular/core&apos;;import &#123; AppComponent &#125; from &apos;./app.component&apos;;// add these importsimport &#123; AgmCoreModule &#125; from &apos;@agm/core&apos;;import &#123; AgmJsMarkerClustererModule &#125; from &apos;@agm/js-marker-clusterer&apos;;@NgModule(&#123;declarations: [ AppComponent],imports: [ BrowserModule, AgmCoreModule.forRoot(&#123; apiKey: [&apos;YOUR_API_KEY_HERE&apos;] &#125;), AgmJsMarkerClustererModule],providers: [],bootstrap: [AppComponent]&#125;)export class AppModule &#123; &#125; Import the modules in angular component 123import * as MarkerClusterer from &quot;@google/markerclusterer&quot;new MarkerClusterer(map, opt_markers, opt_options) Use in your angular component 123456&lt;agm-map style=&quot;height: 300px&quot; [latitude]=&quot;51.673858&quot; [longitude]=&quot;7.815982&quot;&gt; &lt;agm-marker-cluster imagePath=&quot;https://raw.githubusercontent.com/googlemaps/v3-utility-library/master/markerclustererplus/images/m&quot;&gt; &lt;agm-marker [latitude]=&quot;51.673858&quot; [longitude]=&quot;7.815982&quot;&gt; &lt;/agm-marker&gt;&lt;!-- multiple markers --&gt; &lt;/agm-marker-cluster&gt;&lt;/agm-map&gt; Customize your marker clustersThere are many ways to adjust how your marker clusters look and function. Many of them won’t even require that you make edits to the underlying library. Instead, there are a number of options you can set when you create your clusters. gridSize: the number of pixels within the cluster grid zoomOnClick: whether to zoom in on a cluster when clicked maxZoom: what farthest level you can zoom in before regular markers are always displayed styles: an array of objects for each cluster type that includes textColor, textSize, and other features of the cluster Example: 123456const clusterOptions = &#123; imagePath: &quot;https://developers.google.com/maps/documentation/javascript/examples/markerclusterer/m&quot;, gridSize: 30, zoomOnClick: false, maxZoom: 10,&#125; OutPut:","categories":[{"name":"Web","slug":"Web","permalink":"https://nayan.co/categories/Web/"}],"tags":[{"name":"Angular","slug":"Angular","permalink":"https://nayan.co/tags/Angular/"},{"name":"Google Maps","slug":"Google-Maps","permalink":"https://nayan.co/tags/Google-Maps/"},{"name":"Large markers","slug":"Large-markers","permalink":"https://nayan.co/tags/Large-markers/"}],"author":"Abhishek Rana"},{"title":"Generating Pdf in Ruby on Rails using Prawn","slug":"generating-pdf-in-ruby-on-rails","date":"2020-02-26T06:02:58.000Z","updated":"2020-08-26T05:49:07.480Z","comments":true,"path":"/Ruby-on-Rails/generating-pdf-in-ruby-on-rails/","link":"","permalink":"https://nayan.co/Ruby-on-Rails/generating-pdf-in-ruby-on-rails/","excerpt":"","text":"We all must have got requirement to generate PDFs at backend and store that on cloud. Well, here is a quick guide on how you can start generating the PDFs your own way without any limits. Lets start generating PDF step by step:-1: Add prawn gem to your Gemfile 1gem 'prawn' 2: Create an instance of PDF document 1receipt_pdf = Prawn::Document.new 3: Draw some text and style that in your own way 1234567receipt_pdf = Prawn::Document.newreceipt_pdf.text 'My Text'receipt_pdf.text 'My Styled Text', style: :boldreceipt_pdf.text 'My Sized Text', size: 20receipt_pdf.text 'My Colored Text', color: '7f7f7f'receipt_pdf.text 'My Aligned Text', align: :rightreceipt_pdf.render_file 'my_pdf_file.pdf' 4: Adding a space/gap before and after drawing a text 12345receipt_pdf = Prawn::Document.newreceipt_pdf.text 'My Text'receipt_pdf.move_down 50receipt_pdf.text 'My Text After Moving down'receipt_pdf.render_file 'my_pdf_file.pdf' 5: Generate Output file 12# This will create a file at your project root directoryreceipt_pdf.render_file 'my_pdf_file.pdf' Drawing tables:-1: Prepare data to draw the table 123456789101112131415# Prepare receipt details to show in a table in the following format# [[a1,a2]# [b1,b2]]## this will form following table# __|_1___2__# a | a1 a2# b | b1 b2table_data = [['Items', 'Rates'], ['Item1', \"1\"], ['Item2', \"2\"], ['', ''], # For adding gap between my data ['Item3', \"3\"], ['Item4', \"4\"]] 2: Add prawn/table requirement before drawing the table 1require 'prawn/table' 3: Draw Table using prepared data 123receipt_pdf = Prawn::Document.newreceipt_pdf.table table_data # table_data used from previous stepreceipt_pdf.render_file 'my_pdf_file.pdf' 4: Styling Rows/Columns/Cells 1234567891011121314receipt_pdf = Prawn::Document.newreceipt_pdf.table table_data, cell_style: &#123;border_width: 0, width: 250, padding: [5, 0, 5, 0], text_color: '373737', inline_format: true&#125; do # Aligning a specific column cells' text to right columns(-1).align = :right # To add bottom padding to a specific row row(-2).padding_bottom = 10 # To set width of border for a specific row row(-1).border_top_width = 1endreceipt_pdf.render_file 'my_pdf_file.pdf' Adding custom font to your PDF document1: Download Font Files in .ttf format2: Place them into font/your_font_name directory at project root level(Create one if it’s not there)3: Set that font to your PDF document instance 12345678910111213141516171819202122232425262728your_font = 'font/your_font_name/your_font_name.ttf'receipt_pdf = Prawn::Document.newdefault_font = receipt_pdf.font.namereceipt_pdf.text 'Default Font Text'# This will change font of your entire document after setting thisreceipt_pdf.font your_fontreceipt_pdf.text 'Your Font Text'# This will change font of your entire document to default fontreceipt_pdf.font default_fontreceipt_pdf.move_down 30# To Style all cells of a tablereceipt_pdf.table table_data, cell_style: &#123;font: your_font&#125;receipt_pdf.move_down 30# To Style all cells of a specific columnreceipt_pdf.table table_data do columns(-1).font = your_fontendreceipt_pdf.render_file 'my_pdf_file.pdf' Limit the PDF page size to drawn area only OR Remove extra white area after drawing all your datamy_pdf_file.pdf 1: Set page length to much higher value than what you want to draw while creating instance on your PDF document 1receipt_pdf = Prawn::Document.new(page_size: [600, 2000], margin: 50) 2: Do your drawing 1receipt_pdf.text 'My Drawing Here' 3: Clip or Resize the document to the drawn area 1234567padding_after_drawing = 15initial_width = 600initial_height = 2000receipt_pdf.page.dictionary.data[:MediaBox] = [0, receipt_pdf.y - padding_after_drawing, initial_width, initial_height]receipt_pdf.render_file 'my_pdf_file.pdf' References:- Prawn Guide with examples Prawn Table Guide with examples Some good reads you may like:- Override Devise Auth Token Controllers Paytm Gateway Integration","categories":[{"name":"Ruby on Rails","slug":"Ruby-on-Rails","permalink":"https://nayan.co/categories/Ruby-on-Rails/"}],"tags":[{"name":"Ashish Jajoria","slug":"Ashish-Jajoria","permalink":"https://nayan.co/tags/Ashish-Jajoria/"},{"name":"backend","slug":"backend","permalink":"https://nayan.co/tags/backend/"},{"name":"rails","slug":"rails","permalink":"https://nayan.co/tags/rails/"},{"name":"ruby","slug":"ruby","permalink":"https://nayan.co/tags/ruby/"},{"name":"ror","slug":"ror","permalink":"https://nayan.co/tags/ror/"},{"name":"ruby on rails","slug":"ruby-on-rails","permalink":"https://nayan.co/tags/ruby-on-rails/"},{"name":"pdf","slug":"pdf","permalink":"https://nayan.co/tags/pdf/"},{"name":"generate pdf","slug":"generate-pdf","permalink":"https://nayan.co/tags/generate-pdf/"}],"author":"Ashish Jajoria"},{"title":"Angular Charts Features","slug":"angular-advanced","date":"2020-02-11T11:33:31.000Z","updated":"2020-08-26T05:49:07.476Z","comments":true,"path":"/Web/angular-advanced/","link":"","permalink":"https://nayan.co/Web/angular-advanced/","excerpt":"","text":"Animation ConfigurationChart.js animates charts out of the box. A number of options are provided to configure how the animation looks and how long it takes. The following animation options are available. The global options for are defined in Chart.defaults.global.animation. Duration: Number of milliseconds an animation takes to complete Easing: linear easeInQuad easeOutQuad easeInOutQuad easeInCubic easeOutCubic easeInOutCubic easeInQuart easeOutQuart easeInOutQuart easeInQuint easeOutQ Animation Callbacks The onProgress and onComplete callbacks are useful for synchronizing an external draw to the chart animation. The callback is passed a Chart.Animation instance: 12345678910111213141516171819202122&#123; // Chart object chart: Chart, // Current Animation frame number currentStep: number, // Number of animation frames numSteps: number, // Animation easing to use easing: string, // Function that renders the chart render: function, // User callback onAnimationProgress: function, // User callback onAnimationComplete: function&#125; Example of preogress bar animation1234567891011var chart = new Chart(ctx, &#123; type: &apos;line&apos;, data: data, options: &#123; animation: &#123; onProgress: function(animation) &#123; progress.value = animation.animationObject.currentStep / animation.animationObject.numSteps; &#125; &#125; &#125;&#125;); Legend ConfigurationThe chart legend displays data about the datasets that are appearing on the chart. PositionPosition of the legend. Options are: top left bottom right AlignAlignment of the legend. Options are: start center end Legend Item Interface Items passed to the legend onClick function are the ones returned from labels.generateLabels. These items must implement the following interface. 12345678910111213141516171819202122232425262728293031323334&#123; // Label that will be displayed text: string, // Fill style of the legend box fillStyle: Color, // If true, this item represents a hidden dataset. Label will be rendered with a strike-through effect hidden: boolean, // For box border. See https://developer.mozilla.org/en/docs/Web/API/CanvasRenderingContext2D/lineCap lineCap: string, // For box border. See https://developer.mozilla.org/en-US/docs/Web/API/CanvasRenderingContext2D/setLineDash lineDash: number[], // For box border. See https://developer.mozilla.org/en-US/docs/Web/API/CanvasRenderingContext2D/lineDashOffset lineDashOffset: number, // For box border. See https://developer.mozilla.org/en-US/docs/Web/API/CanvasRenderingContext2D/lineJoin lineJoin: string, // Width of box border lineWidth: number, // Stroke style of the legend box strokeStyle: Color, // Point style of the legend box (only used if usePointStyle is true) pointStyle: string | Image, // Rotation of the point in degrees (only used if usePointStyle is true) rotation: number&#125; ExampleThe following example will create a chart with the legend enabled and turn all of the text red in color. 123456789101112var chart = new Chart(ctx, &#123; type: &apos;bar&apos;, data: data, options: &#123; legend: &#123; display: true, labels: &#123; fontColor: &apos;rgb(255, 99, 132)&apos; &#125; &#125; &#125;&#125;); TooltipTooltip Configuration Position ModesPossible modes are: average nearest AlignmentThe titleAlign, bodyAlign and footerAlign options define the horizontal position of the text lines with respect to the tooltip box. The following values are supported. * `left` * `right` * `center`Tooltip Callbacks Label Callback The label callback can change the text that displays for a given data point. A common example to round data values; the following example rounds the data to two decimal places. 12345678910111213141516171819var chart = new Chart(ctx, &#123; type: &apos;line&apos;, data: data, options: &#123; tooltips: &#123; callbacks: &#123; label: function(tooltipItem, data) &#123; var label = data.datasets[tooltipItem.datasetIndex].label || &apos;&apos;; if (label) &#123; label += &apos;: &apos;; &#125; label += Math.round(tooltipItem.yLabel * 100) / 100; return label; &#125; &#125; &#125; &#125;&#125;); Label Color Callback For example, to return a red box for each item in the tooltip you could do: 12345678910111213141516171819var chart = new Chart(ctx, &#123; type: &apos;line&apos;, data: data, options: &#123; tooltips: &#123; callbacks: &#123; labelColor: function(tooltipItem, chart) &#123; return &#123; borderColor: &apos;rgb(255, 0, 0)&apos;, backgroundColor: &apos;rgb(255, 0, 0)&apos; &#125;; &#125;, labelTextColor: function(tooltipItem, chart) &#123; return &apos;#543453&apos;; &#125; &#125; &#125; &#125;&#125;); Tooltip Item Interface The tooltip items passed to the tooltip callbacks implement the following interface. 123456789101112131415161718192021222324252627&#123; // Label for the tooltip label: string, // Value for the tooltip value: string, // X Value of the tooltip // (deprecated) use `value` or `label` instead xLabel: number | string, // Y value of the tooltip // (deprecated) use `value` or `label` instead yLabel: number | string, // Index of the dataset the item comes from datasetIndex: number, // Index of this data item in the dataset index: number, // X position of matching point x: number, // Y position of matching point y: number&#125; References https://jtblin.github.io/angular-chart.js/ https://github.com/jtblin/angular-chart.js/blob/master/README.md https://valor-software.com/ng2-charts/","categories":[{"name":"Web","slug":"Web","permalink":"https://nayan.co/categories/Web/"}],"tags":[{"name":"Charts","slug":"Charts","permalink":"https://nayan.co/tags/Charts/"},{"name":"Plugins","slug":"Plugins","permalink":"https://nayan.co/tags/Plugins/"},{"name":"Customization","slug":"Customization","permalink":"https://nayan.co/tags/Customization/"}],"author":"Abhishek Rana"},{"title":"Flask + PyMongo Integration","slug":"Flask-PyMongo-Integration","date":"2020-02-05T11:09:31.000Z","updated":"2020-08-26T05:49:07.420Z","comments":true,"path":"/uncategorized/Flask-PyMongo-Integration/","link":"","permalink":"https://nayan.co/uncategorized/Flask-PyMongo-Integration/","excerpt":"","text":"Flask + PyMongo Integration In my college days I found quite difficulty regarding “How to integrate my application with the database”. May be most of us (mostly college students) still have the same problem. So today in this post I will show you how to integrate a Flask application with PyMongo. What is PyMongo? As according to its official site,“PyMongo is a Python distribution containing tools for working with MongoDB, and is the recommended way to work with MongoDB from Python”. The PyMongo is very easy to use and quite easy to integrate with Flask. For this you must have install MongoDb in your machine. Let’s get started. If you are new to flask then before diving more into it, I highly recommend to check out my previous post in which I discussed about how to create your first application. Firstly we will write some basic files for the project before heading towards the main backend logic. Firstly we write the run.py file and write the code to start our flask server from labeler import app as application from labeler import config application.config.from_object(config) application.config.from_pyfile(&apos;config/config.py&apos;) if __name__ == &quot;__main__&quot;: application.run(host=&apos;0.0.0.0&apos;, port=8000)Now, write the init.py from flask import Flask from flask_bootstrap import Bootstrap app = Flask(__name__) Bootstrap(app) from labeler import routesNow make a settings.py file inside the config folder and write the below code in it. import pymongo from pymongo import MongoClient ENV = &quot;test&quot; if ENV.lower() == &quot;production&quot;: MONGO_DB_NAME = &apos;image_search&apos; MONGO_DB_URL = &apos;localhost&apos; else: MONGO_DB_NAME = &apos;image_search_local&apos; MONGO_DB_URL = &apos;localhost&apos; CLIENT = MongoClient() CLIENT = MongoClient(MONGO_DB_URL, 27017) DB = CLIENT[MONGO_DB_NAME]In this script, we simple configure our mongo database and connect pymongo with the DB. Now, its time to make a script for mongo queries. Lets create a mongo.py file inside mongodb folder and write the below lines in it. import pymongo from pymongo import MongoClient import sys from labeler.config.settings import DB class settingupDb: def __init__(self, query, coll_name): self.query = query self.coll_name = coll_name def constructDb(self): self.coll = DB[self.coll_name] return self.coll def insertsToDb(self,db,coll,query): self.post_id = coll.insert(self.query, check_keys=False) print(&apos;Data inserted for Object ID:: &apos;,self.post_id) def updatesInfo(self, db, coll, query, newVal): self.query = query self.newVal = newVal self.updatedColl = coll.update_many(self.query, self.newVal) def fetchInfo(self, db, coll, query): self.results = coll.find(query) return self.results def aggregateQuery(self, db, coll, query_in_list): self.results = coll.aggregate(query_in_list) return self.results def insertData(query, collection): c_db = settingupDb(query, collection) coll = c_db.constructDb() c_db.insertsToDb(DB, coll, query) def fetchData(collection, query): c_db = settingupDb(query, collection) coll = c_db.constructDb() res = c_db.fetchInfo(DB, coll, query) return res def groupingData(collection, query): c_db = settingupDb(query, collection) coll = c_db.constructDb() res = c_db.aggregateQuery(DB, coll, query) return res def updateData(query, newVal, collection): c_db = settingupDb(query, collection) coll = c_db.constructDb() c_db.updatesInfo(DB, coll, query, newVal)In this script, I created a class settingupDb, it basically sets up the db. Then defines some methods based on the queries. In this project we use some basic queries like :- Insert Find Update Aggregate Will explain the use of these queries when we use them in the project. Now lets make our routes.py file. from labeler import app import json import os from werkzeug import secure_filename import flask from flask import render_template from labeler.backend.handle_requests import STATIC_FOLDER = os.path.dirname(os.path.abspath(__file__)) + &apos;/static/&apos;Firstly we will import everything in the script. The last line is for the static folder where we server our media files. Since our database is empty, so firstly we will write a Data Insertion endpoint. Now, write a handle_requests.py file inside the backend folder. from labeler.mongodb.mongo import fetchData, insertData COLL = &quot;Image-Data&quot; def isLabelInDb(label, image_path): query = {&quot;label&quot;: label} res = fetchData(COLL, query) alreadyPresent = False if res.count() == 0: insert_q = {&quot;label&quot;: label, &quot;image_path&quot;: image_path} insertData(insert_q, COLL) else: alreadyPresent = True return alreadyPresentThis function deals with the insertion and fetching part. The input is the label and the image. Firstly it checks the given label is already present in the database or not, if the label is not present then it inserts it into the database along with the image and if the label is already there in the database then it sets the “alreadyPresent” flag. Now according to the this logic we write our endpoint in the routes.py file. @app.route(&quot;/createLabels&quot;, methods=[&apos;GET&apos;, &apos;POST&apos;]) def createLabels(): if flask.request.method == &apos;POST&apos;: image = flask.request.files[&apos;image&apos;] label = flask.request.form[&apos;label&apos;] image.save(STATIC_FOLDER + secure_filename(image.filename)) alreadyPresent = isLabelInDb(label, image.filename) if alreadyPresent: message = &quot;The label is already in the database. Try with other label&quot; return render_template(&apos;error.html&apos;, message = message) else: message = &quot;The label is successfully inserted to the database&quot; return render_template(&apos;success.html&apos;, message = message) else: return render_template(&apos;createLabels.html&apos;)This endpoint takes two input for a POST request. The “image” and “label”. It calls the function described above and it renders a template with a message. Now we defines our “home” endpoint. In routes.py write the below code. @app.route(&apos;/home&apos;) def home(): allImages = getAllImages() data = {} if len(list(allImages._CommandCursor__data)) != 0: for r in allImages: label = r[&apos;_id&apos;] images = r[&apos;image_path&apos;] data[label] = images return render_template(&apos;home.html&apos;, results=data)This renders all the images with their function to the home page. Lets create a “getAllImages” function for this code. In the handle_requests.py write below code. def getAllImages(): group_q = {&quot;$group&quot;: {&quot;_id&quot;: &quot;$label&quot;, &quot;image_path&quot;: {&quot;$push&quot;: &quot;$image_path&quot;}}} project_q = {&quot;$project&quot;: {&quot;label&quot;: 1, &quot;image_path&quot;:1}} pipeline = [group_q, project_q] res = groupingData(COLL, pipeline) return resThis function is basically grouping all the data based on the labels name present in the database and all image values associated with that are pushed into an array. Now, we will write a function for fetching of image from database. In the handle_requests.py write the below code. def getRequiredImages(label): query = {&quot;label&quot;: label} res = fetchData(COLL, query) totalImages = [] if res.count() != 0: for i in res: image_name = i[&apos;image_path&apos;] totalImages.append(image_name) return totalImagesThis function fetches all the images from the database for the given label name and returns them as a list. Now lets create its endpoint in the routes.py. @app.route(&apos;/fetchImages&apos;, methods=[&apos;POST&apos;]) def fetchImages(): if flask.request.method == &apos;POST&apos;: label = flask.request.form[&apos;label&apos;] totalImages = getRequiredImages(label) if len(totalImages) == 0: message = &quot;No image is present in the database with the label &quot; + str(label) return render_template(&apos;error.html&apos;, message = message) else: data = [totalImages, label] return render_template(&apos;show_images.html&apos;, results=data)This endpoint takes a label name as input and fetches its images from the database and renders the image on the template. Now the last type of operation left is UPDATE. For this write the below code in handle_requests.py. def updateInfo(image_path, curr_label, new_label): curr_label_q = {&quot;label&quot;: curr_label} new_label_q = {&quot;$set&quot;: {&quot;label&quot;: new_label}} updateData(curr_label_q, new_label_q, COLL)This function takes three input :- the image name the current label name the new label name Now its corresponding endpoint in the routes.py @app.route(&apos;/updateLabel&apos;, methods=[&apos;POST&apos;]) def updateLabel(): if flask.request.method == &apos;POST&apos;: image = flask.request.form[&apos;image&apos;] curr_value = flask.request.form[&apos;current_label&apos;] new_value = flask.request.form[&apos;new_label&apos;] updateInfo(image, curr_value, new_value) message = &quot;Label is updated !!&quot; return render_template(&apos;success.html&apos;, message=message)This endpoint shows a message after successful updation of labels Conclusion This is the very basic application which is made by integrating flask with pymongo. The code is available on github. The necessary templates are also uploaded there.","categories":[],"tags":[],"author":"Himanshu Garg"},{"title":"App-Heartbeat","slug":"App-Heartbeat","date":"2020-02-03T13:40:48.000Z","updated":"2020-08-26T05:49:07.408Z","comments":true,"path":"/Android/App-Heartbeat/","link":"","permalink":"https://nayan.co/Android/App-Heartbeat/","excerpt":"","text":"This is an era of mobile technology where everyone is a smartphone user. To be able to use a smartphone we need to ‘Interact’ with it. A simple touch with a finger to open an app is an example of this ‘interact’ and this phenomenon is called ‘User Interaction’.Heart of application will continue beat, as long as user interact. Heartbeat is used to calculate sessions, for how much time a user is interacted with application. When a user is started using app, we create a session. A session has two values, start time and end time. For a new session both start and end time will same (current time of system). Every one minute, we check, is user interacted or not. If user is interacted then we update that session by changing its end time (now end time for that session will be current time of system). If user is not interacted then we create a new session. Reason behind to create a heartbeat of application , we will have at least 60 seconds lost. Let’s get startedCreate a modelIn that data class, we will handle some sessions related tasks (check session is active or not, update session etc.). A session will have two values start time and end time. 1234data class Session( val startTime: Long, var endTime: Long ) We perform some operations in session - - Is session active :In that we check current session is active or not. If difference between System current time and end time of that session is less than a heartbeat( heartbeat duration + heartbeat buffer). In our case heartbeat duration is 60 seconds and heartbeat buffer is 2 seconds. 123fun isActive(): Boolean &#123; return System.currentTimeMillis() - endTime &lt; HEARTBEAT_DURATION + HEARTBEAT_BUFFER &#125; - Update current session:If session is active then we update current session. For updating current session we will put system current time in end time of that session. 1234fun update(): Session &#123; endTime = System.currentTimeMillis() return this &#125; Create a managerFor handling all heartbeat operations, we will create a session manager. A session manager will manage all sessions activities like — add session, update session. When we open application in onResume() of activity we will start a handler and onPause(),will stop handler. 12345678910111213141516init &#123; lifecycleOwner.lifecycle.addObserver(object : LifecycleObserver &#123; @OnLifecycleEvent(Lifecycle.Event.ON_RESUME) fun startSession() &#123; Timber.d(&quot;Starting Session for activity:$&#123;lifecycleOwner.javaClass.name&#125;&quot;) handler.post(runnable) &#125; @OnLifecycleEvent(Lifecycle.Event.ON_PAUSE) fun pauseSession() &#123; Timber.d(&quot;Pausing Session for activity: $&#123;lifecycleOwner.javaClass.name&#125;&quot;) handler.removeCallbacks(runnable) &#125; &#125;) &#125; In handler we are checking heartbeat of application, that user is interacted with app for last 60 seconds or not. If user is interacted then update current session otherwise create new session. We store these sessions into shared preferences. 123456789101112131415fun heartBeat() &#123; if (hasUserInteracted) &#123; val currentSession = getLastSession() if (currentSession != null &amp;&amp; currentSession.isActive()) &#123; updateCurrentSession() &#125; else &#123; createNewSession() &#125; &#125; else &#123; handler.removeCallbacks(runnable) onSessionTimeoutListener.onTimeout() &#125; hasUserInteracted = false&#125; For more sample code , see the App-Heartbeat And we’re done!","categories":[{"name":"Android","slug":"Android","permalink":"https://nayan.co/categories/Android/"}],"tags":[{"name":"android","slug":"android","permalink":"https://nayan.co/tags/android/"},{"name":"user interaction","slug":"user-interaction","permalink":"https://nayan.co/tags/user-interaction/"},{"name":"sessions","slug":"sessions","permalink":"https://nayan.co/tags/sessions/"},{"name":"Diwakar Singh","slug":"Diwakar-Singh","permalink":"https://nayan.co/tags/Diwakar-Singh/"}],"author":"Diwakar Singh"},{"title":"Override Devise Auth Token Controllers","slug":"override-devise-auth-token-controllers","date":"2020-01-29T15:56:25.000Z","updated":"2020-08-26T05:49:07.484Z","comments":true,"path":"/Ruby-on-Rails/override-devise-auth-token-controllers/","link":"","permalink":"https://nayan.co/Ruby-on-Rails/override-devise-auth-token-controllers/","excerpt":"","text":"For authentication and token management at backend in Ruby On Rails we use devise-token-auth. Sometimes we need to update some of the following default behaviours:- Registration(via facebook, twitter, mobile, email etc.) Password reset flow(email reset link OR OTP based) We would like to add or remove some fields from the signin API.etc Configutation:-Use devise-token-auth-guide to setup your devise configuration. After configuration, your routes.rb would look like this: 12# config/routes.rbmount_devise_token_auth_for 'User', at: 'auth' Overriding:- Create a package named overrides, in cotrollers package. For overriding RegistrationsController used for signup flow, add registrations_controller.rb to the package we just created and extent the RegistrationsController by DeviseTokenAuth::RegistrationsController. 12345module Overrides class RegistrationsController &lt; DeviseTokenAuth::RegistrationsController ... endend Now write the create method yourself for your custom parameters you want to use while signing up a user with custom conditions and if there is any condition when you don’t want to handle, then just call super and the default signup flow will work for that case. 1234567891011121314151617181920212223module Overrides class RegistrationsController &lt; DeviseTokenAuth::RegistrationsController def create ... #Your custom conditions and handling @resource = User.new(email: email) #This may vary based on your params and conditions you want @resource.name = params[:name] @resource.password = params[:password] unless @resource.save render json: &#123; message: @resource.errors.full_messages.join(', ') &#125;, status: :bad_request return end @token = @resource.create_token @resource.save update_auth_header render_create_success end endend Now Update your routes.rb 123mount_devise_token_auth_for 'User', at: 'auth', controllers: &#123; registrations: 'overrides/registrations'&#125; Likewise we can override following controllers:- ConfirmationsController PasswordsController OmniauthCallbacksController SessionsController TokenValidationsController&nbsp; References:- devise_token_auth gem Devise Token Auth Guide Some good reads you may like:- Paytm Gateway Integration Generating Pdf in Ruby on Rails using Prawn","categories":[{"name":"Ruby on Rails","slug":"Ruby-on-Rails","permalink":"https://nayan.co/categories/Ruby-on-Rails/"}],"tags":[{"name":"Ashish Jajoria","slug":"Ashish-Jajoria","permalink":"https://nayan.co/tags/Ashish-Jajoria/"},{"name":"backend","slug":"backend","permalink":"https://nayan.co/tags/backend/"},{"name":"rails","slug":"rails","permalink":"https://nayan.co/tags/rails/"},{"name":"ruby","slug":"ruby","permalink":"https://nayan.co/tags/ruby/"},{"name":"ror","slug":"ror","permalink":"https://nayan.co/tags/ror/"},{"name":"authentication","slug":"authentication","permalink":"https://nayan.co/tags/authentication/"},{"name":"devise","slug":"devise","permalink":"https://nayan.co/tags/devise/"},{"name":"devise_auth_token","slug":"devise-auth-token","permalink":"https://nayan.co/tags/devise-auth-token/"}],"author":"Ashish Jajoria"},{"title":"Character-Recognition-with-CNN-network","slug":"Character-Recognition-with-CNN-network","date":"2020-01-29T11:47:04.000Z","updated":"2020-08-26T05:49:07.412Z","comments":true,"path":"/AI/Character-Recognition-with-CNN-network/","link":"","permalink":"https://nayan.co/AI/Character-Recognition-with-CNN-network/","excerpt":"","text":"IntroductionThis post talks about a simple Convolution Neural Network (CNN) which is used torecognize characters i.e. Numeric and Alphabet. We have total 10 Numeric and 26Alphabets that sums up the total number of classes in our network to 36. Inorder to get characters from the License Plates we first need to use some kindof License Plate detector which is followed by a Character segmentation methodin order to extract character from the License Plates (LP). Architecture of modelWe have used very familiar CNN network for OCR, usually CNN consists of someConvolution layers(All Convolution layers are followed by max pooling layers)and fully connected layers. We already know much about Convolution layers so i am gonna talk about maxpooling and fully connected layers here. Pooling layers section would reduce the number of parameters when theimages are too large. Spatial pooling also called sub-sampling or down-samplingwhich reduces the dimensionality of each map but retains important information.Spatial pooling can be of different types: Max Pooling Average Pooling Sum Pooling Max pooling takes the largest element from the rectified feature map. Taking thelargest element could also take the average pooling. Sum of all elements in thefeature map call as sum pooling. The layer we call as Fully Connected Layer (FC) layer, we flattened ourmatrix into vector and feed it into a fully connected layer like a neuralnetwork. After the last max pooling layer there will be a sequence of FC layers.Finally we will apply an activation function such as softmax or sigmoid toclassify the outputs between classes. Model configuration is given below: Total layer : 14 Convolution with 64 different filters in size of (3x3) Max Pooling by 2 ReLU activation function Batch Normalization Convolution with 128 different filters in size of (3x3) Max Pooling by 2 ReLU activation function Batch Normalization Convolution with 256 different filters in size of (5x5) Max Pooling by 2 ReLU activation function Batch Normalization Convolution with 512 different filters in size of (5x5) Max Pooling by 2 ReLU activation function Batch Normalization Flattening the 3-D output of the last convolving operations. Fully Connected Layer with 128 units Fully Connected Layer with 256 units Fully Connected Layer with 512 units Fully Connected Layer with 1024 units Fully Connected Layer with 36 units (number of classes) Figure 1. Architecture of model PlaceholdersDefining a placeholder in tensorflow is very common. When we want to declare ourinput and output without initialization this method comes very useful. You canuse them during training of model by feeding them with training data and labels. 12345678910111213def create_placeholders(n_H0, n_W0, n_C0, n_y): X = tf.placeholder(tf.float32, shape = (None, n_H0, n_W0, n_C0), name=&apos;X&apos;) Y = tf.placeholder(tf.float32, shape = (None, n_y), name = &apos;Y&apos;) keep_prob = tf.placeholder(tf.float32, name=&quot;keep_prob&quot;) return X,Y,keep_prob# X_train contains training data with shape (batch_size,height,widht,channel)# Y_train contains labels of training data with shape (batch_size,num_classes,1)m, n_H0, n_W0, n_C0 = X_train.shapen_y = Y_train.shape[1]X, Y, keep_prob = create_placeholders(n_H0, n_W0, n_C0, n_y) Once you have defined your model architecture you now need to define cost andoptimizer for your model which is defined in the next section. Cost function and optimizerCost function gives degree of error between predicted and expected values andafter that it represent it in form of a real number. Whereas optimizer updatethe weight parameters to minimize the cost function. Finally, you’ll define cost, optimizer, and accuracy. The tf.reduce_meantakes an input tensor to reduce, and the input tensor is the results of certainloss functions between predicted results and ground truths. We have to measureloss over 36 classes, tf.nn.softmax_cross_entropy_with_logis function isused. When training the network, what you want is minimize the cost by applying aalgorithm of your choice. It could be SGD,AdamOptimizer,AdagradOptimizer orsomething else. You have to study how each algorithm works to choose whatto use, but AdamOptimizer works fine for most cases in general. Please find cost and optimizer sample below: 12345678910111213141516learning_rate = 0.001# X is placeholder you defined in previous sectionZ3 = forward_propagation(X, keep_prob)# Z3 has the model structure# Loss and Optimizercost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=Z3, labels=Y))optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)# Accuracyy_pred = tf.nn.softmax(Z3)y_pred_class = tf.argmax(y_pred, axis = 1)y_true_class = tf.argmax(Y, axis = 1)correct_prediction = tf.equal(y_pred_class, y_true_class)accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) ConclusionSo in this post i have explained basic steps to train simple CNN network for anyclassification task i.e. OCR in this particular post. I have given all the stepsexcept the training part for that you just need to use session of tensorflowwhile feeding image data and labels for those images to placeholder you havecreated to the session.run function.","categories":[{"name":"AI","slug":"AI","permalink":"https://nayan.co/categories/AI/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://nayan.co/tags/Machine-Learning/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"https://nayan.co/tags/Deep-Learning/"},{"name":"OCR","slug":"OCR","permalink":"https://nayan.co/tags/OCR/"},{"name":"Character Recognition","slug":"Character-Recognition","permalink":"https://nayan.co/tags/Character-Recognition/"}],"author":"Piyush Jain"},{"title":"Getting Started with Ruby on Rails","slug":"rails-getting-started","date":"2020-01-28T08:00:00.000Z","updated":"2020-08-26T05:49:07.516Z","comments":true,"path":"/Rails/rails-getting-started/","link":"","permalink":"https://nayan.co/Rails/rails-getting-started/","excerpt":"","text":"We recently inducted a couple of engineers into our Rails team. Both of them had a web frontend experience, but zero experience with Ruby or backend development. Thanks to the simplicity and convention focussed approach of the Rails framework, both of them were writing test driven production grade code within a week! First thing was setting up their machines. We decided to go with Ubuntu 18.04 for the OS RVM as the Ruby version manager RubyMine as the IDE Postgresql as the database RubyFirst step was to get them comfortable with Ruby. We believe that doing is the best way of learning. So we got them to complete the wonderful koans at http://rubykoans.com/ to get hands on practice. An important thing while working in a team is to have consistent coding style across all members. We follow the style guide at https://github.com/rubocop-hq/ruby-style-guide , so they read through the guide. RailsWe feel the best guide for Rails is the official guide itself. https://edgeguides.rubyonrails.org/ The three sections that were assigned were, Getting Started with Rails Models Controllers We skipped the views as we mostly work on API only apps. After this, we covered the Rails style guide https://github.com/rubocop-hq/rails-style-guide . TestsWe use RSpec and FactoryBot internally for writing our tests. For RSpec, the Github page is a good starting point https://github.com/rspec/rspec-rails For FactoryBot, we assigned the Getting Started guide on Github https://github.com/thoughtbot/factory_bot/blob/master/GETTING_STARTED.md Continuing EducationWith these basic tutorials, the engineers were basically ready for contributing to production. Their first few pull requests had many comments, but they came down significantly within the first two weeks. For continuing our Rails education, we keep reading up on the frequent gems that we use, such as devise aasm active_model_serializers pundit resque whenever carrierwave and other excellent gems. ConclusionWe got a pleasant reminder as to why Rails is our favorite framework to work on. Within the first few weeks only the engineers were writing production grade, well tested code. Hats off to the Ruby philosophy and Matz that our fresh Ruby engineers were able to start guessing the function names for different classes almost immediately!","categories":[{"name":"Rails","slug":"Rails","permalink":"https://nayan.co/categories/Rails/"}],"tags":[{"name":"Ruby","slug":"Ruby","permalink":"https://nayan.co/tags/Ruby/"},{"name":"Rails","slug":"Rails","permalink":"https://nayan.co/tags/Rails/"}],"author":"Anuj Middha"},{"title":"MySQL in custom Grafana panels","slug":"using-mysql-in-custom-grafana-panel","date":"2020-01-26T19:18:48.000Z","updated":"2020-08-26T05:49:07.516Z","comments":true,"path":"/Web/using-mysql-in-custom-grafana-panel/","link":"","permalink":"https://nayan.co/Web/using-mysql-in-custom-grafana-panel/","excerpt":"","text":"In this post, we will see how to use MySQL in your custom panel in Grafana. If you don’t know how to make custom panels read this post before this. https://nayan.co/blog/2020/01/06/create-custom-grafana-panel/ MySQL in GrafanaTo use your MySQL in your dashboard, you need to link your MySQL db with your dashboard. To do so, follow the steps below: Find the Configurations icon of your Grafana on the left side of your dashbaord. Click Add Datasource Click on Add New Source Choose MySQL and add your databse credentials. Using Data in your panel.To show data from your SQL db to your panel. You need to use it in your panel’s module.ts and frontend page. This example has data integration, if you want to see a code for referral.https://github.com/grafana/piechart-panel To get the data in your panel you need to call your function when queries fetch the data from DB. There are 5 events in a grafana panel which are listed below: 12345this.events.on(&apos;render&apos;, this.onRender.bind(this));this.events.on(&apos;data-received&apos;, this.onDataReceived.bind(this));this.events.on(&apos;data-error&apos;, this.onDataError.bind(this));this.events.on(&apos;data-snapshot-load&apos;, this.onDataReceived.bind(this));this.events.on(&apos;init-edit-mode&apos;, this.onInitEditMode.bind(this)); The only one we need to think about right now is data-received event. This event is called whenever an SQL query is entered in the Query Editor of a panel in Grafana. onDataReceived123456onDataReceived(dataList: any) &#123; console.log(dataList) this.series = dataList.map(this.seriesHandler.bind(this)); this.data = this.parseSeries(this.series); this.render(this.data);&#125; This function is called whenever data is received on panel. Since we bind this in this function. Its own object is sent in this function. dataList contains all the data in this case. You can manipulate and show the data however you want in this function. You can enter any number of queries in the Queries Section of your panel in the dashboard and use them as they are sent in dashboard as an Array output. This is how you use SQL DB in your custom Grafana Panel Tips: You can use $__timeFilter(column_name) to filter the data according to the timestamp provided in Grafana dashboard. This is an inbuilt function in grafana MySQL. References: https://grafana.com/ https://github.com/grafana/piechart-panel","categories":[{"name":"Web","slug":"Web","permalink":"https://nayan.co/categories/Web/"}],"tags":[{"name":"Grafana","slug":"Grafana","permalink":"https://nayan.co/tags/Grafana/"},{"name":"MySQL","slug":"MySQL","permalink":"https://nayan.co/tags/MySQL/"}],"author":"Manan"},{"title":"boost-up-neural-networks-using-docker-containers-and-spark","slug":"boost-up-neural-networks-using-docker-containers-and-spark","date":"2020-01-09T15:13:20.000Z","updated":"2020-08-26T05:49:07.480Z","comments":true,"path":"/AI/boost-up-neural-networks-using-docker-containers-and-spark/","link":"","permalink":"https://nayan.co/AI/boost-up-neural-networks-using-docker-containers-and-spark/","excerpt":"","text":"Boost up neural networks using docker containers and pyspark part : 1 src: https://images.app.goo.gl/itH6Cbq8LK7cNZxM8 Introduction: In this blog we will learn how we can use spark to boost up the inference speed of the neural network model. The whole topic is too long to cover in a single blog so we will divide it in two parts Create spark cluster on which we will run multiple instances of the model Run the neural networks on the cluster Lets begin, For creating cluster we will use docker containers and create a common network where they can communicate with each other Lets start with introduction of Docker: Docker is a set of platform as a service products that use OS-level virtualization to deliver software in packages called containers. Containers are isolated from one another and bundle their own software, libraries and configuration files; they can communicate with each other through well-defined channels. [https://en.wikipedia.org/wiki/Docker(software)](https://en.wikipedia.org/wiki/Docker_(software))_ Thanks to docker containers we will be able to create several worker nodes on which we can run spark for distributed processing. Docker Installation: Docker installation is very easy just follow the steps in the following link Get Docker Engine - Community for Ubuntu Create Docker image : To create a docker image we need to first create a Dockerfile which upon building will install all the libraries that we want. Sample docker file: 1234567891011121314151617181920FROM nvidia/cuda:10.2-cudnn7-devel-ubuntu18.04MAINTAINER XYZRUN apt-get update -y &amp;&amp; apt-get install -y python3-pip python3-dev libsm6 libxext6 libxrender-dev# addonsRUN \\ apt-get install -y \\ wget \\ unzip \\ ffmpeg \\ gitRUN pip3 install opencv-pythonRUN pip3 install moviepyRUN pip3 install pandasRUN pip3 install requestsRUN pip3 install numbaRUN pip3 install imutilsRUN pip3 install filterpyRUN pip3 install sklearnRUN pip3 install kafka-python Now build the docker image using the following command 1$ docker build -t &quot;my_docker_image&quot; . This will create a docker image with name my_docker_image You can see the list of images by using the following command 1$ docker images You should see your newly created image. Now we are ready to create containers using the above image. Create docker network: Docker network is essential when containers wants to communicate . The most common and default network is bridge. Run the following command to create a network 1$ docker network create &quot;my_network&quot; Run the following command to list all existing networks 1$ docker network ls You should see your newly created networks Now that we have created the network , we can now create containers and bind them with the network that we created. Create docker container: Let’s first create a master node of the cluster Run the following command to create the master docker container 1$ docker run -it --name master-docker --network my_network my_image /bin/sh This command will create the container and attach a shell with the container To check if the docker is running , run the following command to list down all running container 1$ docker ps -a You should see the container master-docker running Now lets create 2 worker nodes The following commands will create two worker nodes with names worker_docker_1 and worker_docker_2 1$ docker run -it --name worker_docker_1 --network my_network my_image /bin/sh 1$ docker run -it --name worker_docker_2 --network my_network my_image /bin/sh Now again check the running container now you will be able to see one master node and 2 worker nodes. That’s it for this part , tune in for part 2 of the blog where we will run spark over the cluster and run neural network over it","categories":[{"name":"AI","slug":"AI","permalink":"https://nayan.co/categories/AI/"}],"tags":[{"name":"spark","slug":"spark","permalink":"https://nayan.co/tags/spark/"},{"name":"Docker","slug":"Docker","permalink":"https://nayan.co/tags/Docker/"},{"name":"neural-network","slug":"neural-network","permalink":"https://nayan.co/tags/neural-network/"}],"author":"Ajay Singh"},{"title":"Efficient-Residual-Factorized-Neural-Network-for-Semantic-Segmentation","slug":"Efficient-Residual-Factorized-Neural-Network-for-Semantic-Segmentation","date":"2020-01-09T08:37:31.000Z","updated":"2020-08-26T05:49:07.412Z","comments":true,"path":"/AI/Efficient-Residual-Factorized-Neural-Network-for-Semantic-Segmentation/","link":"","permalink":"https://nayan.co/AI/Efficient-Residual-Factorized-Neural-Network-for-Semantic-Segmentation/","excerpt":"","text":"This post explains a research paper ERFNET, a real time and accurate ConvNet for semantic segmentation and the underlying concepts. Semantic SegmentationSemantic segmentation is the task of classifying each image pixel to a class label. It is a classification task but at pixel level instead of image level. The labels could include a person, car, flower, piece of furniture, etc. For example, in the below image, all the cars will have the same labels. However, one can differentiate between the same class objects, this is called instance segmentation. For example, in an image that has many cars, instance segmentation can differentiate between each car object. Semantic segmentation has many applications in autonomous vehicles, Advanced Driver Assistance Systems (ADAS), robotics, self-driving cars because it is important to know the context in which the agent operates. Convolutional Neural Networks (CNN), which initially designed for classification tasks, have impressive capabilities in solving complex segmentation tasks as well. Residual layers have created a new trend in ConvNets design. Their reformulation of the convolutional layers to avoid the degradation problem of deep architectures allowed neural networks to achieve very high accuracies with large amounts of layers. Computation resources are key factors in self-driving and autonomous vehicles. Algorithms are not only required to operate reliably, but they are required to operate fast (real-time), fit in embedded devices due to space constraints (compactness), and have low power consumption to affect as minimum as possible the vehicle autonomy. Considering a reasonable amount of layers, enlarging the depth with more convolutions achieves only small gains in accuracy while significantly increasing the required computational resources. Residual LayerThe paper proposes a new architecture design that leverages skip connections and convolutions with 1D kernels. While the skip connections allow the convolutions to learn residual functions that facilitate training, the 1D factorized convolutions allow a significant reduction of the computational costs while retaining a similar accuracy compared to the 2D ones. Residual blocks allow convolutional layers to learn the residual functions. For example, in the above image, x is the input vector and F(X)+x is the output vector of the y vector. F(X) is the residual function to be learned. This residual formulation facilitates learning and significantly reduces the degradation problem present in architectures that stack a large number of layers. Non-bottleneck Residual LayersThere can be two instances of residual layer: the non-bottleneck design with two 3x3 convolutions as depicted in Fig. 1(a), and the bottleneck version as depicted in Fig. 1(b). Both versions have a similar number of parameters and almost equivalent accuracy. However, the bottleneck requires less computational resources and these scale in a more economical way as depth increases. Hence, the bottleneck design has been commonly adopted in state-of-the-art networks. However, it has been reported that non-bottleneck ResNets gain more accuracy from increased depth than the bottleneck versions, which indicates that they are not entirely equivalent and that the bottleneck design still suffers from the degradation problem The paper proposed a new implementation of the residual layer that decomposes 2D convolution into a pair of 1D convolutions to accelerate and reduce the parameters of the original non-bottleneck layer. We refer to this proposed module as “non-bottleneck-1D” (non-bt-1D), which is depicted in Fig. 1(c). This module is faster (as in computation time) and has fewer parameters than the bottleneck design while keeping a learning capacity and accuracy equivalent to the non-bottleneck one. Dilated ConvolutionsDilated convolutions are convolutions applied to input images with gaps. The standard convolution is 1-Dilated convolution. Dilated convolutions other than standard convolutions increase the receptive field of the network. Dilated convolutions are more effective in terms of computational cost and parameters than the convolutions with larger kernel size. The paper proposes a network that uses dilated convolution. Network ArchitectureThe paper presents an encoder-decoder architecture for semantic segmentation. The encoder segment produces downsampled feature maps and the decoder segments upsample the features to match input image resolution. Full network architecture is given in Figure 2 References https://pixabay.com/photos/traffic-locomotion-roadway-mobility-3612474/ ERFNet: Efficient Residual Factorized ConvNet for Real-time Semantic Segmentation. https://cdn-images-1.medium.com/max/800/1*wz4x8BcAOFBPNL6nX4tx-g.gif https://miro.medium.com/max/395/0*3cTXIemm0k3Sbask.gif","categories":[{"name":"AI","slug":"AI","permalink":"https://nayan.co/categories/AI/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://nayan.co/tags/Machine-Learning/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"https://nayan.co/tags/Deep-Learning/"},{"name":"Residual Networks","slug":"Residual-Networks","permalink":"https://nayan.co/tags/Residual-Networks/"},{"name":"Dilated Convolutions","slug":"Dilated-Convolutions","permalink":"https://nayan.co/tags/Dilated-Convolutions/"},{"name":"Semantic segmentation","slug":"Semantic-segmentation","permalink":"https://nayan.co/tags/Semantic-segmentation/"},{"name":"Autonomous driving","slug":"Autonomous-driving","permalink":"https://nayan.co/tags/Autonomous-driving/"}],"author":"Anand Kummari"},{"title":"Creating custom Grafana panel","slug":"create-custom-grafana-panel","date":"2020-01-06T11:29:03.000Z","updated":"2020-08-26T05:49:07.480Z","comments":true,"path":"/Web/create-custom-grafana-panel/","link":"","permalink":"https://nayan.co/Web/create-custom-grafana-panel/","excerpt":"","text":"GrafanaGrafana is an open source platform for monitoring and observability. Grafana allows you to query, visualize, alert on and understand your metrics no matter where they are stored. Create, explore, and share dashboards with your team and foster a data driven culture. Grafana is a tool that is used to create dashboards. Dashboards consists of Panels which are used to visualize data in many ways. Grafana has many types of panels available to visualize your data in many forms. Such panels include Graphs, Tables, Single stats, etc. But what if you want to use the Grafana but show data in your own format. Grafana allows that as well. These components are known as plugins. In this post we will see how to make our own custom plugin and use it in our Grafana dashboard. Requisites: npm or yarn Git First download the Grafana repo from Grafana websitehttps://grafana.com/grafana/download Then clone Hello World Custom panel with the command written below in the folder data/plugins or var/lib/grafana/plugins. If no such folder exists in root directory create one.Git clone https://github.com/grafana/simple-angular-panel This folder will be created in your data/plugins. Install the dependencies required for that panel using npm install or yarn. Then build the plugin using yarn build or npm run build. (Running the build script defined in package.json) Then start the grafana-server: By running the command ./bin/grafana-server in the root directory of your grafana repo (Linux). By running the grafana-server.exe in bin Folder (Windows). Then panel will be availables in your visualization section when you create a new panel Plugin.jsonIt defines your panel and uniquely identifies it. 1234567891011121314151617181920212223242526&#123;&quot;type&quot;: &quot;panel&quot;,&quot;name&quot;: &quot;Simple Angular&quot;,&quot;id&quot;: &quot;simple-angular-panel&quot;,&quot;info&quot;: &#123;&quot;description&quot;: &quot;Simple Angular Panel&quot;,&quot;author&quot;: &#123;&quot;name&quot;: &quot;Grafana Labs&quot;&#125;,&quot;keywords&quot;: [ &quot;discrete&quot;, &quot;events&quot;, &quot;strings&quot; ],&quot;logos&quot;: &#123;&quot;small&quot;: &quot;img/logo.svg&quot;,&quot;large&quot;: &quot;img/logo.svg&quot;&#125;,&quot;links&quot;: [],&quot;screenshots&quot;: [],&quot;version&quot;: &quot;%VERSION%&quot;,&quot;updated&quot;: &quot;%TODAY%&quot;&#125;,&quot;dependencies&quot;: &#123;&quot;grafanaVersion&quot;: &quot;6.3.x&quot;,&quot;plugins&quot;: [ ]&#125;&#125; “name”: Name Displayed on panel “id”: Unique Identified of your plugin. If two panels have same ID only 1 will be shown “logos”: shows the images shown for your panel which can be found in src/img folder. Module.tsIt is the entry point of your panel. Functions and variables can be created in module.ts and be used In html page using “ctrl” as the object of the class. Front HTML Page.This page can be found at partials/module.html. So if you create a variable named “text” in the SimpleCtrl Class (in module.ts). It can be accessed in HTML page using ““. Options.html page contains the page the visualization section which is shown when you create or edit a panel. After You create your html pages and functionality in module.ts, you can run npm run build or yarn build and use the panel in your own Grafana however you like. Tips : If you want to see changes in your panel while you are creating it. Use npm run watch or yarn watch. It implements changes as you make them. It basically creates build with environment settings as development. You can see the changes by refreshing the webpage. References: https://grafana.com/grafana/download https://github.com/grafana/simple-angular-panel","categories":[{"name":"Web","slug":"Web","permalink":"https://nayan.co/categories/Web/"}],"tags":[{"name":"Grafana","slug":"Grafana","permalink":"https://nayan.co/tags/Grafana/"}],"author":"Manan"},{"title":"Angular Charts","slug":"angular-charts","date":"2020-01-02T16:27:55.000Z","updated":"2020-08-26T05:49:07.476Z","comments":true,"path":"/Web/angular-charts/","link":"","permalink":"https://nayan.co/Web/angular-charts/","excerpt":"","text":"Chart.js is a popular JavaScript charting library and ng2-charts is a wrapper for Angular 2+ that makes it easy to integrate Chart.js in Angular. Let’s go over the basic usage. Installation Install ng2-charts using npm: npm install --save ng2-charts Install Chart.js library: npm install --save chart.js [Options] Then, if you’re using the Angular CLI, you can simply add Chart.js to the list of scripts in your .angular-cli.json file so that it gets bundled with the app: angular-cli.json 123&quot;scripts&quot;: [ &quot;../node_modules/chart.js/dist/Chart.min.js&quot;], APINow you’ll want to import ng2-chart’s ChartsModule into your app module or a feature module: app.module.ts 12345678910111213141516import &#123; BrowserModule &#125; from &apos;@angular/platform-browser&apos;;import &#123; NgModule &#125; from &apos;@angular/core&apos;;import &#123; ChartsModule &#125; from &apos;ng2-charts&apos;;import &#123; AppComponent &#125; from &apos;./app.component&apos;;@NgModule(&#123; declarations: [AppComponent], imports: [ BrowserModule, ChartsModule ], providers: [], bootstrap: [AppComponent]&#125;)export class AppModule &#123;&#125; Usageng2-charts gives us a baseChart directive that can be applied on an HTML canvas element. Here’s an example showing-off some of the options to pass-in as inputs and the chartClick event that’s outputted by the directive: app.component.html 1234567891011&lt;div style=&quot;width: 40%;&quot;&gt; &lt;canvas baseChart [chartType]=&quot;&apos;line&apos;&quot; [datasets]=&quot;chartData&quot; [labels]=&quot;chartLabels&quot; [options]=&quot;chartOptions&quot; [legend]=&quot;true&quot; (chartClick)=&quot;onChartClick($event)&quot;&gt; &lt;/canvas&gt;&lt;/div&gt; And here’s what it can look like in our component class: app.component.ts 1234567891011121314151617181920import &#123; Component &#125; from &apos;@angular/core&apos;;@Component(&#123; ... &#125;)export class AppComponent &#123; chartOptions = &#123; responsive: true &#125;; chartData = [ &#123; data: [330, 600, 260, 700], label: &apos;Account A&apos; &#125;, &#123; data: [120, 455, 100, 340], label: &apos;Account B&apos; &#125;, &#123; data: [45, 67, 800, 500], label: &apos;Account C&apos; &#125; ]; chartLabels = [&apos;January&apos;, &apos;February&apos;, &apos;Mars&apos;, &apos;April&apos;]; onChartClick(event) &#123; console.log(event); &#125;&#125; OptionsHere’s a quick breakdown of the different input options chartType: This sets the base type of the chart. The value can be pie, doughnut, bar, line, polarArea, radar or horizontalBar. legend: A boolean for whether or not a legend should be displayed above the chart. datasets: This should be an array of objects that contain a data array and a label for each data set. data: If your chart is simple and has only one data set, you can use data instead of datasets and pass-in an array of data points. labels: An array of labels for the X-axis. options: An object that contains options for the chart. You can refer to the official Chart.js documentation for details on the available options. In the above example we set the chart to be responsive and adapt depending on the viewport size. colors: Not shown in the above example, but you can define your own colors with the colors input. Pass-in an array of object literals that contain the following value: 1234567891011myColors = [ &#123; backgroundColor: &apos;rgba(103, 58, 183, .1)&apos;, borderColor: &apos;rgb(103, 58, 183)&apos;, pointBackgroundColor: &apos;rgb(103, 58, 183)&apos;, pointBorderColor: &apos;#fff&apos;, pointHoverBackgroundColor: &apos;#fff&apos;, pointHoverBorderColor: &apos;rgba(103, 58, 183, .8)&apos; &#125;, ... other colors]; EventsTwo events are emitted, chartClick and chartHover, and they allow to react to the user interacting with the chart. The currently active points and labels are returned as part of the emitted event’s data. chartClick: fires when click on a chart has occurred, returns information regarding active points and labels chartHover: fires when mousemove (hover) on a chart has occurred, returns information regarding active points and labels Updating Datasets Dynamically: Of course, the beauty of Chart.js is that your charts can easily by dynamic and update/respond to data received from a backend or from user input. In the bellow example we add a new data points for the month of May: app.component.ts 1234567891011newDataPoint(dataArr = [100, 100, 100], label) &#123; this.chartData.forEach((dataset, index) =&gt; &#123; this.chartData[index] = Object.assign(&#123;&#125;, this.chartData[index], &#123; data: [...this.chartData[index].data, dataArr[index]] &#125;); &#125;); this.chartLabels = [...this.chartLabels, label];&#125; And it can be used like this: app.component.html 123&lt;button (click)=&quot;newDataPoint([900, 50, 300], &apos;May&apos;)&quot;&gt; Add data point&lt;/button&gt; SchematicsThere are schematics that may be used to generate chart components using Angular CLI. The components are defined in package ng2-charts-schematics. Installation of Schematics Packagenpm instal --save-dev ng2-charts-schematics Example of Generating a Line Chart using Angular CLIng generate ng20chart0schematics:line my-line-chart This calls angular’s component schematics and then modifies the result, so all the options for the component schematic are also usable here. This schematics will also add the ChartsModule as an imported module in the main app module (or another module as specified in the –module command switch).","categories":[{"name":"Web","slug":"Web","permalink":"https://nayan.co/categories/Web/"}],"tags":[{"name":"Line Chart","slug":"Line-Chart","permalink":"https://nayan.co/tags/Line-Chart/"},{"name":"Javascript","slug":"Javascript","permalink":"https://nayan.co/tags/Javascript/"},{"name":"Data Visualization","slug":"Data-Visualization","permalink":"https://nayan.co/tags/Data-Visualization/"}],"author":"Abhishek Rana"},{"title":"Paytm Gateway Integration","slug":"paytm-gateway-integration","date":"2020-01-02T14:43:06.000Z","updated":"2020-08-26T05:49:07.484Z","comments":true,"path":"/Ruby-on-Rails/paytm-gateway-integration/","link":"","permalink":"https://nayan.co/Ruby-on-Rails/paytm-gateway-integration/","excerpt":"","text":"A complete guide on adding payments to your Android app with backend as RoR Steps :- Install SDK Add Static SMS Permission (for SMS autoread) Add Runtime SMS Permission (for SMS autoread) Add Proguard Rules Get Order Checksum from Server (our Server) Generate and send Checksum (from Our Server) Start Payment Transaction Send Payment Response to Server Confirm with Paytm Gateway about payment status Update Order Status Show Order Status on App&nbsp;&nbsp; Step 1: Install SDKAdd the following dependency to your app level build.gradle. 12345678dependencies &#123; ... // Paytm SDK implementation('com.paytm:pgplussdk:1.4.4') &#123; transitive = true &#125;&#125; Step 2: Add Static SMS Permission (for SMS autoread)Add the following permissions to your AndroidManifest.xml. 12&lt;uses-permission android:name=”android.permission.READ_SMS”/&gt;&lt;uses-permission android:name=”android.permission.RECEIVE_SMS”/&gt; Step 3: Add Runtime SMS Permission (for SMS autoread)We used Dexter library for handling runtime permissions. To install that add the following dependency to your app level build.gradle. 123456dependencies &#123; ... // Dexter runtime permissions implementation 'com.karumi:dexter:4.2.0'&#125; Add the following code before starting your transaction process. 12345678910111213141516171819202122232425Dexter.withActivity(this) .withPermissions( android.Manifest.permission.READ_SMS”, android.Manifest.permission.RECEIVE_SMS” ) .withListener(object : MultiplePermissionsListener &#123; override fun onPermissionsChecked(report: MultiplePermissionsReport?) &#123; report?.let &#123; if (it.areAllPermissionsGranted()) &#123; beginPaytmTransaction() &#125; else &#123; showMessage(\"Permission Denied\") &#125; &#125; &#125; override fun onPermissionRationaleShouldBeShown(permissions: MutableList&lt;PermissionRequest&gt;?, token: PermissionToken?) &#123; token?.continuePermissionRequest() &#125; &#125;) .withErrorListener &#123; showMessage(\"Error occurred! $it\") &#125; .onSameThread() .check() Step 4: Add Proguard RulesAdd the following rules to your proguard-rules.pro. 123-keepclassmembers class com.paytm.pgsdk.paytmWebView$PaytmJavaScriptInterface &#123; public *;&#125; Step 5: Get Order Checksum from Server (our Server)Make an API call to your server to get the order checksum 1234interface OurService &#123; @GET(\"/subscriptions/new\") suspend fun newSubscription(): NewSubscriptionResponse&#125; 123class UserRepository(private val ourService: OurService) &#123; suspend fun createNewSubscription(): NewSubscriptionResponse = ourService.newSubscription()&#125; 1open class ActivityState 1234567891011121314151617181920212223class ProfileViewModel(private val userRepository: UserRepository) : ViewModel() &#123; private val _paytmState: MutableLiveData&lt;ActivityState&gt; = MutableLiveData(InitialState) val paytmState: LiveData&lt;ActivityState&gt; = _paytmState ... fun createNewSubscription() &#123; viewModelScope.launch &#123; try &#123; _paytmState.value = ProgressState _paytmState.value = PaytmChecksumState(userRepository.createNewSubscription()) &#125; catch (e: HttpException) &#123; _paytmState.value = ErrorState(e) &#125; catch (e: IOException) &#123; _paytmState.value = ErrorState(e) &#125; &#125; &#125; object ProgressState : ActivityState() data class PaytmChecksumState(val checksumResponse: NewSubscriptionResponse) : ActivityState() data class ErrorState(val exception: Exception) : ActivityState()&#125; 123456789class ProfileFragment : Fragment() &#123; private val profileViewModel: ProfileViewModel by viewModel() ... private fun beginPaytmTransaction() &#123; profileViewModel.createNewSubscription() &#125;&#125; Step 6: Generate and send Checksum (from Our Server)To your project directory add a package named paytm. Add checksum_tool.rb and encryption_new_pg.rb to the paytm package from Paytm_App_Checksum_Kit_Ruby We will be creating order for a PaymentRequest and we follow model heavy approach for business logic. So we added a static method to generate checksum for our order in PaymentRequest model itself. 1234567891011121314151617181920212223242526272829303132333435class PaymentRequest &lt; ApplicationRecord ... def self.create_checksum(user, order_id) require './paytm/encryption_new_pg.rb' require './paytm/checksum_tool.rb' require 'uri' paytm_hash = Hash.new is_staging = 'true' == ENV['PAYTM_STAGING'] merchant_id = is_staging ? ENV['STAGING_PAYTM_MERCHANT_ID'] : ENV['PAYTM_MERCHANT_ID'] industry_type = is_staging ? ENV['STAGING_PAYTM_INDUSTRY_TYPE'] : ENV['PAYTM_INDUSTRY_TYPE'] paytm_website = is_staging ? ENV['STAGING_PAYTM_WEBSITE'] : ENV['PAYTM_WEBSITE'] paytm_callback = is_staging ? ENV['STAGING_PAYTM_CALLBACK'] : ENV['PAYTM_CALLBACK'] paytm_hash[\"REQUEST_TYPE\"] = 'DEFAULT' paytm_hash[\"MID\"] = merchant_id #Provided by Paytm paytm_hash[\"ORDER_ID\"] = order_id; #unique OrderId for every request\\ paytm_hash[\"CUST_ID\"] = user.id.to_s #unique customer identifier paytm_hash[\"INDUSTRY_TYPE_ID\"] = industry_type #Provided by Paytm paytm_hash[\"CHANNEL_ID\"] = 'WAP'; #Provided by Paytm paytm_hash[\"TXN_AMOUNT\"] = '1'; #transaction amount paytm_hash[\"WEBSITE\"] = paytm_website #Provided by Paytm paytm_hash[\"EMAIL\"] = user.email; #customer email id if user.phone_number.present? paytm_hash[\"MOBILE_NO\"] = user.phone_number; #customer 10 digit mobile no. end paytm_hash[\"CALLBACK_URL\"] = paytm_callback + \"#&#123;order_id&#125;\" checksum_hash = ChecksumTool.new.get_checksum_hash(paytm_hash).gsub(\"\\n\", '') paytm_hash[\"CHECKSUMHASH\"] = checksum_hash paytm_hash endend First create a subscription for current user, then create a payment request for that subscription and create checksum treating that payment request as your Order. 123456789101112class SubscriptionsController &lt; ApplicationController before_action :authenticate_user! def new user = current_user subscription = user.subscriptions.create! payment_request = subscription.payment_requests.create! checksum = PaymentRequest.create_checksum(user, payment_request.id) render json: &#123; paytm_params: checksum, is_staging: 'true' == ENV['PAYTM_STAGING']&#125;, status: :ok endend Step 7: Start Payment TransactionWith checksum response from server, initiate the paytm purchase. 123456789101112131415161718192021222324252627282930313233343536class ProfileFragment : Fragment() &#123; ... private fun initiatePaytmPurchase(checksumResponse: NewSubscriptionResponse) &#123; val order = PaytmOrder(checksumResponse.paytmParams) val service = if (checksumResponse.isStaging) PaytmPGService.getStagingService(null) else PaytmPGService.getProductionService() service.initialize(order, null) service.startPaymentTransaction(context, true, true, object : PaytmPaymentTransactionCallback &#123; override fun onTransactionResponse(inResponse: Bundle?) &#123; &#125; override fun clientAuthenticationFailed(inErrorMessage: String?) &#123; &#125; override fun someUIErrorOccurred(inErrorMessage: String?) &#123; &#125; override fun onTransactionCancel(inErrorMessage: String?, inResponse: Bundle?) &#123; &#125; override fun networkNotAvailable() &#123; &#125; override fun onErrorLoadingWebPage(iniErrorCode: Int, inErrorMessage: String?, inFailingUrl: String?) &#123; &#125; override fun onBackPressedCancelTransaction() &#123; &#125; &#125;) &#125;&#125; Step 8: Send Payment Response to ServerOn Transaction response from paytm payments Activity, send the response to your server to update payment status. 12345interface OurService &#123; @POST(\"/payment_requests/&#123;requestId&#125;/update_status\") suspend fun updatePaymentStatus(@Path(value = \"requestId\") requestId: Int, @Body transactionResponse: JsonObject): UpdatePaymentResponse&#125; 123class UserRepository(private val ourService: OurService) &#123; suspend fun updatePaymentResponse(requestId: Int, transactionResponse: JsonObject): UpdatePaymentResponse = ourService.updatePaymentStatus(requestId, transactionResponse)&#125; 1open class ActivityState 1234567891011121314151617181920212223242526class ProfileViewModel(private val userRepository: UserRepository) : ViewModel() &#123; private val _paytmState: MutableLiveData&lt;ActivityState&gt; = MutableLiveData(InitialState) val paytmState: LiveData&lt;ActivityState&gt; = _paytmState ... fun updatePaymentStatus(requestId: Int, transactionResponse: JsonObject) &#123; viewModelScope.launch &#123; try &#123; _paytmState.value = ProgressState val response = userRepository.updatePaymentResponse(requestId, transactionResponse) _paytmState.value = PaytmStatusState(response) _paytmState.value = PaytmIdleState &#125; catch (e: HttpException) &#123; _paytmState.value = ErrorState(e) &#125; catch (e: IOException) &#123; _paytmState.value = ErrorState(e) &#125; &#125; &#125; object ProgressState : ActivityState() data class PaytmStatusState(val updatePaymentResponse: UpdatePaymentResponse) : ActivityState() object PaytmIdleState : ActivityState() data class ErrorState(val exception: Exception) : ActivityState()&#125; 1234567891011121314151617181920class ProfileFragment : Fragment() &#123; ... override fun onTransactionResponse(inResponse: Bundle?) &#123; val orderId = inResponse?.getString(\"ORDERID\") orderId?.let &#123; val responseJson = JsonObject() inResponse.keySet()?.forEach &#123; responseJson.addProperty(it, inResponse.getString(it)) &#125; val responseJsonWrapper = JsonObject() responseJsonWrapper.add(\"gateway_response\", responseJson) profileViewModel.updatePaymentStatus(Integer.parseInt(orderId), responseJsonWrapper) &#125; &#125; ...&#125; Step 9: Confirm with Paytm Gateway about payment statusConfirm with Paytm gateway using Transaction Status API. 1234567891011121314151617181920212223class PaymentRequest &lt; ApplicationRecord ... def confirm_with_gateway(user) is_staging = 'true' == ENV['PAYTM_STAGING'] status_api_url = is_staging ? ENV['STAGING_PAYTM_STATUS'] : ENV['PAYTM_STATUS'] merchant_id = is_staging ? ENV['STAGING_PAYTM_MERCHANT_ID'] : ENV['PAYTM_MERCHANT_ID'] order_id = self.id response = HTTParty.post(status_api_url, body: &#123; MID: merchant_id, ORDERID: order_id, CHECKSUMHASH: PaymentRequest.create_checksum(user, order_id)[\"CHECKSUMHASH\"] &#125;.to_json, multipart: false, headers: &#123; 'Content-Type' =&gt; 'application/json' &#125;, timeout: 10000) update_status(response) # We will learn about this in next step endend After confirming with gateway, send back the response to App. 123456789101112131415161718192021222324252627class PaymentRequestsController &lt; ApplicationController before_action :authenticate_user! def update_status payment_request = PaymentRequest.find(params[:id]) payment_request.initiate! #We are using aasm gem for this https://github.com/aasm/aasm payment_request.confirm_with_gateway(current_user) payment_request.reload render json: &#123; message: get_status_message(payment_request), status: payment_request.aasm_state &#125;, status: :ok end def get_status_message(payment_request) case payment_request.aasm_state when :gateway_confirmation_pending 'Payment is still under process, please wait until the status of transaction is updated' when :success 'Payment was successful' else 'Payment has failed' end endend Step 10: Update Order StatusBased on response codes, update the payment status of order. Response codes and statuses can be found Transaction Status API’s Response codes and Messages section and Transaction response codes and messages 12345678910111213141516171819class PaymentRequest &lt; ApplicationRecord ... private def update_status(response) response = response.symbolize_keys response_code = response[:RESPCODE] if response_code == \"01\" self.update(transaction_reference: response[:TXNID], metadata: response) self.mark_as_succeed! elsif response_code == \"400\" || response_code == \"402\" elsif response_code == \"294\" self.mark_as_expired! else self.update(transaction_reference: response[:TXNID], error_message: response[:RESPMSG], metadata: response) self.mark_as_failed! end endend Step 11: Show Order Status on AppBased on server response, show messages on UI. 1234567891011121314151617181920212223class ProfileFragment : Fragment() &#123; private val paytmStateObserver: Observer&lt;ActivityState&gt; = Observer &#123; when (it) &#123; is ProfileViewModel.PaytmStatusState -&gt; &#123; when (it.updatePaymentResponse.status) &#123; \"success\" -&gt; &#123; Snackbar.make(progressBar, \"Payment success\", Snackbar.LENGTH_LONG).show() &#125; \"failed\" -&gt; &#123; Snackbar.make(progressBar, \"Payment failed. $&#123;it.updatePaymentResponse.message&#125;\", Snackbar.LENGTH_LONG).show() &#125; \"expired\" -&gt; &#123; Snackbar.make(progressBar, \"Payment expired. Please try again\", Snackbar.LENGTH_LONG).show() &#125; \"gateway_confirmation_pending\" -&gt; &#123; Snackbar.make(progressBar, \"Payment pending. $&#123;it.updatePaymentResponse.message&#125;\", Snackbar.LENGTH_LONG).show() &#125; else -&gt; &#123;&#125; &#125; &#125; &#125; &#125;&#125; References:- Add payments to your Android app with Paytm SDK Paytm_App_Checksum_Kit_Ruby Android Payment Gateway Integration Guide: PAYTM Transaction Status API Some good reads you may like:- Override Devise Auth Token Controllers Generating Pdf in Ruby on Rails using Prawn","categories":[{"name":"Android","slug":"Android","permalink":"https://nayan.co/categories/Android/"},{"name":"Ruby on Rails","slug":"Ruby-on-Rails","permalink":"https://nayan.co/categories/Ruby-on-Rails/"}],"tags":[{"name":"android","slug":"android","permalink":"https://nayan.co/tags/android/"},{"name":"Ashish Jajoria","slug":"Ashish-Jajoria","permalink":"https://nayan.co/tags/Ashish-Jajoria/"},{"name":"backend","slug":"backend","permalink":"https://nayan.co/tags/backend/"},{"name":"rails","slug":"rails","permalink":"https://nayan.co/tags/rails/"},{"name":"ror","slug":"ror","permalink":"https://nayan.co/tags/ror/"},{"name":"paytm","slug":"paytm","permalink":"https://nayan.co/tags/paytm/"},{"name":"gateway","slug":"gateway","permalink":"https://nayan.co/tags/gateway/"},{"name":"payment","slug":"payment","permalink":"https://nayan.co/tags/payment/"}],"author":"Ashish Jajoria"},{"title":"How to make a web application for any AI model","slug":"How-to-make-a-web-application-for-any-AI-model","date":"2019-12-03T11:15:16.000Z","updated":"2020-08-26T05:49:07.420Z","comments":true,"path":"/AI/How-to-make-a-web-application-for-any-AI-model/","link":"","permalink":"https://nayan.co/AI/How-to-make-a-web-application-for-any-AI-model/","excerpt":"","text":"Yes! you read the title right. So, today in this post I’ll show you how to setup a basic image-classifier in the form of a web application. Basic requirementsBefore we getting dive more into it, I am listing down the basic ingredients which are required to make a web application in python. Flask Flask Bootstrap Please note that, I am not showing about how to create an AI classifier model, so make sure you have your classifier already before seeking into this post, if not then you can download a pre-trained model. Let’s get started ! InstallationYou need to have installed the above mentioned libraries. You can easily install them by using pip. 12pip install flaskpip install flask-bootstrap Getting startedSo, firstly we arrange our files and folders in the below shown order. You can change the main folder name image-classifier (In my case) to any other name as you like. So, firstly we will write a basic flask app structure in __init__.py. This file can be found inside the classifier. Inside the __init__.py file write the below code 123456from flask import Flaskfrom flask_bootstrap import Bootstrapfrom classifier import routesapp = Flask(__name__) ## defining our flask applicationBootstrap(app) ## giving a nice bootstrap touch to our application Now, writing our run.py file. You can find this run.py file inside the main folder. Open the file and write the below code 123456from classifier import app as applicationapplication.config.from_pyfile(&apos;config/config.py&apos;) if __name__ == &quot;__main__&quot;: application.run(host=&apos;0.0.0.0&apos;, port=8000) ## This tells our ##application will run on this host and on this port. Now, we will create write a config.py file. This file contains the configuration for the application. The basic configuration we can put now is that, we can just put our application in DEBUG mode. So, open the config.py file and write the below line. 123456import osfrom os.path import join, dirname, realpathDEBUG = True## If True, then it refresh the server after making any changes to the code.UPLOAD_FOLDER = join(dirname(realpath(__file__)), &apos;uploaded_images/&apos;) Make a uploaded_images folder inside the config folder, this folder contains the image which will be uploaded on the server via user. Writing a basic route for our flask appWe have completely setup our basic flask environment. Now, its time to write a very basic flask api for hello world. Open the routes.py inside the classifier folder. Add the below lines to it 123456from classifier import appimport flask@app.route(&apos;/testing&apos;)def testing(): return &quot;&lt;h1&gt;Hello world&lt;/h1&gt;&quot; Here, the route() function of the Flask class is a decorator, which tells the application which URL should call the associated function. 1app.route(rule, options) The rule parameter represents URL binding with the function. The options is a list of parameters to be forwarded to the underlying Rule object. and in the end we just returned a simple message using some HTML tags. To run this code, follows below steps. 12cd path/to/main-folderpython run.py Just open the browser and type localhost:8000/testing. You should see a screen just like below Lets start our main routeLets make our template. For this, firstly makes a folder called template inside the classifier folder. Now inside the template, create a home.html file and paste the code from this link in it. Now, we make our home url and our backend part !!. So we will rewrite our routes.py file 123@app.route(&apos;/home&apos;)def home(): return render_template(&apos;home.html&apos;) ## The template which we created above Now, our image processing function will be like below 123456789101112131415from flask import render_templatefrom werkzeug import secure_filenamefrom classifier.backend.prediction import image_prediction@app.route(&quot;/fetchingImage&quot;, methods = [&apos;POST&apos;])def fetchingImage(): if flask.request.method == &apos;POST&apos; image = flask.request.files[&apos;image&apos;] image.save(app.config[&apos;UPLOAD_FOLDER&apos;] + secure_filename(image.filename)) full_img = app.config[&apos;UPLOAD_FOLDER&apos;] + image.filename data = image_prediction(full_img) if len(data)==2: return render_template(&apos;prediction.html&apos;, results = data) else: return render_template(&apos;error.html&apos;, results = data) Here, firstly we get the image from the form in the home.html, then save the image into the system and then fetch that file and send it to another function image_prediction for processing. Then, we simply render the response from the model to the webpage. If there is no error, then we display the prediction.html template or else, we render the error.html. Now, working on our image_prediction function. For making the this, firstly create a folder named backend inside the classifier folder, then inside the backend folder, create a prediction.py file and write the below code into it. 1234567891011121314151617181920212223import kerasimport numpy as npimport tensorflow as tfimport cv2from keras.models import load_modelfrom keras.applications.vgg19 import VGG19from keras.applications.vgg19 import decode_predictionsdef image_prediction(image): MODEL = VGG19() try: image = cv2.imread(image) image = cv2.resize(image, (224, 224)) image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2])) yhat = MODEL.predict(image) label = decode_predictions(yhat) label = label[0][0] label, conf = label[1], label[2]*100 results = [label, conf] except Exception as e: results = &quot;Please check the image.&quot; return results Now, let’s make our error.html and prediction.html templates. These templates are also, saved inside the templates folder. So, you can get the code for both the templates from here. Well the coding part is mostly done, now we will test our web application. Now open your console and run the below command 1python run.py After this, it will firstly download the VGG19 pretrained weights (if you are following my code.), then it will start the server. Now, open the browser and go to localhost:8000/home, you will see something like this Now, click on browse to upload any image and click the predict button. You’ll see some message like this (based on your image) If there is any some issue with your image, then below screen will appear ConclusionSo, this is how we can make a very basic web application for our image classifier. You can also find the whole code from my github. References https://www.tutorialspoint.com/flask/index.htm https://www.w3schools.com/bootstrap/bootstrap_templates.asp https://pythonprogramming.net/flask-send-file-tutorial/","categories":[{"name":"AI","slug":"AI","permalink":"https://nayan.co/categories/AI/"}],"tags":[{"name":"Image-Classification","slug":"Image-Classification","permalink":"https://nayan.co/tags/Image-Classification/"},{"name":"Keras","slug":"Keras","permalink":"https://nayan.co/tags/Keras/"}],"author":"Himanshu Garg"},{"title":"Android Testing Strategy","slug":"Android-Testing-Strategy","date":"2019-11-29T17:07:05.000Z","updated":"2020-08-26T05:49:07.408Z","comments":true,"path":"/Android/Android-Testing-Strategy/","link":"","permalink":"https://nayan.co/Android/Android-Testing-Strategy/","excerpt":"","text":"Testing android application is quite hard. There is no set guidelines for us to follow. When ever I started thinking of testing my application I always get confused where to start, should I write unit tests or instrumentation tests and should I start with integration and End to end tests. There is also a lot of confusion on frameworks available for android testing.This article has been written in a sense to address this confusion and show us as developers what kind of testing is most preferred and what frameworks are available. Kinds of TestsUnit TestThis is often referred to as local tests and doesn’t require a device or emulator for running them. These can be classified broadly into categories Local/ Pure Unit tests - tests which can run on JVM, mainly written for testing business logic. On android JUnit, Mockito, the mockable Android JARs give us a nice combination of tools to run fast unit-style tests in the local JVM. Android Unit Tests - tests which requires the Android system (ART) and for this we need to replace android dependencies using RoboelectricGuidelines If you have dependencies on the Android framework, particularly those that create complex interactions with the framework, it’s better to include framework dependencies using Robolectric. If your tests have minimal dependencies on the Android framework, or if the tests depend only on your own objects, it’s fine to include mock dependencies using a mocking framework like Mockito and PowerMock. Reference Url : https://developer.android.com/training/testing/unit-testing/local-unit-tests Instrumentation TestsThese tests requires device or an emulator for running. This is mostly used for UI testing but it can be used to test none UI logic as well. This is useful when you need to test code that has a dependency on context. UI tests can be an essential component of any testing strategy since they can uncover issues related to UI, hardware, firmware, and backwards compatibility Reference Url: https://developer.android.com/training/testing/unit-testing/instrumented-unit-tests Testing FrameworksAndroid X Test Framework It is testing framework and APIs provided by android team for writing unit tests. RoboelectricIt is a framework that brings fast and reliable unit tests to android. Runs inside JVM or your workstation in seconds. This is usually used to Integration testing. Integration tests validate how your code interacts with other parts of the system but without the added complexity of a UI framework. app/src/test/java - for any unit test which can run on the JVM Question : How does it work?Answer : Unlike traditional emulators-based androids tests, it tests run inside a sandbox which allows the android environment to be precisely configured to the desired conditions for each test. It lets you run your tests on your workstation, or on your continuous integration environment in a regular JVM, without an emulator. It handles inflation of views, resource loading, and lots of other stuff that’s implemented in native C code on Android devices. This allows tests to do most things you could do on a real device. It’s easy to provide our own implementation for specific SDK methods too, so you could simulate error conditions or real-world sensor behaviour. It allows a test style that is closer to black box testing, making the tests more effective for refactoring and allowing the tests to focus on the behaviour of the application instead of the implementation of Android Question : Why should be prefer this?Answer : In order for this to run tests it needs regular JVM, Because of this, the dexing, packaging, and installing-on-the emulator steps aren’t necessary, reducing test cycles from minutes to seconds so you can iterate quickly and refactor your code with confidence. Robolectric executes your code against real (not mock) Android JARs in the local JVM. Espresso Use it to write concise, beautiful, and reliable Android UI tests. These tests are called Instrumentation tests and unlike unit tests takes more time to run them. app/src/androidTest/java - for any instrumentation test which should run on an Android Question : How does it work?Answer : it requires an emulator or a real device to run tests. At the time of execution along with the main application, A testing application is also installed in the device which controls main application automatically. UI AutomatorIt allows us to write cross application functional tests ( End to End) . Example, Sharing messages via Text intent or sending email via locally installed email clients. Monkey Runner CL Monkey is a command line tool which sends pseudo random events to your device. You can restrict Monkey to run only for a certain package and therefore instruct Monkey to test only your application. it can be used for Stress testing for android. Reference Url : https://developer.android.com/studio/test/monkey Recommandations Creating test groups - @SmallTest. @MediumTest and @LargeTest annotation allows us to classify tests. Allows you to run, for example, only short running tests for development cycle. You may run your long running tests on a continuous integration server.This can be easily configured this via InstrumentationTestRunner in user build.gradle (app) We can use a three tiered approach Pure Unit tests : These can be written for our business logic which are completely android independent of API and can run on JVM. These can be written using Junit Framework. Roboelectric Unit tests: where code has only small dependencies on android APIs and can be easily mocked with Roborelectric. Android Instrumentation tests : where code heavily interact with device hardware, sensors and android APIs. These tests will usually take most time to run.","categories":[{"name":"Android","slug":"Android","permalink":"https://nayan.co/categories/Android/"}],"tags":[{"name":"Instrumentation Test","slug":"Instrumentation-Test","permalink":"https://nayan.co/tags/Instrumentation-Test/"},{"name":"Unit Test","slug":"Unit-Test","permalink":"https://nayan.co/tags/Unit-Test/"},{"name":"Android Testing","slug":"Android-Testing","permalink":"https://nayan.co/tags/Android-Testing/"}],"author":"Puneet Kashyap"},{"title":"Tracking Deep Learning experiments using Keras,MlFlow and MongoDB","slug":"Tracking-Deep-Learning-Experiments-using-Keras-MlFlow-and-MongoDb","date":"2019-11-29T12:06:56.000Z","updated":"2020-08-26T05:49:07.424Z","comments":true,"path":"/AI/Tracking-Deep-Learning-Experiments-using-Keras-MlFlow-and-MongoDb/","link":"","permalink":"https://nayan.co/AI/Tracking-Deep-Learning-Experiments-using-Keras-MlFlow-and-MongoDb/","excerpt":"","text":"It is late 2019 and Deep Learning is not a buzzword anymore. It is significantly used in the technology industry to attain feats of wonders which traditional machine learning and logic based techniques would take a longer time to achieve. The main ingredient in Deep Learning are Neural Networks, which are computation units called neurons, connected in a specific fashion to perform the task of learning and understanding data. When these networks become extremely deep and sophisticated, they are referred to as Deep Neural Networks and thus Deep Learning is performed. Neural Networks are so called because they are speculated to be imitating the human brain in some manner. Though it is not entirely true, but the learning mechanism is mostly similar in nature. A human brain learns about an object or concept when it visually experiences it for a longer amount of time. Similar to that, a neural network learns about objects and what they actually represent when it is fed with a large amount of data. For example, let us consider the LeNet architecture . It is a small two layered CNN (Convolution Neural Network). Convolution Neural Networks are a special kind of neural network where the mathematical computation being done at every layer are convolution operations. If enough images of a certain kind are fed to the LeNet architecture, it starts to understand and classify those images better. That was a simple introduction to what neural networks are and how they behave. In this article we will be mostly looking into three main frameworks which can ease out the developer experience of building these neural networks and tracking there performance efficiently. Nowadays, neural networks are heavily used for classifying objects, predicting data and other similar tasks by many companies out there. When it comes it to training neural networks and keeping track of their performance, the experience is not too subtle. When building a neural network, a developer would be trying out multiple datasets and experimenting with different hyperparameters. It is essential to keep a track a of these parameters and how they affect the output of the neural networks. Also debugging neural networks is an extremely cumbersome task. The output performance of different neural networks may vary due to different reasons. Some of the possible causes maybe inadequate data pre-processing, incorrect optimizer, a learning rate which is too low or too high. The number of variables which affect the performance of a neural network are quite a few. Hence it is essential that every parameter is properly tracked and maintained. Some of the available options present out there include the infamous *Tensorboard, Bokeh *to name a few. In this project we will be using MlFlow ,an open source platform to manage the entire deep learning development cycle. MlFlow allows developers to log and track the outputs of every experiment performed with great precision. We will be looking into MlFlow’s components with more detail in the subsequent sections. The framework we would be using for writing our neural networks and training them is Keras. We will be using the FashionMNIST dataset. It contains a total of 70000 gray scale images (training:test = 60000:10000) , each scaled at 28x28 associated with one from 10 classes. (Fig 1) The folder structure of our project looks as shown in Fig 2 below. The data folder contains our fashion mnist dataset files which will be used for training the model. The db folder contains the python driver code to perform operations on MongoDb collections. MongoDB is an extremely easy to use NoSql database which has been built keeping in developer satisfaction. It is easily integrable with modern day applications and has a large developer community contributing to it’s extensions regularly. The model folder contains piece of code with the neural network model definition. The mlruns folder is created automatically once *mlflow *is invoked in the main code. The aim of the project is to track multiple deep learning experiments and check how the outputs are affected when parameters are changed and data is changed. Since we have only one dataset, we will split it into equal parts in order to simulate a multiple dataset scenario. Let’s start off with the create_dataset script, which is used to split the fashion mnist into equal parts and store them inside the data folder with proper serial number. In Fig 3 shown below, we import fashion mnist from *keras.datasets *and perform the necessary normalization step Next, we write the methods to split the dataset into equal parts and save them with proper incremental serial numbers inside the data folder, inside the root project directory (Fig 4) equal parts for training* After this we go ahead and write the necessary driver code to manage our newly created dataset using MongoDb. Some might say that using MongoDb for such a small project might be an overkill, but personally I find MongoDb to be an excellent tool for managing data with flexibility. Given NoSql’s schema-less nature, managing collections and documents is a breeze. The ease with which any document can be edited in MongoDB is superb. The best part is, whenever a collection is queried , the result returned is a *json *making it extremely easy to be parsed using any programming language. Aggregation queries in mongo are also very simple and allows users to cross reference collections in a swift manner. In order to connect our python scripts with MongoDb we will be using pymongo which can be easily installed using the pip install pymongo. To install MongoDb, follow this tutorial ***https://docs.mongodb.com/manual/tutorial/install-mongodb-on-ubuntu/*** Once MongoDb is installed and tested to be running properly on your system, create a new database called fashion_mnist. Inside the database create a new collection named dataset as shown in Fig 5 below. A great GUI to interact with MongoDb is robo3t. It’s free and easy to use. It can be downloaded from the following link ***https://robomongo.org/download.***Since our DB is setup and datasets are created, we can progress with the task of inserting necessary information into the dataset collection In Fig 6 shown above, we are importing MongoClient from the pymongo library which will essentially be used to connect to our mongoDB database. Fig 7 below describes mongoQueue class which has been written in order to interact with our dataset collection. In line 18 and 19, the collection name is initialized, which is used in all the member functions. The Enqueue* method in line 6 is used to insert dataset information into the dataset collection. The *Dequeue method in line 10 fetches the first dataset which has a status field of ‘Not** *Processed’.* The setAsProcessing* and setAsProcessed *methods are used to set the status field of respective dataset documents in the collection. Fig 8: Methods to insert data into MongoDB We use the insert_into_db method shown in Fig 8, line 1, to insert information about our newly created datasets into our mongoDb dataset collection. In line 23 of the main function, we iterate over the dataset folder and call *insert_into_db *to insert the necessary information for that dataset into the collection. Once every dataset is successfully inserted into the collection, the fields appear as shown in Fig 9 below. We can now define our model for training our deep learning network. Inside *model/model.py we import all necessary **keras*** packages to build our CNN network (shown in Fig 10a) Fig 10b shows the model architecture. It is a simple two layer CNN, with two MaxPool layers and RelU activation in between. Two Dense layers are also added with 32 and 10 neurons respectively. I have also added a Dropout of 0.5 before the last Dense layer. Now, in our train.py script we import all the necessary modules needed from the keras library to get on with our training. Along with all the keras libraries we import *mlflow *as well (Fig 11) All the hyperparameters which will be used for training is stored in a config file named as train_config.json. This file is read (Fig 12a) and used for defining training parameters In Fig 12b, we have defined out training function , which takes arguments *trainX (our training set) ,*trainY** (*training set labels) and the **model** *(CNN model) From line 38 (In Fig 13), the main function starts where we define our *MongoQueue object using the **MongoQueue** class defined inside script mentioned before, **db_ops.py . *As it can be seen in line 41, *mq*** is our object. In line 42 (Fig 13) , a new CNN model is created by calling the *model function which accepts the optimizer type as input. In this experiment we would be using the ‘*SGD’ (Stochastic Gradient Descent**) to train the network . Everytime we invoke mlflow in our training code for logging, it is known as an mlflow run. MlFlow provides us with an API for starting and managing MlFlow runs. For example, Fig 14a and 14b In our code we start the mlflow run using the python context manager as shown in Fig 14b. At line 46 in Fig 13, we define our our mlflow run with the run name as ‘fashion mnist’.* All data and metrics will be logged under this run name on the **mlflow*** dashboard. From line 47 we start a while loop, which continuously invokes the *dequeue function from the *MongoQeueue** class. What this does is fetches every row corresponding to a particular dataset from the dataset collection which has a status field = *Not Processed (Fig 9). As soon as this dataset is fetched, *setAsProcessing* function is called in line 51 which sets the status of that dataset to *Processing *in *MongoDb. This enables us to understand which dataset is currently being trained by our system. This is particularly helpful is large systems where there are multiple datasets and many training instances running in parallel. In lines 54 and 55, the datasets are loaded from the *data folder corresponding to the **dataset_id** *fetched from the db. Lines 57 and 58 loads the test sets and the training is started at line 59 by calling the *train *function. We then use the trained model to predict our scores as shown in line 60 in Fig 15. The training output looks as shown below (Fig 16a and Fig 16b) As shown in Fig 16b, the training happens for an epoch and the evaluation metrics for the test dataset gets logged. All outputs of the evaluation done using our trained model can be logged using the MlFlow tracking api (as shown in Fig 17). The tracking API comes with functions such as log_param and log_metric which enables us to log every hyperparameter and output values into mlflow. The best feature about mlflow is the dashboard it provides. It has a very intuitive UI and can be used efficiently for tracking our experiments. The dashboard can be started easily by simply hitting *mlflow ui *in your terminal as shown in Fig 18 below. To access the dashboard, just type ***http://localhost:5000** *in your browser and hit enter. Fig 19 shows how the dashboard looks like. Each MlFlow run is logged using a run ID and a run name. The Parameters and the Metrics column log display the parameters and the metrics which were logged while we were training our model Further clicking on a particular run, takes us to another page where we can display all information about our run (Fig 20) MlFlow provides us with this amazing feature to generate plots for our results. As you can see in Fig 21a, the test accuracy change can be visualized across different training datasets and time. We can also choose to display other metrics such as the eval loss, as shown in Fig 21b. The smoothness of the curve can also be controlled using the slider. We can also log important files or scripts in our project to MlFlow using the mlflow.log_artifact command . Fig 22a shows how to use it in your training script and Fig 22b shows how it is displayed on the mlflow dashboard. MlFlow also allows users to compare two runs simultaneously and generate plots for it. Just tick the check-boxes against the runs you want to compare and press on the blue Compare button (Fig 23) Once you click on compare, another page pops up where all metrics and parameters of two different runs can be viewed and studied in parallel (Fig 24a) The user can also choose to display metrics such as accuracy and loss in parallel charts as shown in Fig 24b. Users can add an MlFlow Project file (a text file in YAML syntax) to their MlFlow project allowing them to package their code better and run and recreate results from any machine. The MlFlow Project file for our fashion_mnsit project looks as shown below in Fig 25a We can also specify a conda environment for our MlFlow project and specify a conda.yaml file (Fig 25b). Hence, with this we conclude our project. Developers face several on-demand requirements for monitoring metrics during training a neural network. These metrics can be extremely critical for predicting the output of their neural networks and are also critical in understanding how to modify a neural network for better performance. Traditionally when starting off with deep learning experiments, many developers are unaware of proper tools to help them. I hope this was piece of writing was helpful in understanding how deep learning experiments can be conducted in a proper manner during production when large numbers of datasets are needed to be managed and multiple training and evaluation instances are required to be monitored. MlFlow also has multiple other features which is beyond the scope of this tutorial and can be covered later. For any information on how to use MlFlow one can head to the ***https://mlflow.org/***","categories":[{"name":"AI","slug":"AI","permalink":"https://nayan.co/categories/AI/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://nayan.co/tags/Machine-Learning/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"https://nayan.co/tags/Deep-Learning/"},{"name":"Keras","slug":"Keras","permalink":"https://nayan.co/tags/Keras/"},{"name":"Mlflow","slug":"Mlflow","permalink":"https://nayan.co/tags/Mlflow/"},{"name":"MongoDB","slug":"MongoDB","permalink":"https://nayan.co/tags/MongoDB/"},{"name":"Tracking","slug":"Tracking","permalink":"https://nayan.co/tags/Tracking/"}],"author":"Abhishek Bose"},{"title":"Sharing modules across Android apps","slug":"sharing-modules-across-android-apps","date":"2019-11-27T13:00:00.000Z","updated":"2020-08-26T05:49:07.516Z","comments":true,"path":"/Android/sharing-modules-across-android-apps/","link":"","permalink":"https://nayan.co/Android/sharing-modules-across-android-apps/","excerpt":"","text":"Sharing modules across Android appsWhile most android apps are created with a single default app module, in the last few years people have startedmoving to a multi module structure for their Android apps. Having multiple smaller modules have a few distinctadvantages Build times are noticeably faster Your code is decoupled with clear dependencies Better distribution of ownership across different parts of the app Allows modules to be reused across apps Example modulesA possible strategy is to have one module per feature. app: This is the main module which will be the entry point into your app. It acts mainly as a coordinator betweenother modules core: This will contain the model definitions that are core to your app and will be required across modules networking: This provides the networking code for the other modules login: Login/Signup logic goes here dashboard: User dashboard will be here There are many posts on the advantages and strategies for modularizing your Android apps. For this post, we willfocus on the strategy to reuse modules across apps. Reusing modules across appsWe have multiple apps in our company that share the core and login logic. So we decided to share these modules amongthe two applications. One obvious way to share Android Library modules would be to share the generated .aar files and add them asdependencies to the different apps. While this is simpler, the main applications and the library modules will bedifferent Android Studio projects. If any change needs to be done in the library, the .aar will have to be regeneratedand manually updated. There has to be a better way. The solution we decided to use for sharing modules is git submodules. Though it had a small overhead in bringingthe entire team up to speed with submodules, it has worked exceptionally well for us. In the above example, we have two git submodules, core and login. And the submodules will be added as dependencies just as any Android module, Creating a new submodulesTo create a new submodule, we follow the following process Create an empty repo on Github and initialize with a README Add a new submodule to our app git submodule add git@github.com:username/core.git Create a new Android Library module in the new directory Commit the changes in the library module and push Commit the changes in the main repo and push Next time we need to use this submodule in another app, we only need to Add a new submodule to our app git submodule add git@github.com:username/core.git Add the newly added module to settings.gradle and a dependency in build.gradle Committing changes to a submoduleEvery time we make some changes to a submodule, we just need to make sure that we commit and push those changesbefore committing the changes in the main repo. Submodule: If you are on a detached head, git checkout -b new-branch git add . &amp;&amp; git commit -am &quot;commit message&quot; git push origin new-branch Main repo: git add . &amp;&amp; git commit -am &quot;commit message&quot; git push Fetching remote changesEvery time we do a git pull, we just need to remember to update the submodules as well git submodule update and that’s it. We have the latest version of the submodule locally! Hope this works for you. Happy coding!","categories":[{"name":"Android","slug":"Android","permalink":"https://nayan.co/categories/Android/"}],"tags":[{"name":"android","slug":"android","permalink":"https://nayan.co/tags/android/"},{"name":"git submodules","slug":"git-submodules","permalink":"https://nayan.co/tags/git-submodules/"}],"author":"Anuj Middha"},{"title":"Text detection in number plates","slug":"Text-detection-in-number-plates","date":"2019-11-26T15:21:29.000Z","updated":"2020-08-26T05:49:07.424Z","comments":true,"path":"/AI/Text-detection-in-number-plates/","link":"","permalink":"https://nayan.co/AI/Text-detection-in-number-plates/","excerpt":"","text":"Text detection in number platesOne of the vital modules in the optical character recognition(OCR) pipeline is text detectionand segmentation which is also called text localization. In this post, we will apply variedpreprocessing techniques to the input image and find out how to localize text in theenhanced image, so that we can feed the segments to our text recognition network. Image PreprocessingSometimes images can be distorted, noisy and other problems that can scale back the OCRaccuracy. To make a better OCR pipeline, we need to do some image preprocessing. Grayscale the image: Generally you will get an image which is having 3channels(color images), we need to convert this image into a grayscale form whichcontains only one channel. We can also process images with three channels but itonly increases the complexity of the model and increases the processing time.OpenCV provides a built-in function that can do it for you. 12import cv2grayscale_image = cv2.cvtColor(image, cv2.COLOR_BRG2GRAY) Or you can convert the image to grayscale while reading the image. 12#opencv reads image in BGR formatgraysacle_image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE) Noise reduction: Images come with various types of noises. OpenCV provides a lot ofnoise reduction function. I am using the Non-local Means Denoising algorithm. 1denoised_image = cv2.fastNlMeansDenoising(grayscale_img, None, 10, 7, 21) Contrast adjustment: Sometimes we have low contrast images. This makes it difficultto separate text from the image background. We need high contrast text images forthe localization process. We can increase image contrast using Contrast LimitedAdaptive Histogram Equalization (CLAHE) among many other contrast enhancementmethods provided by skimage. 12from skimage import exposurecontrast_enhanced_image = exposure.equalize_adapthist(denoised, clip_limit=0.03) So now we are done with image preprocessing let us move on to the second part, textlocalization. Text LocalizationIn this part, we will see how to detect a large number of text region candidates andprogressively removes those less likely to contain text. Using the MSER feature descriptor tofind text candidates in the image. It works well for text because the consistent color and highcontrast of text lead to stable intensity profiles. 123#constructor for MSER detectormser = cv2.MSER_create()regions, mser_bboxes = mser.detectRegions(contrast_enhance_image) Along with the text MSER picked up many other stable regions that are not text. Now, thegeometric properties of text can be used to filter out non-text regions using simplethresholds. Before moving on with the filtering process, let’s write some functions to display the results ina comprehensible manner. 12345678910111213141516171819202122232425262728import numpy as npimport matplotlib.pyplot as plt#display imagesdef pltShow(*images): #count number of images to show count = len(images) #three images per columnn Row = np.ceil(count / 3.) for i in range(count): plt.subplot(nRow, 3, i+1) if len(images[i][0], cmap=’gray’) plt.imshow(images[i][0], cmap=’gray’) else: plt.imshow(images[i][0]) #remove x-y axis from subplots plt.xticks([]) plt.yticks([]) plt.title(images[i][1]) plt.show()#color each MSER region in imagedef colorRegion(image_like_arr, region): image_like_arr[region[:, 1], region[:, 0], 0] = np.random.randint(low=100, high=256) image_like_arr[region[:, 1], region[:, 0], 1] = np.random.randint(low=100, high=256) image_like_arr[region[:, 1], region[:, 0], 2] = np.random.randint(low=100, high=256) return image The geometric properties we are going to use to discriminate between text and non-textregion are: Region area Region perimeter Aspect ratio Occupancy Compactness We will apply simple thresholds over these parameters to eliminate non-text regions. Firstlet’s write method to compute these parameters. 12345678910111213141516171819202122232425262728293031#values for the parametersAREA_LIM = 2.0e-4PERIMETER_LIM = 1e-4ASPECT_RATIO_LIM = 5.0OCCUPATION_LIM = (0.23, 0.90)COMPACTNESS_LIM = (3e-3, 1e-1)def getRegionShape(self, region): return (max(region[:, 1]) - min(region[:, 1]), max(region[:, 0]) - min(region[:, 0])) #compute areadef getRegionArea(region): return len(list(region))#compute perimeterdef getRegionPerimeter(image, region): #get top-left coordinate, width and height of the box enclosing the region x, y, w, h = cv2.boundingRect(region) return len(np.where(image[y:y+h, x:x+w] != 0)[0])) #compute aspect ratiodef getAspectRatio(region): return (1.0 * max(getRegionShape(region))) / (min(getRegionShape(region)) + 1e-4)#compute area occupied by the region area in the shapedef getOccupyRate(region): return (1.0 * getRegionArea(region)) / (getRegionShape(region)[0] * \\getRegionShape(region)[1] + 1.0e-10) #compute compactness of the regiondef getCompactness(region): return (1.0 * getRegionArea(region)) / (1.0 * getRegionPerimeter(region) ** 2) Now apply these methods to filter out text regions as follows: 123456789101112131415161718192021222324252627282930#total number of MSER regionsn1 = len(regions)bboxes=[]for i, region in enumerate(regions): self.colorRegion(res, region) if self.getRegionArea(region) &gt; self.grayImg.shape[0] * self.grayImg.shape[1] * AREA_LIM: #number of regions meeting the area criteria n2 += 1 self.colorRegion(res2, region) if self.getRegionPerimeter(region) &gt; 2 * (self.grayImg.shape[0] + \\ self.grayImg.shape[1]) * PERIMETER_LIM: #number of regions meeting the perimeter criteria n3 += 1 self.colorRegion(res3, region) if self.getAspectRatio(region) &lt; ASPECT_RATIO_LIM: #number of regions meeting the aspect ratio criteria n4 += 1 self.colorRegion(res4, region) if (self.getOccupyRate(region) &gt; OCCUPATION_LIM[0]) and \\ (self.getOccupyRate(region) &lt; OCCUPATION_LIM[1]): n5 += 1 self.colorRegion(res5, region) if (self.getCompactness(region) &gt; \\COMPACTNESS_LIM[0]) and \\(self.getCompactness(region) &lt; \\COMPACTNESS_LIM[1]): #final number of regions left n6 += 1 self.colorRegion(res6, region) bboxes.append(mser_bboxes[i]) After eliminating non-text regions, I draw bounding boxes on the remaining regions andvoila, we have successfully detected and segmented the characters on the number plate.Note: Apply NMS to remove overlapping bounding boxes. 12for bbox in bboxes: cv2.rectangle(img,(bbox[0]-1,bbox[1]-1),(bbox[0]+bbox[2]+1,box[1]+bbox[3]+1),(255,0,0), 1) Enough coding. Let’s see some results. 1234567891011pltShow(&quot;MSER Result Analysis&quot;, \\ (self.img, &quot;Image&quot;), \\ (self.cannyImg, &quot;Canny&quot;), \\ (res, &quot;MSER,(&#123;&#125; regions)&quot;.format(n1)), \\ (res2, &quot;Area=&#123;&#125;,(&#123;&#125; regions)&quot;.format(config.mser_areaLimit, n2)), \\ (res3, &quot;Perimeter=&#123;&#125;,(&#123;&#125; regions)&quot;.format(config.mser_perimeterLimit, n3)), \\ (res4, &quot;Aspect Ratio=&#123;&#125;,(&#123;&#125; regions)&quot;.format(config.mser_aspectRatioLimit, n4)), \\ (res5, &quot;Occupation=&#123;&#125;,(&#123;&#125; regions)&quot;.format(config.mser_occupationLimit, n5)), \\ (res6, &quot;Compactness=&#123;&#125;,(&#123;&#125; regions)&quot;.format(config.mser_compactnessLimit, n6)), \\ (boxRes, &quot;Segmented Image&quot;) \\ ) ConclusionIn this post, we covered the various image preprocessing techniques and learned about howto perform text localization on number plates.","categories":[{"name":"AI","slug":"AI","permalink":"https://nayan.co/categories/AI/"}],"tags":[{"name":"Deep Learning","slug":"Deep-Learning","permalink":"https://nayan.co/tags/Deep-Learning/"}],"author":"Akshay Bajpai"}]}