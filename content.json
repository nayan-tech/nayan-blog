{"meta":{"title":"Nayan Blog","subtitle":"","description":"","author":"NayanTech","url":"https://nayan.co/blog","root":"/blog/"},"pages":[],"posts":[{"title":"Text detection in number plates","slug":"Text-detection-in-number-plates","date":"2019-11-26T15:21:29.000Z","updated":"2019-11-26T11:14:16.256Z","comments":true,"path":"/2019/11/26/Text-detection-in-number-plates/","link":"","permalink":"https://nayan.co/blog/2019/11/26/Text-detection-in-number-plates/","excerpt":"","text":"Text detection in number platesOne of the vital modules in the optical character recognition(OCR) pipeline is text detectionand segmentation which is also called text localization. In this post, we will apply variedpreprocessing techniques to the input image and find out how to localize text in theenhanced image, so that we can feed the segments to our text recognition network. Image PreprocessingSometimes images can be distorted, noisy and other problems that can scale back the OCRaccuracy. To make a better OCR pipeline, we need to do some image preprocessing. Grayscale the image: Generally you will get an image which is having 3channels(color images), we need to convert this image into a grayscale form whichcontains only one channel. We can also process images with three channels but itonly increases the complexity of the model and increases the processing time.OpenCV provides a built-in function that can do it for you. 12import cv2grayscale_image = cv2.cvtColor(image, cv2.COLOR_BRG2GRAY) Or you can convert the image to grayscale while reading the image. 12#opencv reads image in BGR formatgraysacle_image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE) Noise reduction: Images come with various types of noises. OpenCV provides a lot ofnoise reduction function. I am using the Non-local Means Denoising algorithm. 1denoised_image = cv2.fastNlMeansDenoising(grayscale_img, None, 10, 7, 21) Contrast adjustment: Sometimes we have low contrast images. This makes it difficultto separate text from the image background. We need high contrast text images forthe localization process. We can increase image contrast using Contrast LimitedAdaptive Histogram Equalization (CLAHE) among many other contrast enhancementmethods provided by skimage. 12from skimage import exposurecontrast_enhanced_image = exposure.equalize_adapthist(denoised, clip_limit=0.03) So now we are done with image preprocessing let us move on to the second part, textlocalization. Text LocalizationIn this part, we will see how to detect a large number of text region candidates andprogressively removes those less likely to contain text. Using the MSER feature descriptor tofind text candidates in the image. It works well for text because the consistent color and highcontrast of text lead to stable intensity profiles. 123#constructor for MSER detectormser = cv2.MSER_create()regions, mser_bboxes = mser.detectRegions(contrast_enhance_image) Along with the text MSER picked up many other stable regions that are not text. Now, thegeometric properties of text can be used to filter out non-text regions using simplethresholds. Before moving on with the filtering process, let’s write some functions to display the results ina comprehensible manner. 12345678910111213141516171819202122232425262728import numpy as npimport matplotlib.pyplot as plt#display imagesdef pltShow(*images): #count number of images to show count = len(images) #three images per columnn Row = np.ceil(count / 3.) for i in range(count): plt.subplot(nRow, 3, i+1) if len(images[i][0], cmap=’gray’) plt.imshow(images[i][0], cmap=’gray’) else: plt.imshow(images[i][0]) #remove x-y axis from subplots plt.xticks([]) plt.yticks([]) plt.title(images[i][1]) plt.show()#color each MSER region in imagedef colorRegion(image_like_arr, region): image_like_arr[region[:, 1], region[:, 0], 0] = np.random.randint(low=100, high=256) image_like_arr[region[:, 1], region[:, 0], 1] = np.random.randint(low=100, high=256) image_like_arr[region[:, 1], region[:, 0], 2] = np.random.randint(low=100, high=256) return image The geometric properties we are going to use to discriminate between text and non-textregion are: Region area Region perimeter Aspect ratio Occupancy Compactness We will apply simple thresholds over these parameters to eliminate non-text regions. Firstlet’s write method to compute these parameters. 12345678910111213141516171819202122232425262728293031#values for the parametersAREA_LIM = 2.0e-4PERIMETER_LIM = 1e-4ASPECT_RATIO_LIM = 5.0OCCUPATION_LIM = (0.23, 0.90)COMPACTNESS_LIM = (3e-3, 1e-1)def getRegionShape(self, region): return (max(region[:, 1]) - min(region[:, 1]), max(region[:, 0]) - min(region[:, 0])) #compute areadef getRegionArea(region): return len(list(region))#compute perimeterdef getRegionPerimeter(image, region): #get top-left coordinate, width and height of the box enclosing the region x, y, w, h = cv2.boundingRect(region) return len(np.where(image[y:y+h, x:x+w] != 0)[0])) #compute aspect ratiodef getAspectRatio(region): return (1.0 * max(getRegionShape(region))) / (min(getRegionShape(region)) + 1e-4)#compute area occupied by the region area in the shapedef getOccupyRate(region): return (1.0 * getRegionArea(region)) / (getRegionShape(region)[0] * \\getRegionShape(region)[1] + 1.0e-10) #compute compactness of the regiondef getCompactness(region): return (1.0 * getRegionArea(region)) / (1.0 * getRegionPerimeter(region) ** 2) Now apply these methods to filter out text regions as follows: 123456789101112131415161718192021222324252627282930#total number of MSER regionsn1 = len(regions)bboxes=[]for i, region in enumerate(regions): self.colorRegion(res, region) if self.getRegionArea(region) &gt; self.grayImg.shape[0] * self.grayImg.shape[1] * AREA_LIM: #number of regions meeting the area criteria n2 += 1 self.colorRegion(res2, region) if self.getRegionPerimeter(region) &gt; 2 * (self.grayImg.shape[0] + \\ self.grayImg.shape[1]) * PERIMETER_LIM: #number of regions meeting the perimeter criteria n3 += 1 self.colorRegion(res3, region) if self.getAspectRatio(region) &lt; ASPECT_RATIO_LIM: #number of regions meeting the aspect ratio criteria n4 += 1 self.colorRegion(res4, region) if (self.getOccupyRate(region) &gt; OCCUPATION_LIM[0]) and \\ (self.getOccupyRate(region) &lt; OCCUPATION_LIM[1]): n5 += 1 self.colorRegion(res5, region) if (self.getCompactness(region) &gt; \\COMPACTNESS_LIM[0]) and \\(self.getCompactness(region) &lt; \\COMPACTNESS_LIM[1]): #final number of regions left n6 += 1 self.colorRegion(res6, region) bboxes.append(mser_bboxes[i]) After eliminating non-text regions, I draw bounding boxes on the remaining regions andvoila, we have successfully detected and segmented the characters on the number plate.Note: Apply NMS to remove overlapping bounding boxes. 12for bbox in bboxes: cv2.rectangle(img,(bbox[0]-1,bbox[1]-1),(bbox[0]+bbox[2]+1,box[1]+bbox[3]+1),(255,0,0), 1) Enough coding. Let’s see some results. 1234567891011pltShow(&quot;MSER Result Analysis&quot;, \\ (self.img, &quot;Image&quot;), \\ (self.cannyImg, &quot;Canny&quot;), \\ (res, &quot;MSER,(&#123;&#125; regions)&quot;.format(n1)), \\ (res2, &quot;Area=&#123;&#125;,(&#123;&#125; regions)&quot;.format(config.mser_areaLimit, n2)), \\ (res3, &quot;Perimeter=&#123;&#125;,(&#123;&#125; regions)&quot;.format(config.mser_perimeterLimit, n3)), \\ (res4, &quot;Aspect Ratio=&#123;&#125;,(&#123;&#125; regions)&quot;.format(config.mser_aspectRatioLimit, n4)), \\ (res5, &quot;Occupation=&#123;&#125;,(&#123;&#125; regions)&quot;.format(config.mser_occupationLimit, n5)), \\ (res6, &quot;Compactness=&#123;&#125;,(&#123;&#125; regions)&quot;.format(config.mser_compactnessLimit, n6)), \\ (boxRes, &quot;Segmented Image&quot;) \\ ) ConclusionIn this post, we covered the various image preprocessing techniques and learned about howto perform text localization on number plates.","categories":[{"name":"AI","slug":"AI","permalink":"https://nayan.co/blog/categories/AI/"}],"tags":[],"author":"Akshay Bajpai"},{"title":"Detecting Lanes using Deep Neural Networks","slug":"Detecting-Lanes-using-Deep-Neural-Networks","date":"2019-11-26T11:17:10.000Z","updated":"2019-11-26T11:14:16.248Z","comments":true,"path":"/2019/11/26/Detecting-Lanes-using-Deep-Neural-Networks/","link":"","permalink":"https://nayan.co/blog/2019/11/26/Detecting-Lanes-using-Deep-Neural-Networks/","excerpt":"","text":"This post explains how to use deep neural networks to detect highway lanes. Lane markings are the main static component on highways.They instruct the vehicles to interactively and safely drive on the highways. Lane detection is also an important task in autonomous driving, which provides localization information to the control of the car. It is also used in ADAS(Advanced Driver Assistance System). For the task of lane detection, we have two open-source datasets available. One is the Tusimple dataset and the other is the CULane dataset. Let’s have a brief look at one of the datasets. Tusimple DatasetThis dataset was released as part of the Tusimple Lane Detection Challenge. It contains 3626 video clips of 1 sec duration each. Each of these video clips contains 20 frames of which, the last frame is annotated. These videos were captured by mounting the cameras on a vehicle dashboard. You can download the dataset from here. The directory structure looks like the figure below, Each sub-directory contains 20 sequential images of which, the last frame is annotated. label_data_(date).json contains labels in JSON format for the last frame. Each line in the JSON file is a dictionary with key values… raw_file: string type. the file path in the clip lanes: it is a list of list of lanes. Each list corresponds to a lane and each element of the inner list is x-coordinate of ground truth lane. h_samples: it is a list of height values corresponding to the lanes. Each element in this list is y-coordinate of ground truth lane In this dataset, at most four lanes are annotated - the two ego lanes (two lane boundaries in which the vehicle is currently located) and the lanes to the right and left of ego lanes. All the lanes are annotated at an equal interval of height, therefore h_samples contain only one list whose elements correspond to y-coordinates for all lists in lanes. For a point in h_samples, if there is no lane at the location, its corresponding x-coordinate has -2. For example, a line in the JSON file looks like : 12345678910&#123; &quot;lanes&quot;: [ [-2, -2, -2, -2, 632, 625, 617, 609, 601, 594, 586, 578, 570, 563, 555, 547, 539, 532, 524, 516, 508, 501, 493, 485, 477, 469, 462, 454, 446, 438, 431, 423, 415, 407, 400, 392, 384, 376, 369, 361, 353, 345, 338, 330, 322, 314, 307, 299], [-2, -2, -2, -2, 719, 734, 748, 762, 777, 791, 805, 820, 834, 848, 863, 877, 891, 906, 920, 934, 949, 963, 978, 992, 1006, 1021, 1035, 1049, 1064, 1078, 1092, 1107, 1121, 1135, 1150, 1164, 1178, 1193, 1207, 1221, 1236, 1250, 1265, -2, -2, -2, -2, -2], [-2, -2, -2, -2, -2, 532, 503, 474, 445, 416, 387, 358, 329, 300, 271, 241, 212, 183, 154, 125, 96, 67, 38, 9, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2], [-2, -2, -2, 781, 822, 862, 903, 944, 984, 1025, 1066, 1107, 1147, 1188, 1229, 1269, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2, -2] ], &quot;h_samples&quot;: [240, 250, 260, 270, 280, 290, 300, 310, 320, 330, 340, 350, 360, 370, 380, 390, 400, 410, 420, 430, 440, 450, 460, 470, 480, 490, 500, 510, 520, 530, 540, 550, 560, 570, 580, 590, 600, 610, 620, 630, 640, 650, 660, 670, 680, 690, 700, 710], &quot;raw_file&quot;: &quot;path_to_clip&quot;&#125; It says that there are four lanes in the image, and the first lane starts at (632,280), the second lane starts at (719,280), the third lane starts at (532,290) and the fourth lane starts at (781,270). DataSet Visualization12345678910111213141516# import required packagesimport jsonimport numpy as npimport cv2import matplotlib.pyplot as plt# read each line of json filejson_gt = [json.loads(line) for line in open(&apos;label_data.json&apos;)]gt = json_gt[0]gt_lanes = gt[&apos;lanes&apos;]y_samples = gt[&apos;h_samples&apos;]raw_file = gt[&apos;raw_file&apos;]# see the imageimg = cv2.imread(raw_file)cv2.imshow(&apos;image&apos;,img)cv2.WaitKey(0)cv2.destroyAllWindows() Now see the JSON points visualization on the image 1234567gt_lanes_vis = [[(x, y) for (x, y) in zip(lane, y_samples) if x &gt;= 0] for lane in gt_lanes]img_vis = img.copy()for lane in gt_lanes_vis: cv2.polylines(img_vis, np.int32([lane]), isClosed=False, color=(0,255,0), thickness=5) Now, we have understood the dataset, but we can not pass the above image as a label for the neural network since grayscale images with values ranging from zero to num_classes -1 are to be passed to the deep convolution neural network to outputs an image containing predicted lanes. So, we need to generate label images for the JSON files. Label images can be generated using OpenCV by drawing lines passing through the points in the JSON file. OpenCV has an inbuilt function to draw multiple lines through a set of points. OpenCV’s Polylines method can be used here. First, create a mask of all zeros with height and width equal to the raw file’s height and width using numpy. The image size can be reduced to maintain lesser computations during training, but do not forget to maintain the same aspect ratio. Generating LabelsThe label should be a grayscale image. Generate one label for each clip from the JSON file. First, create a mask of black pixels with a shape similar to the raw_file image from the JSON file. Now, using OpenCV’s polylines method draw lines with different colors (each corresponding to each lane in lanes) on the mask image using the lanes and h_samples from the JSON file. From the three channeled mask image generate a gray scale mask image with values as class numbers. Likewise, create labels for all the images in the JSON file. You can resize the image and its label to a smaller size for lesser computations. 12345678mask = np.zeros_like(img)colors = [[255,0,0],[0,255,0],[0,0,255],[0,255,255]]for i in range(len(gt_lanes_vis)): cv2.polylines(mask, np.int32([gt_lanes_vis[i]]), isClosed=False,color=colors[i], thickness=5)!! create grey-scale label imagelabel = np.zeros((720,1280),dtype = np.uint8)for i in range(len(colors)): label[np.where((mask == colors[i]).all(axis = 2))] = i+1 Build and Train ModelLane Detection is essentially an image segmentation problem. So I am using the ERFNET model for this task, which is efficient and fast. Originally ERFNET was proposed for semantic segmentation problems, but it can also be extended to other image segmentation problems. You can check out for its paper here. It is a CNN with Encoder, Decoder and dilated convolutions along with non-bottleneck residual layers. See Fig.1 for model architecture. Build and create an object of the model. Train it over the dataset created above, for a sufficient number of epochs with Binary Cross Entropy loss or custom loss function which minimizes the per-pixel error. For better memory usage, create a dataset generator and train the model over it. Generators remove the burden of loading all the images into memory (if your dataset is of large size, you should use a generator) which leads to eating up of all memory and the other processes can’t work properly. Fig 2 shows the layers of ERFNET with input and output dimensions. Evaluate ModelAfter training, get the model’s predictions using the code snippet below. I have implemented this in Pytorch. I use the color_lanes method to convert output images from the model (which are two channeled with values as class numbers) to three channeled images. im_seg is the final overlayed image shown in Image 4. 123456789101112131415161718192021222324252627282930313233343536#using pytorchimport torchfrom torchvision.transforms import ToTensordef color_lanes(image, classes, i, color, HEIGHT, WIDTH): buffer_c1 = np.zeros((HEIGHT, WIDTH), dtype=np.uint8) buffer_c1[classes == i] = color[0] image[:, :, 0] += buffer_c1 buffer_c2 = np.zeros((HEIGHT, WIDTH), dtype=np.uint8) buffer_c2[classes == i] = color[1] image[:, :, 1] += buffer_c2 buffer_c3 = np.zeros((HEIGHT, WIDTH), dtype=np.uint8) buffer_c3[classes == i] = color[2] image[:, :, 2] += buffer_c3 return imageimg = cv2.imread(&apos;images/test.jpg&apos;) img = cv2.resize(img,(WIDTH, HEIGHT),interpolation = cv2.INETR_CUBIC)op_transforms = transforms.Compose([transforms.ToTensor()])device = torch.device(&apos;cuda&apos; if torch.cuda.is_available() else &apos;cpu&apos;)im_tensor = torch.unsqueeze(op_transforms(img), dim=0)im_tensor = im_tensor.to(device)model = ERFNET(5)model = model.to(device)model = model.eval()out = model(im_tensor)out = out.max(dim=1)[1]out_np = out.cpu().numpy()[0]out_viz = np.zeros((HEIGHT, WIDTH, 3))for i in range(1, NUM_LD_CLASSES): rand_c1 = random.randint(1, 255) rand_c2 = random.randint(1, 255) rand_c3 = random.randint(1, 255) out_viz = color_lanes( out_viz, out_np, i, (rand_c1, rand_c2, rand_c3), HEIGHT, WIDTH)instance_im = out_viz.astype(np.uint8)im_seg = cv2.addWeighted(img, 1, instance_im, 1, 0) Thanks for reading it… References ERFNet: Efficient Residual Factorized ConvNet for Real-time Semantic 2. Segmentation. Lane Detection and Classification using CNNs. https://www.mdpi.com/sensors/sensors-19-00503/article_deploy/html/images/sensors-19-00503-g004.png","categories":[{"name":"AI","slug":"AI","permalink":"https://nayan.co/blog/categories/AI/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://nayan.co/blog/tags/Machine-Learning/"},{"name":"Deep Learning","slug":"Deep-Learning","permalink":"https://nayan.co/blog/tags/Deep-Learning/"},{"name":"Lane Detection","slug":"Lane-Detection","permalink":"https://nayan.co/blog/tags/Lane-Detection/"},{"name":"Advance Driver Assistance","slug":"Advance-Driver-Assistance","permalink":"https://nayan.co/blog/tags/Advance-Driver-Assistance/"},{"name":"Autonomous Driving","slug":"Autonomous-Driving","permalink":"https://nayan.co/blog/tags/Autonomous-Driving/"}],"author":"Anand Kummari"}]}